{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoRA Fine-Tuning for CLIP\n",
        "\n",
        "This notebook fine-tunes CLIP using **LoRA** (Low-Rank Adaptation) on the food recipe dataset.\n",
        "\n",
        "**Hypothesis H1:** The fine-tuned model improves text↔image alignment compared to the CLIP baseline, measured with retrieval metrics (R@K, MRR).\n",
        "\n",
        "**This notebook (M1 LoRA):**\n",
        "- Loads pre-trained CLIP model (no quantization)\n",
        "- Applies LoRA adapters to trainable modules\n",
        "- Fine-tunes on training set using CLIP contrastive loss (only positive pairs, in-batch negatives)\n",
        "- Evaluates on test set with **same protocol as baseline** (by recipe_id)\n",
        "- Saves adapters, logs, and results for comparison\n",
        "\n",
        "**Inputs:**\n",
        "- `fine-tuning-zone/datasets/train_pairs_augmented_with_negatives.csv` (for training, filtered to label==1)\n",
        "- `fine-tuning-zone/datasets/test_pairs_positive.csv` (for evaluation)\n",
        "- `fine-tuning-zone/images/` and `fine-tuning-zone/augmented_images/` (images)\n",
        "\n",
        "**Outputs:**\n",
        "- `fine-tuning-zone/experiments/lora/run_{run_id}/` — Complete run directory with:\n",
        "  - `config.yaml` — Training configuration\n",
        "  - `adapters/` — LoRA adapter weights\n",
        "  - `training_logs.json` — Loss and timing logs\n",
        "  - `results_lora.json` — Evaluation metrics\n",
        "  - `examples_top5.json` — Qualitative examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import math\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "# Check Python environment\n",
        "print(\"=\" * 60)\n",
        "print(\"Python Environment Check\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Python executable: {sys.executable}\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Check if we're in a virtual environment\n",
        "venv_path = Path(sys.executable).parent.parent\n",
        "if (venv_path / \"pyvenv.cfg\").exists() or \"venv\" in str(sys.executable).lower():\n",
        "    print(f\"✓ Running in virtual environment: {venv_path}\")\n",
        "else:\n",
        "    print(f\"⚠ Not in a virtual environment\")\n",
        "    print(f\"  Current Python: {sys.executable}\")\n",
        "    print(f\"  If you have a venv, make sure Jupyter is using it!\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import boto3\n",
        "from botocore.config import Config\n",
        "from botocore.exceptions import ClientError\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "import yaml\n",
        "\n",
        "# CLIP and LoRA imports\n",
        "TRANSFORMERS_AVAILABLE = False\n",
        "PEFT_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from transformers import CLIPModel, CLIPProcessor\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "    print(\"✓ transformers imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Failed to import transformers: {e}\")\n",
        "\n",
        "try:\n",
        "    from peft import LoraConfig, get_peft_model, TaskType\n",
        "    PEFT_AVAILABLE = True\n",
        "    print(\"✓ peft imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Failed to import peft: {e}\")\n",
        "    TaskType = None\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Library Status\")\n",
        "print(\"=\" * 60)\n",
        "if not TRANSFORMERS_AVAILABLE:\n",
        "    print(\"  ✗ transformers: NOT AVAILABLE\")\n",
        "    print(\"    Install with: pip install transformers\")\n",
        "else:\n",
        "    print(\"  ✓ transformers: AVAILABLE\")\n",
        "    \n",
        "if not PEFT_AVAILABLE:\n",
        "    print(\"  ✗ peft: NOT AVAILABLE\")\n",
        "    print(\"    Install with: pip install peft\")\n",
        "else:\n",
        "    print(\"  ✓ peft: AVAILABLE\")\n",
        "\n",
        "if TRANSFORMERS_AVAILABLE and PEFT_AVAILABLE:\n",
        "    print(\"\\n✓ All required libraries are available!\")\n",
        "    print(\"  You can proceed with training.\")\n",
        "elif not TRANSFORMERS_AVAILABLE or not PEFT_AVAILABLE:\n",
        "    print(\"\\n✗ Some required libraries are missing!\")\n",
        "    print(\"  Please install missing libraries before continuing.\")\n",
        "\n",
        "# Load environment variables\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent\n",
        "\n",
        "ENV_PATHS = [\n",
        "    PROJECT_ROOT / \"notebooks\" / \".env\",\n",
        "    PROJECT_ROOT / \"app\" / \".env\",\n",
        "    PROJECT_ROOT / \".env\",\n",
        "]\n",
        "\n",
        "env_loaded = False\n",
        "for env_path in ENV_PATHS:\n",
        "    if env_path.exists():\n",
        "        load_dotenv(env_path)\n",
        "        print(f\"✓ Loaded .env from: {env_path}\")\n",
        "        env_loaded = True\n",
        "        break\n",
        "\n",
        "if not env_loaded:\n",
        "    print(\"⚠ No .env file found, trying default load_dotenv()...\")\n",
        "    load_dotenv()\n",
        "\n",
        "# MinIO Configuration\n",
        "MINIO_USER = os.getenv(\"MINIO_USER\")\n",
        "MINIO_PASSWORD = os.getenv(\"MINIO_PASSWORD\")\n",
        "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\n",
        "\n",
        "# Bucket configuration\n",
        "FINE_TUNING_BUCKET = \"fine-tuning-zone\"\n",
        "DATASETS_PREFIX = \"datasets\"\n",
        "IMAGES_PREFIX = \"images\"\n",
        "AUGMENTED_IMAGES_PREFIX = \"augmented_images\"\n",
        "EXPERIMENTS_PREFIX = \"experiments\"\n",
        "\n",
        "# Input/Output paths\n",
        "TRAIN_AUGMENTED_KEY = f\"{DATASETS_PREFIX}/train_pairs_augmented_with_negatives.csv\"\n",
        "TEST_MANIFEST_KEY = f\"{DATASETS_PREFIX}/test_pairs_positive.csv\"  # Test set from baseline split\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"  # Same as baseline\n",
        "\n",
        "def pick_device():\n",
        "    \"\"\"Pick device, testing if CUDA actually works (not just available).\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return \"cpu\"\n",
        "    try:\n",
        "        _ = torch.randn(1, device=\"cuda\")\n",
        "        return \"cuda\"\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ CUDA visible pero no usable: {e}\")\n",
        "        return \"cpu\"\n",
        "\n",
        "DEVICE = pick_device()\n",
        "\n",
        "# LoRA configuration\n",
        "LORA_CONFIG = {\n",
        "    \"r\": 8,\n",
        "    \"lora_alpha\": 16,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \"bias\": \"none\",\n",
        "    \"task_type\": TaskType.FEATURE_EXTRACTION if TaskType else \"FEATURE_EXTRACTION\",  # Use enum for compatibility\n",
        "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],  # Only attention modules (safer for CLIP)\n",
        "}\n",
        "\n",
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"num_epochs\": 3,\n",
        "    \"batch_size\": 2,  # Smaller batch for CPU training\n",
        "    \"gradient_accumulation_steps\": 16,  # Effective batch size = 2 * 16 = 32\n",
        "    \"warmup_steps\": 100,\n",
        "    \"save_steps\": 500,\n",
        "    \"logging_steps\": 50,\n",
        "    \"random_seed\": 42,\n",
        "}\n",
        "\n",
        "# Evaluation configuration (same as baseline)\n",
        "K_VALUES = [1, 5, 10]\n",
        "COMPUTE_MRR = True\n",
        "\n",
        "# Generate run ID\n",
        "RUN_ID = f\"lora_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
        "RUN_DIR = f\"{EXPERIMENTS_PREFIX}/lora/run_{RUN_ID}\"\n",
        "\n",
        "# Output paths\n",
        "CONFIG_KEY = f\"{RUN_DIR}/config.yaml\"\n",
        "ADAPTERS_DIR = f\"{RUN_DIR}/adapters\"\n",
        "LOGS_KEY = f\"{RUN_DIR}/training_logs.json\"\n",
        "RESULTS_KEY = f\"{RUN_DIR}/results_lora.json\"\n",
        "EXAMPLES_KEY = f\"{RUN_DIR}/examples_top5.json\"\n",
        "\n",
        "# Update config to include dataset info\n",
        "TRAINING_CONFIG[\"train_dataset\"] = TRAIN_AUGMENTED_KEY\n",
        "TRAINING_CONFIG[\"test_dataset\"] = TEST_MANIFEST_KEY\n",
        "TRAINING_CONFIG[\"filter_applied\"] = \"label == 1 (positive pairs only)\"\n",
        "\n",
        "# Logit scale clamping constants (CLIP best practice)\n",
        "LOGIT_SCALE_MIN = math.log(1/100)  # Minimum temperature: 0.01\n",
        "LOGIT_SCALE_MAX = math.log(100)     # Maximum temperature: 100\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "print(f\"  Method: LoRA\")\n",
        "print(f\"  LoRA r: {LORA_CONFIG['r']}, alpha: {LORA_CONFIG['lora_alpha']}\")\n",
        "print(f\"  Training: {TRAINING_CONFIG['num_epochs']} epochs, LR={TRAINING_CONFIG['learning_rate']}\")\n",
        "print(f\"  Batch size: {TRAINING_CONFIG['batch_size']} (effective: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']})\")\n",
        "print(f\"  Run ID: {RUN_ID}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize MinIO Client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize S3/MinIO client\n",
        "session = boto3.session.Session(\n",
        "    aws_access_key_id=MINIO_USER,\n",
        "    aws_secret_access_key=MINIO_PASSWORD,\n",
        "    region_name=\"us-east-1\"\n",
        ")\n",
        "s3 = session.client(\n",
        "    \"s3\",\n",
        "    endpoint_url=MINIO_ENDPOINT,\n",
        "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"})\n",
        ")\n",
        "\n",
        "def ensure_bucket_exists(bucket: str) -> bool:\n",
        "    \"\"\"Create bucket if it doesn't exist.\"\"\"\n",
        "    try:\n",
        "        s3.head_bucket(Bucket=bucket)\n",
        "        return True\n",
        "    except ClientError as e:\n",
        "        error_code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
        "        if error_code in (\"404\", \"NoSuchBucket\"):\n",
        "            try:\n",
        "                s3.create_bucket(Bucket=bucket)\n",
        "                print(f\"✓ Created bucket '{bucket}'\")\n",
        "                return True\n",
        "            except ClientError as create_error:\n",
        "                print(f\"✗ Failed to create bucket '{bucket}': {create_error}\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "# Verify buckets\n",
        "print(\"Checking buckets...\")\n",
        "ensure_bucket_exists(FINE_TUNING_BUCKET)\n",
        "print(\"✓ Buckets ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Training and Test Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_csv_from_minio(bucket: str, key: str) -> pd.DataFrame:\n",
        "    \"\"\"Load CSV file from MinIO into a DataFrame.\"\"\"\n",
        "    try:\n",
        "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "        df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
        "        print(f\"✓ Loaded {len(df)} rows from s3://{bucket}/{key}\")\n",
        "        return df\n",
        "    except ClientError as e:\n",
        "        print(f\"✗ Failed to load s3://{bucket}/{key}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Load training dataset (augmented, with negatives)\n",
        "print(\"Loading augmented training dataset...\")\n",
        "train_full_df = load_csv_from_minio(FINE_TUNING_BUCKET, TRAIN_AUGMENTED_KEY)\n",
        "\n",
        "if train_full_df.empty:\n",
        "    raise RuntimeError(f\"Could not load training data from s3://{FINE_TUNING_BUCKET}/{TRAIN_AUGMENTED_KEY}\")\n",
        "\n",
        "print(f\"\\nFull training dataset shape: {train_full_df.shape}\")\n",
        "print(f\"Columns: {list(train_full_df.columns)}\")\n",
        "if \"label\" in train_full_df.columns:\n",
        "    print(f\"\\nLabel distribution:\")\n",
        "    print(train_full_df[\"label\"].value_counts())\n",
        "    \n",
        "    # Filter only positive pairs (label=1)\n",
        "    train_df = train_full_df[train_full_df[\"label\"] == 1].copy()\n",
        "    print(f\"\\n✓ Filtered to {len(train_df)} positive pairs (label=1)\")\n",
        "    print(f\"  Removed {len(train_full_df) - len(train_df)} negative pairs\")\n",
        "else:\n",
        "    # If no label column, assume all are positive\n",
        "    train_df = train_full_df.copy()\n",
        "    print(f\"\\n✓ No label column found, using all {len(train_df)} pairs as positive\")\n",
        "\n",
        "# Load test dataset (from baseline split, already filtered)\n",
        "print(\"\\nLoading test dataset...\")\n",
        "test_df = load_csv_from_minio(FINE_TUNING_BUCKET, TEST_MANIFEST_KEY)\n",
        "\n",
        "if test_df.empty:\n",
        "    raise RuntimeError(f\"Could not load test data from s3://{FINE_TUNING_BUCKET}/{TEST_MANIFEST_KEY}\")\n",
        "\n",
        "print(f\"\\nTraining dataset:\")\n",
        "print(f\"  Pairs: {len(train_df)}\")\n",
        "print(f\"  Recipes: {train_df['recipe_id'].nunique()}\")\n",
        "print(f\"  Images: {train_df['image_key'].nunique()}\")\n",
        "print(f\"  Captions: {train_df['caption'].nunique()}\")\n",
        "\n",
        "print(f\"\\nTest dataset:\")\n",
        "print(f\"  Pairs: {len(test_df)}\")\n",
        "print(f\"  Recipes: {test_df['recipe_id'].nunique()}\")\n",
        "print(f\"  Images: {test_df['image_key'].nunique()}\")\n",
        "print(f\"  Captions: {test_df['caption'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Helper Functions for Image Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image_from_minio(bucket: str, key: str) -> Optional[Image.Image]:\n",
        "    \"\"\"Load an image from MinIO.\"\"\"\n",
        "    try:\n",
        "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "        img = Image.open(io.BytesIO(obj[\"Body\"].read()))\n",
        "        img.load()\n",
        "        return img.convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# Cache for image paths to avoid head_object per sample (performance optimization)\n",
        "_image_path_cache: Dict[str, Tuple[str, str]] = {}\n",
        "\n",
        "def build_image_path_cache(df: pd.DataFrame) -> Dict[str, Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Pre-build cache of image_key -> (bucket, full_key) to avoid head_object calls during training.\n",
        "    This significantly improves throughput when loading images from MinIO.\n",
        "    \"\"\"\n",
        "    print(\"Building image path cache...\")\n",
        "    cache = {}\n",
        "    bucket = FINE_TUNING_BUCKET\n",
        "    unique_keys = df[\"image_key\"].unique()\n",
        "    \n",
        "    for image_key in tqdm(unique_keys, desc=\"Caching image paths\"):\n",
        "        # Check if it's already a full path\n",
        "        if image_key.startswith(\"images/\") or image_key.startswith(\"augmented_images/\"):\n",
        "            cache[image_key] = (bucket, image_key)\n",
        "        else:\n",
        "            # Try images/ first\n",
        "            key = f\"{IMAGES_PREFIX}/{image_key}\"\n",
        "            # Check if exists, if not try augmented_images/\n",
        "            try:\n",
        "                s3.head_object(Bucket=bucket, Key=key)\n",
        "                cache[image_key] = (bucket, key)\n",
        "            except ClientError as e:\n",
        "                if e.response.get(\"Error\", {}).get(\"Code\") == \"404\":\n",
        "                    # Try augmented_images/\n",
        "                    key = f\"augmented_images/{image_key}\"\n",
        "                    cache[image_key] = (bucket, key)\n",
        "                else:\n",
        "                    raise\n",
        "    \n",
        "    print(f\"✓ Cached {len(cache)} image paths\")\n",
        "    return cache\n",
        "\n",
        "def get_image_path_in_minio(image_key: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Determine bucket and full key for an image (uses cache if available).\n",
        "    \n",
        "    Images can be in:\n",
        "    - fine-tuning-zone/images/...\n",
        "    - fine-tuning-zone/augmented_images/...\n",
        "    \n",
        "    If cache is built, uses it. Otherwise falls back to head_object (slower).\n",
        "    \"\"\"\n",
        "    # Use cache if available\n",
        "    if image_key in _image_path_cache:\n",
        "        return _image_path_cache[image_key]\n",
        "    \n",
        "    # Fallback: check if it's already a full path\n",
        "    bucket = FINE_TUNING_BUCKET\n",
        "    if image_key.startswith(\"images/\") or image_key.startswith(\"augmented_images/\"):\n",
        "        key = image_key\n",
        "    else:\n",
        "        # Try images/ first\n",
        "        key = f\"{IMAGES_PREFIX}/{image_key}\"\n",
        "        # Check if exists, if not try augmented_images/\n",
        "        try:\n",
        "            s3.head_object(Bucket=bucket, Key=key)\n",
        "        except ClientError as e:\n",
        "            if e.response.get(\"Error\", {}).get(\"Code\") == \"404\":\n",
        "                # Try augmented_images/\n",
        "                key = f\"augmented_images/{image_key}\"\n",
        "            else:\n",
        "                raise\n",
        "    \n",
        "    return bucket, key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load CLIP Model with LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not TRANSFORMERS_AVAILABLE or not PEFT_AVAILABLE:\n",
        "    raise ImportError(\"Required libraries not available. Install: pip install transformers peft\")\n",
        "\n",
        "print(f\"Loading CLIP model with LoRA: {MODEL_NAME}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "model = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "\n",
        "# Load processor\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(\"Model ready for training (LoRA)\")\n",
        "\n",
        "# Validate target_modules match actual CLIP module names (guardrail)\n",
        "print(\"\\nValidating target_modules against actual CLIP architecture...\")\n",
        "all_module_names = [name for name, _ in model.named_modules()]\n",
        "target_modules = LORA_CONFIG[\"target_modules\"]\n",
        "matched_modules = []\n",
        "for target in target_modules:\n",
        "    matches = [name for name in all_module_names if target in name]\n",
        "    if matches:\n",
        "        matched_modules.extend(matches)\n",
        "        print(f\"  ✓ '{target}' matches: {matches[:3]}...\")  # Show first 3\n",
        "    else:\n",
        "        print(f\"  ⚠ '{target}' not found in model modules!\")\n",
        "\n",
        "if not matched_modules:\n",
        "    print(\"\\n⚠ WARNING: No target_modules matched! LoRA may not be applied.\")\n",
        "    print(\"Available attention-related modules (sample):\")\n",
        "    attn_modules = [name for name in all_module_names if \"attn\" in name.lower() or \"proj\" in name.lower()]\n",
        "    for mod in attn_modules[:10]:  # Show first 10\n",
        "        print(f\"  - {mod}\")\n",
        "    print(\"  ... (use these to adjust target_modules)\")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_CONFIG[\"r\"],\n",
        "    lora_alpha=LORA_CONFIG[\"lora_alpha\"],\n",
        "    lora_dropout=LORA_CONFIG[\"lora_dropout\"],\n",
        "    bias=LORA_CONFIG[\"bias\"],\n",
        "    task_type=LORA_CONFIG[\"task_type\"],\n",
        "    target_modules=LORA_CONFIG[\"target_modules\"],\n",
        ")\n",
        "\n",
        "# Apply LoRA adapters\n",
        "print(\"Applying LoRA adapters...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Model Information\")\n",
        "print(\"=\" * 60)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Check logit_scale trainability\n",
        "print(f\"\\nlogit_scale requires_grad: {model.logit_scale.requires_grad}\")\n",
        "if not model.logit_scale.requires_grad:\n",
        "    print(\"⚠ logit_scale is frozen. Enabling it for better fine-tuning (1 extra parameter, almost free).\")\n",
        "    model.logit_scale.requires_grad_(True)\n",
        "    print(\"✓ logit_scale is now trainable\")\n",
        "\n",
        "# Get model info\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n✓ Model loaded successfully\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  Trainable ratio: {trainable_params/total_params*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create DataLoader for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "class CLIPDataset(Dataset):\n",
        "    \"\"\"Dataset for CLIP training with image-text pairs.\"\"\"\n",
        "    \n",
        "    def __init__(self, df: pd.DataFrame, processor, s3_client, bucket: str):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.s3 = s3_client\n",
        "        self.bucket = bucket\n",
        "        self.failed_image_count = 0\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        \n",
        "        # Load image\n",
        "        bucket, key = get_image_path_in_minio(row[\"image_key\"])\n",
        "        img = load_image_from_minio(bucket, key)\n",
        "        \n",
        "        # Get caption\n",
        "        caption = row[\"caption\"]\n",
        "        \n",
        "        if img is None:\n",
        "            # Return a placeholder if image fails to load\n",
        "            self.failed_image_count += 1\n",
        "            return {\"image\": None, \"text\": caption, \"is_valid\": False}\n",
        "        \n",
        "        return {\n",
        "            \"image\": img,\n",
        "            \"text\": caption, \n",
        "            \"is_valid\": True\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for DataLoader.\"\"\"\n",
        "    # Filter out invalid samples (images that failed to load)\n",
        "    batch = [b for b in batch if b.get(\"is_valid\", True) and b[\"image\"] is not None]\n",
        "    \n",
        "    if len(batch) == 0:\n",
        "        return None  # Signal to skip this batch\n",
        "    \n",
        "    images = [item[\"image\"] for item in batch]\n",
        "    texts = [item[\"text\"] for item in batch]\n",
        "    \n",
        "    # Process with CLIP processor\n",
        "    inputs = processor(\n",
        "        text=texts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "    )\n",
        "    \n",
        "    # Filter to only include what CLIPModel.forward() accepts\n",
        "    # CLIPModel expects: input_ids, attention_mask, pixel_values\n",
        "    allowed_keys = {\"input_ids\", \"attention_mask\", \"pixel_values\"}\n",
        "    return {k: v for k, v in inputs.items() if k in allowed_keys}\n",
        "\n",
        "# Build image path cache (performance optimization: avoid head_object per sample)\n",
        "print(\"Building image path cache for training set...\")\n",
        "_image_path_cache.update(build_image_path_cache(train_df))\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "print(\"\\nCreating datasets and dataloaders...\")\n",
        "\n",
        "train_dataset = CLIPDataset(train_df, processor, s3, FINE_TUNING_BUCKET)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,  # Set to 0 to avoid issues with MinIO\n",
        ")\n",
        "\n",
        "print(f\"✓ Created training dataloader\")\n",
        "print(f\"  Total batches: {len(train_dataloader)}\")\n",
        "print(f\"  Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
        "print(f\"  Gradient accumulation steps: {TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
        "print(f\"  Effective batch size: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualize Retrieval Results\n",
        "\n",
        "Visualize the retrieval results to see how well the fine-tuned model performs:\n",
        "- **Text → Image**: For each caption, show the top-5 retrieved images\n",
        "- **Image → Text**: For each image, show the top-5 retrieved captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "\n",
        "# Use random seed from training config\n",
        "RANDOM_SEED = TRAINING_CONFIG[\"random_seed\"]\n",
        "\n",
        "def visualize_text_to_image_retrieval(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    caption_to_idx: Dict[str, int],\n",
        "    image_keys: List[str],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    n_examples: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize Text → Image retrieval: for each recipe, show top-5 images.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Sample random recipes\n",
        "    unique_recipe_ids = test_df[\"recipe_id\"].unique()\n",
        "    sample_recipe_ids = pd.Series(unique_recipe_ids).sample(\n",
        "        min(n_examples, len(unique_recipe_ids)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for recipe_idx, recipe_id in enumerate(sample_recipe_ids):\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption for this recipe\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        \n",
        "        # Get caption embedding\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        \n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        similarities = image_embeddings @ text_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]  # Top 5, highest first\n",
        "        \n",
        "        # Get ground truth: images from this recipe_id\n",
        "        gt_image_indices = set(recipe_to_image_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Create figure with more vertical space for captions\n",
        "        fig = plt.figure(figsize=(16, 5))\n",
        "        gs = gridspec.GridSpec(1, 6, width_ratios=[2, 1, 1, 1, 1, 1], hspace=0.3, bottom=0.15)\n",
        "        \n",
        "        # Show query caption\n",
        "        ax0 = fig.add_subplot(gs[0])\n",
        "        ax0.text(0.5, 0.5, f'Query:\\n\"{caption}\"', \n",
        "                ha='center', va='center', fontsize=14, fontweight='bold',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
        "        ax0.axis('off')\n",
        "        \n",
        "        # Show top-5 images\n",
        "        for i, img_idx in enumerate(top5_indices):\n",
        "            img_key = image_keys[img_idx]\n",
        "            bucket, key = get_image_path_in_minio(img_key)\n",
        "            img = load_image_from_minio(bucket, key)\n",
        "            \n",
        "            if img is None:\n",
        "                continue\n",
        "            \n",
        "            # Check if this image is from the ground truth recipe\n",
        "            is_correct = img_idx in gt_image_indices\n",
        "            \n",
        "            # Get recipe_id for this image to show its representative caption\n",
        "            img_recipe_id = image_key_to_recipe_id.get(img_key, \"Unknown\")\n",
        "            # Note: Shows representative caption of the recipe (longest caption), not necessarily the exact caption for this image\n",
        "            img_caption = recipe_to_caption_text.get(img_recipe_id, \"Unknown\")\n",
        "            \n",
        "            ax = fig.add_subplot(gs[i + 1])\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "            \n",
        "            # Color border: green if correct, red if incorrect\n",
        "            border_color = 'green' if is_correct else 'red'\n",
        "            border_width = 3 if is_correct else 2\n",
        "            for spine in ax.spines.values():\n",
        "                spine.set_edgecolor(border_color)\n",
        "                spine.set_linewidth(border_width)\n",
        "            \n",
        "            # Title with similarity score\n",
        "            similarity_score = similarities[img_idx]\n",
        "            rank = i + 1\n",
        "            title = f\"Rank {rank}\\n{similarity_score:.3f}\"\n",
        "            if is_correct:\n",
        "                title += \"\\n✓ Correct\"\n",
        "            ax.set_title(title, fontsize=10, color=border_color, fontweight='bold')\n",
        "            \n",
        "            # Add recipe caption below the image\n",
        "            # Truncate long captions for display\n",
        "            display_caption = img_caption if len(img_caption) <= 40 else img_caption[:37] + \"...\"\n",
        "            ax.text(0.5, -0.15, f'\"{display_caption}\"', \n",
        "                   ha='center', va='top', fontsize=9, \n",
        "                   transform=ax.transAxes,\n",
        "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'),\n",
        "                   wrap=True)\n",
        "        \n",
        "        plt.suptitle(f'Text → Image Retrieval Example {recipe_idx + 1} (Recipe: {recipe_id_str})', \n",
        "                     fontsize=16, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print()\n",
        "\n",
        "def visualize_image_to_text_retrieval(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    unique_captions: List[str],\n",
        "    n_examples: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize Image → Text retrieval: for each image, show top-5 captions.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Sample random images\n",
        "    unique_images = test_df[\"image_key\"].unique()\n",
        "    sample_images = pd.Series(unique_images).sample(\n",
        "        min(n_examples, len(unique_images)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for img_idx_example, img_key in enumerate(sample_images):\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        similarities = text_embeddings @ img_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]  # Top 5, highest first\n",
        "        \n",
        "        # Get ground truth: recipe_id for this image\n",
        "        gt_recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        # Use all captions for ground truth (consistent with evaluation)\n",
        "        gt_caption_indices = set(recipe_to_all_caption_indices.get(gt_recipe_id_str, [])) if gt_recipe_id_str else set()\n",
        "        \n",
        "        # Load query image\n",
        "        bucket, key = get_image_path_in_minio(img_key)\n",
        "        query_img = load_image_from_minio(bucket, key)\n",
        "        \n",
        "        if query_img is None:\n",
        "            continue\n",
        "        \n",
        "        # Create figure with more vertical space for recipe name\n",
        "        fig = plt.figure(figsize=(16, 4))\n",
        "        gs = gridspec.GridSpec(1, 6, width_ratios=[1, 1, 1, 1, 1, 1], hspace=0.3, bottom=0.15)\n",
        "        \n",
        "        # Show query image\n",
        "        ax0 = fig.add_subplot(gs[0])\n",
        "        ax0.imshow(query_img)\n",
        "        ax0.axis('off')\n",
        "        ax0.set_title('Query Image', fontsize=12, fontweight='bold')\n",
        "        \n",
        "        # Add recipe name below the query image\n",
        "        if gt_recipe_id_str:\n",
        "            recipe_caption = recipe_to_caption_text.get(gt_recipe_id_str, \"Unknown\")\n",
        "            # Truncate long captions for display\n",
        "            display_recipe = recipe_caption if len(recipe_caption) <= 40 else recipe_caption[:37] + \"...\"\n",
        "            ax0.text(0.5, -0.15, f'Recipe: {gt_recipe_id_str}\\n\"{display_recipe}\"', \n",
        "                   ha='center', va='top', fontsize=9, \n",
        "                   transform=ax0.transAxes,\n",
        "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'),\n",
        "                   wrap=True)\n",
        "        \n",
        "        # Show top-5 captions\n",
        "        for i, text_idx in enumerate(top5_indices):\n",
        "            caption = unique_captions[text_idx]\n",
        "            \n",
        "            # Check if this caption is from the ground truth recipe\n",
        "            is_correct = text_idx in gt_caption_indices\n",
        "            \n",
        "            ax = fig.add_subplot(gs[i + 1])\n",
        "            ax.axis('off')\n",
        "            \n",
        "            # Color border: green if correct, red if incorrect\n",
        "            border_color = 'green' if is_correct else 'red'\n",
        "            border_width = 3 if is_correct else 2\n",
        "            \n",
        "            # Create text box\n",
        "            similarity_score = similarities[text_idx]\n",
        "            rank = i + 1\n",
        "            text_content = f\"Rank {rank}\\n\\n\\\"{caption}\\\"\\n\\nSimilarity: {similarity_score:.3f}\"\n",
        "            if is_correct:\n",
        "                text_content += \"\\n\\n✓ Correct\"\n",
        "            \n",
        "            ax.text(0.5, 0.5, text_content,\n",
        "                   ha='center', va='center', fontsize=10,\n",
        "                   bbox=dict(boxstyle='round', facecolor='lightyellow', \n",
        "                            edgecolor=border_color, linewidth=border_width, alpha=0.8),\n",
        "                   wrap=True)\n",
        "        \n",
        "        plt.suptitle(f'Image → Text Retrieval Example {img_idx_example + 1} (Recipe: {gt_recipe_id_str})', \n",
        "                     fontsize=16, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print()\n",
        "\n",
        "# Visualization functions defined above\n",
        "# They will be called after embeddings are generated (see section after evaluation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clip_contrastive_loss(image_embeds, text_embeds, logit_scale):\n",
        "    \"\"\"\n",
        "    CLIP contrastive loss with in-batch negatives.\n",
        "    \n",
        "    Args:\n",
        "        image_embeds: Image embeddings (batch_size, embed_dim)\n",
        "        text_embeds: Text embeddings (batch_size, embed_dim)\n",
        "        logit_scale: Learnable temperature parameter\n",
        "    \n",
        "    Returns:\n",
        "        loss: Contrastive loss\n",
        "    \"\"\"\n",
        "    # Normalize embeddings\n",
        "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
        "    text_embeds = F.normalize(text_embeds, dim=-1)\n",
        "    \n",
        "    # Compute logits\n",
        "    logit_scale = logit_scale.exp()\n",
        "    logits_per_text = logit_scale * (text_embeds @ image_embeds.T)  # (B, B)\n",
        "    logits_per_image = logits_per_text.T  # (B, B)\n",
        "    \n",
        "    # Labels: diagonal (each text matches its corresponding image)\n",
        "    batch_size = image_embeds.size(0)\n",
        "    labels = torch.arange(batch_size, device=image_embeds.device)\n",
        "    \n",
        "    # Cross-entropy losses\n",
        "    loss_t = F.cross_entropy(logits_per_text, labels)\n",
        "    loss_i = F.cross_entropy(logits_per_image, labels)\n",
        "    \n",
        "    # Average\n",
        "    loss = (loss_t + loss_i) / 2.0\n",
        "    \n",
        "    return loss\n",
        "\n",
        "# Setup optimizer (only trainable parameters)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=TRAINING_CONFIG[\"learning_rate\"],\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Setup learning rate scheduler (linear warmup + cosine decay)\n",
        "# Calculate steps per epoch accounting for gradient accumulation\n",
        "# Defensive check: ensure we have batches to train on\n",
        "if len(train_dataloader) == 0:\n",
        "    raise RuntimeError(\"train_dataloader has 0 batches. Check dataset filtering / loading.\")\n",
        "\n",
        "steps_per_epoch = int(np.ceil(len(train_dataloader) / TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "steps_per_epoch = max(1, steps_per_epoch)  # Ensure at least 1 step\n",
        "total_steps = steps_per_epoch * TRAINING_CONFIG[\"num_epochs\"]\n",
        "# Ensure warmup_steps doesn't exceed total_steps\n",
        "warmup_steps = min(TRAINING_CONFIG[\"warmup_steps\"], total_steps)\n",
        "\n",
        "def get_lr_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    \"\"\"Create learning rate scheduler with warmup.\"\"\"\n",
        "    from torch.optim.lr_scheduler import LambdaLR\n",
        "    \n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
        "    \n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "scheduler = get_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "print(f\"✓ Training setup complete\")\n",
        "print(f\"  Optimizer: AdamW (lr={TRAINING_CONFIG['learning_rate']})\")\n",
        "print(f\"  Scheduler: Linear warmup + Cosine decay\")\n",
        "print(f\"  Total steps: {total_steps}\")\n",
        "print(f\"  Warmup steps: {warmup_steps}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training logs\n",
        "training_logs = {\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"start_time\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"config\": {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"lora\": LORA_CONFIG,\n",
        "        \"training\": TRAINING_CONFIG,\n",
        "    },\n",
        "    \"epochs\": [],\n",
        "    \"total_time_seconds\": 0,\n",
        "    \"peak_memory_mb\": 0,\n",
        "}\n",
        "\n",
        "# Get device for inputs\n",
        "if DEVICE == \"cuda\":\n",
        "    main_device = next(model.parameters()).device\n",
        "else:\n",
        "    main_device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Device: {main_device}\")\n",
        "print(f\"Total epochs: {TRAINING_CONFIG['num_epochs']}\")\n",
        "batches_per_epoch = len(train_dataloader)\n",
        "optimizer_steps_per_epoch = int(np.ceil(batches_per_epoch / TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "print(f\"Batches per epoch: {batches_per_epoch}\")\n",
        "print(f\"Optimizer steps per epoch: {optimizer_steps_per_epoch}\")\n",
        "print(f\"Total optimizer steps: {total_steps}\")\n",
        "print()\n",
        "\n",
        "model.train()\n",
        "training_start_time = time.time()\n",
        "global_step = 0\n",
        "peak_memory = 0\n",
        "step_losses = []  # Track losses per step (not per batch)\n",
        "\n",
        "# Setup mixed precision training (AMP) for GPU\n",
        "# Use generic torch.amp API (works correctly on both CPU and CUDA)\n",
        "use_amp = (DEVICE == \"cuda\")\n",
        "if use_amp:\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=True)\n",
        "    print(\"✓ Mixed precision (AMP) enabled for GPU training\")\n",
        "else:\n",
        "    scaler = None  # No scaler needed for CPU\n",
        "    print(\"ℹ Mixed precision disabled (CPU training)\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(TRAINING_CONFIG[\"random_seed\"])\n",
        "np.random.seed(TRAINING_CONFIG[\"random_seed\"])\n",
        "random.seed(TRAINING_CONFIG[\"random_seed\"])\n",
        "\n",
        "for epoch in range(TRAINING_CONFIG[\"num_epochs\"]):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_losses = []\n",
        "    running_loss = 0.0  # Accumulate loss across gradient accumulation steps\n",
        "    \n",
        "    print(f\"\\nEpoch {epoch + 1}/{TRAINING_CONFIG['num_epochs']}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Reset failed image counter for this epoch\n",
        "    train_dataset.failed_image_count = 0\n",
        "    \n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
        "    \n",
        "    # Track last batch_idx explicitly (more robust than using locals())\n",
        "    last_batch_idx = -1\n",
        "    for batch_idx, inputs in enumerate(progress_bar):\n",
        "        # Skip batches with no valid samples (filtered out in collate_fn)\n",
        "        if inputs is None:\n",
        "            continue\n",
        "        \n",
        "        last_batch_idx = batch_idx\n",
        "        # Move inputs to device\n",
        "        input_ids = inputs[\"input_ids\"].to(main_device)\n",
        "        attention_mask = inputs.get(\"attention_mask\", None)\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.to(main_device)\n",
        "        pixel_values = inputs[\"pixel_values\"].to(main_device)\n",
        "        \n",
        "        # Forward pass using specific methods (avoids PEFT wrapper issues)\n",
        "        # Use get_image_features and get_text_features instead of forward()\n",
        "        text_inputs = {\n",
        "            \"input_ids\": input_ids,\n",
        "        }\n",
        "        if attention_mask is not None:\n",
        "            text_inputs[\"attention_mask\"] = attention_mask\n",
        "        \n",
        "        image_inputs = {\n",
        "            \"pixel_values\": pixel_values,\n",
        "        }\n",
        "        \n",
        "        # Forward pass with mixed precision (AMP) when on GPU\n",
        "        # Using enabled=use_amp for robustness (works even if refactored)\n",
        "        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
        "            # Get embeddings separately\n",
        "            text_embeds = model.get_text_features(**text_inputs)\n",
        "            image_embeds = model.get_image_features(**image_inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = clip_contrastive_loss(\n",
        "                image_embeds,\n",
        "                text_embeds,\n",
        "                model.logit_scale\n",
        "            )\n",
        "            \n",
        "            # Scale loss for gradient accumulation\n",
        "            loss = loss / TRAINING_CONFIG[\"gradient_accumulation_steps\"]\n",
        "        \n",
        "        # Backward pass (with scaler for AMP, direct for CPU)\n",
        "        if use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "        \n",
        "        # Accumulate unscaled loss for this accumulation step\n",
        "        running_loss += float(loss.item() * TRAINING_CONFIG[\"gradient_accumulation_steps\"])\n",
        "        \n",
        "        # Store unscaled loss for epoch average\n",
        "        epoch_losses.append(float(loss.item() * TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "        \n",
        "        # Update weights (only after accumulation steps)\n",
        "        if (batch_idx + 1) % TRAINING_CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
        "            if use_amp:\n",
        "                # Gradient clipping (especially important with trainable logit_scale)\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    [p for p in model.parameters() if p.requires_grad],\n",
        "                    max_norm=1.0\n",
        "                )\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                # Gradient clipping for CPU\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    [p for p in model.parameters() if p.requires_grad],\n",
        "                    max_norm=1.0\n",
        "                )\n",
        "                optimizer.step()\n",
        "            \n",
        "            # Clamp logit_scale to prevent temperature explosion (CLIP best practice)\n",
        "            with torch.no_grad():\n",
        "                model.logit_scale.clamp_(LOGIT_SCALE_MIN, LOGIT_SCALE_MAX)\n",
        "            \n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            # Store average loss for this step (across accumulation)\n",
        "            step_losses.append(running_loss / TRAINING_CONFIG[\"gradient_accumulation_steps\"])\n",
        "            running_loss = 0.0\n",
        "        \n",
        "        # Logging (by step, not by batch)\n",
        "        if global_step > 0 and global_step % TRAINING_CONFIG[\"logging_steps\"] == 0:\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            # Use step_losses for accurate logging\n",
        "            avg_loss = np.mean(step_losses[-TRAINING_CONFIG[\"logging_steps\"]:]) if len(step_losses) >= TRAINING_CONFIG[\"logging_steps\"] else np.mean(step_losses)\n",
        "            # Log logit_scale (temperature) to monitor stability\n",
        "            logit_scale_value = float(model.logit_scale.exp().item())\n",
        "            progress_bar.set_postfix({\n",
        "                \"loss\": f\"{avg_loss:.4f}\",\n",
        "                \"lr\": f\"{current_lr:.2e}\",\n",
        "                \"temp\": f\"{logit_scale_value:.2f}\",  # Temperature (exp of logit_scale)\n",
        "                \"step\": global_step\n",
        "            })\n",
        "            \n",
        "            # Store in training logs for analysis\n",
        "            if \"step_logs\" not in training_logs:\n",
        "                training_logs[\"step_logs\"] = []\n",
        "            training_logs[\"step_logs\"].append({\n",
        "                \"global_step\": global_step,\n",
        "                \"loss\": float(avg_loss),\n",
        "                \"learning_rate\": float(current_lr),\n",
        "                \"logit_scale\": float(model.logit_scale.item()),\n",
        "                \"temperature\": logit_scale_value\n",
        "            })\n",
        "        \n",
        "        # Track peak memory\n",
        "        if DEVICE == \"cuda\":\n",
        "            current_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
        "            peak_memory = max(peak_memory, current_memory)\n",
        "    \n",
        "    # Process remaining gradients at end of epoch if not aligned with accumulation steps\n",
        "    # Protection: ensure last_batch_idx is valid (should be >= 0 if dataloader has batches)\n",
        "    if last_batch_idx >= 0:\n",
        "        remaining_steps = (last_batch_idx + 1) % TRAINING_CONFIG[\"gradient_accumulation_steps\"]\n",
        "    else:\n",
        "        remaining_steps = 0  # No batches processed\n",
        "    \n",
        "    if remaining_steps > 0:\n",
        "        if use_amp:\n",
        "            # Gradient clipping (especially important with trainable logit_scale)\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in model.parameters() if p.requires_grad],\n",
        "                max_norm=1.0\n",
        "            )\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # Gradient clipping for CPU\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in model.parameters() if p.requires_grad],\n",
        "                max_norm=1.0\n",
        "            )\n",
        "            optimizer.step()\n",
        "        \n",
        "        # Clamp logit_scale to prevent temperature explosion (CLIP best practice)\n",
        "        with torch.no_grad():\n",
        "            model.logit_scale.clamp_(LOGIT_SCALE_MIN, LOGIT_SCALE_MAX)\n",
        "        \n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        global_step += 1\n",
        "        # Store average loss for remaining accumulation steps\n",
        "        step_losses.append(running_loss / remaining_steps)\n",
        "        running_loss = 0.0\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    avg_epoch_loss = np.mean(epoch_losses)\n",
        "    \n",
        "    # Calculate optimizer steps for this epoch\n",
        "    batches = len(train_dataloader)\n",
        "    optimizer_steps_this_epoch = int(np.ceil(batches / TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "    \n",
        "    # Report failed image loads (guardrail: detect if training is contaminated)\n",
        "    failed_images_this_epoch = train_dataset.failed_image_count\n",
        "    total_samples = len(train_dataloader) * TRAINING_CONFIG[\"batch_size\"]\n",
        "    failure_rate = (failed_images_this_epoch / total_samples * 100) if total_samples > 0 else 0\n",
        "    \n",
        "    if failed_images_this_epoch > 0:\n",
        "        print(f\"  ⚠ Failed image loads: {failed_images_this_epoch} ({failure_rate:.2f}% of samples)\")\n",
        "        \n",
        "        # Abort if contamination is too high (>0.5% threshold)\n",
        "        CONTAMINATION_THRESHOLD = 0.5\n",
        "        if failure_rate > CONTAMINATION_THRESHOLD:\n",
        "            raise RuntimeError(\n",
        "                f\"Training aborted: image load failure rate ({failure_rate:.2f}%) exceeds threshold ({CONTAMINATION_THRESHOLD}%). \"\n",
        "                f\"Training would be contaminated. Check image paths and MinIO connectivity.\"\n",
        "            )\n",
        "    \n",
        "    epoch_log = {\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"avg_loss\": float(avg_epoch_loss),\n",
        "        \"time_seconds\": float(epoch_time),\n",
        "        \"batches\": batches,\n",
        "        \"optimizer_steps\": optimizer_steps_this_epoch,\n",
        "        \"failed_image_loads\": failed_images_this_epoch,  # Track for analysis\n",
        "    }\n",
        "    training_logs[\"epochs\"].append(epoch_log)\n",
        "    \n",
        "    print(f\"  Average loss: {avg_epoch_loss:.4f}\")\n",
        "    print(f\"  Time: {epoch_time:.2f} seconds\")\n",
        "\n",
        "training_time = time.time() - training_start_time\n",
        "training_logs[\"total_time_seconds\"] = float(training_time)\n",
        "training_logs[\"peak_memory_mb\"] = float(peak_memory)\n",
        "training_logs[\"end_time\"] = datetime.utcnow().isoformat() + \"Z\"\n",
        "training_logs[\"total_steps\"] = global_step\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training Complete\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "print(f\"Peak memory: {peak_memory:.2f} MB\")\n",
        "print(f\"Total steps: {global_step}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_json_to_minio(data: Dict, bucket: str, key: str) -> bool:\n",
        "    \"\"\"Save dictionary as JSON to MinIO.\"\"\"\n",
        "    try:\n",
        "        json_bytes = json.dumps(data, indent=2).encode(\"utf-8\")\n",
        "        s3.put_object(\n",
        "            Bucket=bucket,\n",
        "            Key=key,\n",
        "            Body=json_bytes,\n",
        "            ContentType=\"application/json\",\n",
        "        )\n",
        "        size_kb = len(json_bytes) / 1024\n",
        "        print(f\"✓ Saved to s3://{bucket}/{key} ({size_kb:.1f} KB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to save: {e}\")\n",
        "        return False\n",
        "\n",
        "def save_yaml_to_minio(data: Dict, bucket: str, key: str) -> bool:\n",
        "    \"\"\"Save dictionary as YAML to MinIO.\"\"\"\n",
        "    try:\n",
        "        yaml_str = yaml.dump(data, default_flow_style=False, sort_keys=False)\n",
        "        yaml_bytes = yaml_str.encode(\"utf-8\")\n",
        "        s3.put_object(\n",
        "            Bucket=bucket,\n",
        "            Key=key,\n",
        "            Body=yaml_bytes,\n",
        "            ContentType=\"text/yaml\",\n",
        "        )\n",
        "        size_kb = len(yaml_bytes) / 1024\n",
        "        print(f\"✓ Saved to s3://{bucket}/{key} ({size_kb:.1f} KB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to save: {e}\")\n",
        "        return False\n",
        "\n",
        "# Save adapters locally first, then upload to MinIO\n",
        "print(\"Saving adapters...\")\n",
        "local_adapters_dir = Path(f\"adapters_{RUN_ID}\")\n",
        "model.save_pretrained(str(local_adapters_dir))\n",
        "\n",
        "# Upload adapters to MinIO (upload config and weights - supports both .bin and .safetensors)\n",
        "print(\"Uploading adapters to MinIO...\")\n",
        "adapter_files = [\"adapter_config.json\", \"adapter_model.bin\", \"adapter_model.safetensors\"]\n",
        "for file_name in adapter_files:\n",
        "    file_path = local_adapters_dir / file_name\n",
        "    if file_path.exists():\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            s3.put_object(\n",
        "                Bucket=FINE_TUNING_BUCKET,\n",
        "                Key=f\"{ADAPTERS_DIR}/{file_name}\",\n",
        "                Body=f.read(),\n",
        "            )\n",
        "        print(f\"  ✓ Uploaded {file_name}\")\n",
        "\n",
        "# Save training logs\n",
        "print(\"\\nSaving training logs...\")\n",
        "save_json_to_minio(training_logs, FINE_TUNING_BUCKET, LOGS_KEY)\n",
        "\n",
        "# Save configuration\n",
        "config_data = {\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"method\": \"lora\",\n",
        "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"lora_config\": LORA_CONFIG,\n",
        "    \"training_config\": TRAINING_CONFIG,\n",
        "    \"model_info\": {\n",
        "        \"total_parameters\": int(total_params),\n",
        "        \"trainable_parameters\": int(trainable_params),\n",
        "        \"trainable_ratio\": float(trainable_params / total_params),\n",
        "    },\n",
        "}\n",
        "save_yaml_to_minio(config_data, FINE_TUNING_BUCKET, CONFIG_KEY)\n",
        "\n",
        "print(f\"\\n✅ Adapters and logs saved successfully!\")\n",
        "print(f\"  Adapters: s3://{FINE_TUNING_BUCKET}/{ADAPTERS_DIR}/\")\n",
        "print(f\"  Logs: s3://{FINE_TUNING_BUCKET}/{LOGS_KEY}\")\n",
        "print(f\"  Config: s3://{FINE_TUNING_BUCKET}/{CONFIG_KEY}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluation (Same Protocol as Baseline)\n",
        "\n",
        "Now we evaluate the fine-tuned model using the same evaluation functions as the baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Generate embeddings for test set (same as baseline)\n",
        "print(\"=\" * 60)\n",
        "print(\"Generating Embeddings for Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def generate_image_embeddings(\n",
        "    df: pd.DataFrame, \n",
        "    model, \n",
        "    processor, \n",
        "    device: Union[str, torch.device], \n",
        "    batch_size: int = 32\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"Generate image embeddings for all images in the dataset.\"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    image_keys = []\n",
        "    failed_keys = []\n",
        "    \n",
        "    unique_images = df[\"image_key\"].unique()\n",
        "    print(f\"Generating embeddings for {len(unique_images)} unique images...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(unique_images), batch_size)):\n",
        "            batch_keys = unique_images[i:i+batch_size]\n",
        "            batch_images = []\n",
        "            valid_keys = []\n",
        "            \n",
        "            for img_key in batch_keys:\n",
        "                bucket, key = get_image_path_in_minio(img_key)\n",
        "                img = load_image_from_minio(bucket, key)\n",
        "                if img is not None:\n",
        "                    batch_images.append(img)\n",
        "                    valid_keys.append(img_key)\n",
        "                else:\n",
        "                    failed_keys.append(img_key)\n",
        "            \n",
        "            if not batch_images:\n",
        "                continue\n",
        "            \n",
        "            inputs = processor(images=batch_images, return_tensors=\"pt\", padding=True)\n",
        "            # Move inputs to device explicitly (more robust than .to(device) on BatchEncoding)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            outputs = model.get_image_features(**inputs)\n",
        "            batch_emb = F.normalize(outputs, p=2, dim=1)\n",
        "            \n",
        "            embeddings.append(batch_emb.cpu().numpy())\n",
        "            image_keys.extend(valid_keys)\n",
        "    \n",
        "    if failed_keys:\n",
        "        print(f\"⚠ Failed to load {len(failed_keys)} images\")\n",
        "    \n",
        "    embeddings = np.vstack(embeddings) if embeddings else np.array([])\n",
        "    print(f\"✓ Generated {len(image_keys)} image embeddings (shape: {embeddings.shape})\")\n",
        "    return embeddings, image_keys\n",
        "\n",
        "def generate_text_embeddings(\n",
        "    texts: List[str], \n",
        "    model, \n",
        "    processor, \n",
        "    device: Union[str, torch.device], \n",
        "    batch_size: int = 32\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Generate text embeddings for all captions.\"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    \n",
        "    print(f\"Generating embeddings for {len(texts)} texts...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(texts), batch_size)):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            inputs = processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            # Move inputs to device explicitly (more robust than .to(device) on BatchEncoding)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            outputs = model.get_text_features(**inputs)\n",
        "            batch_emb = F.normalize(outputs, p=2, dim=1)\n",
        "            embeddings.append(batch_emb.cpu().numpy())\n",
        "    \n",
        "    embeddings = np.vstack(embeddings) if embeddings else np.array([])\n",
        "    print(f\"✓ Generated {len(texts)} text embeddings (shape: {embeddings.shape})\")\n",
        "    return embeddings\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Generate embeddings\n",
        "image_embeddings, image_keys = generate_image_embeddings(\n",
        "    test_df, model, processor, main_device, batch_size=16  # Smaller batch for LoRA\n",
        ")\n",
        "\n",
        "unique_captions = test_df[\"caption\"].unique().tolist()\n",
        "text_embeddings = generate_text_embeddings(unique_captions, model, processor, main_device, batch_size=16)\n",
        "\n",
        "# Create official copy to ensure alignment with text_embeddings (never modify this)\n",
        "unique_captions_list = list(unique_captions)  # Explicit copy to prevent accidental modification\n",
        "\n",
        "embedding_time = time.time() - start_time\n",
        "print(f\"\\n✓ Embedding generation complete in {embedding_time:.2f} seconds\")\n",
        "\n",
        "# Create mapping dictionaries\n",
        "image_key_to_idx = {key: idx for idx, key in enumerate(image_keys)}\n",
        "caption_to_idx = {caption: idx for idx, caption in enumerate(unique_captions_list)}\n",
        "\n",
        "# Precompute lookup dictionaries for O(1) access (same as baseline)\n",
        "print(\"\\nPrecomputing lookup dictionaries...\")\n",
        "\n",
        "# recipe_id -> list of image indices\n",
        "recipe_to_image_indices: Dict[str, List[int]] = {}\n",
        "for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "    recipe_id_str = str(recipe_id)\n",
        "    recipe_images = test_df[test_df[\"recipe_id\"] == recipe_id][\"image_key\"].unique()\n",
        "    recipe_to_image_indices[recipe_id_str] = [\n",
        "        image_key_to_idx.get(img_key) \n",
        "        for img_key in recipe_images \n",
        "        if image_key_to_idx.get(img_key) is not None\n",
        "    ]\n",
        "\n",
        "# recipe_id -> list of caption indices (one caption per recipe for text→image query)\n",
        "# Use longest caption as representative (most informative)\n",
        "recipe_to_caption_indices: Dict[str, List[int]] = {}\n",
        "recipe_to_caption_text: Dict[str, str] = {}\n",
        "# recipe_id -> list of ALL caption indices (for image→text ground truth)\n",
        "recipe_to_all_caption_indices: Dict[str, List[int]] = {}\n",
        "for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "    recipe_id_str = str(recipe_id)\n",
        "    recipe_captions = test_df[test_df[\"recipe_id\"] == recipe_id][\"caption\"].unique()\n",
        "    if len(recipe_captions) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Use longest caption as representative (most informative)\n",
        "    # If multiple have same length, use first after sorting alphabetically (deterministic)\n",
        "    representative_caption = max(recipe_captions, key=lambda c: (len(c), c))\n",
        "    \n",
        "    # Store representative caption for text→image queries\n",
        "    cap_idx = caption_to_idx.get(representative_caption)\n",
        "    if cap_idx is not None:\n",
        "        recipe_to_caption_indices[recipe_id_str] = [cap_idx]\n",
        "        recipe_to_caption_text[recipe_id_str] = representative_caption\n",
        "    \n",
        "    # Store ALL captions for image→text ground truth (more fair if multiple captions per recipe)\n",
        "    all_caption_indices = []\n",
        "    for caption in recipe_captions:\n",
        "        cap_idx = caption_to_idx.get(caption)\n",
        "        if cap_idx is not None:\n",
        "            all_caption_indices.append(cap_idx)\n",
        "    recipe_to_all_caption_indices[recipe_id_str] = sorted(set(all_caption_indices))  # Remove duplicates, sorted for determinism\n",
        "\n",
        "# image_key -> recipe_id (as string for consistency)\n",
        "image_key_to_recipe_id: Dict[str, str] = {}\n",
        "for _, row in test_df.iterrows():\n",
        "    if row[\"image_key\"] not in image_key_to_recipe_id:\n",
        "        image_key_to_recipe_id[row[\"image_key\"]] = str(row[\"recipe_id\"])\n",
        "\n",
        "print(f\"✓ Precomputed lookups for {len(recipe_to_image_indices)} recipes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Evaluation Functions (Same as Baseline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy evaluation functions from baseline (same protocol by recipe_id)\n",
        "def compute_recall_at_k(scores: np.ndarray, ground_truth_indices: List[int], k: int) -> float:\n",
        "    \"\"\"\n",
        "    Compute Recall@K.\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "        k: top-K to consider\n",
        "    \n",
        "    Returns:\n",
        "        recall@k: 1.0 if any ground truth is in top-K, else 0.0\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    top_k_indices = np.argsort(scores)[-k:][::-1]  # Top K, highest first\n",
        "    top_k_set = set(top_k_indices)\n",
        "    \n",
        "    # Check if any ground truth is in top-K\n",
        "    for gt_idx in ground_truth_indices:\n",
        "        if gt_idx in top_k_set:\n",
        "            return 1.0\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "def compute_mrr(scores: np.ndarray, ground_truth_indices: List[int]) -> float:\n",
        "    \"\"\"\n",
        "    Compute Mean Reciprocal Rank.\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "    \n",
        "    Returns:\n",
        "        mrr: 1/rank of first correct result, or 0.0 if none found\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Sort by score (descending)\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    \n",
        "    # Find rank of first ground truth\n",
        "    for rank, idx in enumerate(sorted_indices, start=1):\n",
        "        if idx in ground_truth_indices:\n",
        "            return 1.0 / rank\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "def compute_first_hit_rank(scores: np.ndarray, ground_truth_indices: List[int]) -> float:\n",
        "    \"\"\"\n",
        "    Compute rank of first correct result (first hit rank).\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "    \n",
        "    Returns:\n",
        "        first_hit_rank: rank of first correct result (1-indexed), or len(scores)+1 if not found\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return float(len(scores) + 1)\n",
        "    \n",
        "    # Sort by score (descending)\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    \n",
        "    # Find rank of first ground truth\n",
        "    for rank, idx in enumerate(sorted_indices, start=1):\n",
        "        if idx in ground_truth_indices:\n",
        "            return float(rank)\n",
        "    \n",
        "    return float(len(scores) + 1)  # Not found\n",
        "\n",
        "def evaluate_text_to_image_by_recipe(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    k_values: List[int],\n",
        "    should_compute_mrr: bool = True,\n",
        "    should_compute_median_rank: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate Text → Image retrieval by recipe_id.\n",
        "    \n",
        "    For each recipe_id, use its representative caption to retrieve images.\n",
        "    Ground truth: all images from that recipe_id.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating Text → Image retrieval (by recipe_id)...\")\n",
        "    \n",
        "    unique_recipe_ids = test_df[\"recipe_id\"].unique()\n",
        "    \n",
        "    recalls = {f\"R@{k}\": [] for k in k_values}\n",
        "    mrr_scores = [] if should_compute_mrr else None\n",
        "    median_ranks = [] if should_compute_median_rank else None\n",
        "    \n",
        "    for recipe_id in tqdm(unique_recipe_ids, desc=\"Processing recipes\"):\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption embedding (first caption for this recipe)\n",
        "        caption_indices = recipe_to_caption_indices.get(recipe_id_str, [])\n",
        "        if not caption_indices:\n",
        "            continue\n",
        "        \n",
        "        # Use first caption (or could average multiple)\n",
        "        text_idx = caption_indices[0]\n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        \n",
        "        # Compute similarities with all images\n",
        "        similarities = image_embeddings @ text_emb  # (n_images,)\n",
        "        \n",
        "        # Get ground truth: all images from this recipe_id\n",
        "        ground_truth_indices = recipe_to_image_indices.get(recipe_id_str, [])\n",
        "        ground_truth_indices = [idx for idx in ground_truth_indices if idx is not None]\n",
        "        \n",
        "        if len(ground_truth_indices) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Compute metrics\n",
        "        for k in k_values:\n",
        "            recall = compute_recall_at_k(similarities, ground_truth_indices, k)\n",
        "            recalls[f\"R@{k}\"].append(recall)\n",
        "        \n",
        "        if should_compute_mrr:\n",
        "            mrr = compute_mrr(similarities, ground_truth_indices)\n",
        "            mrr_scores.append(mrr)\n",
        "        \n",
        "        if should_compute_median_rank:\n",
        "            first_hit_rank = compute_first_hit_rank(similarities, ground_truth_indices)\n",
        "            median_ranks.append(first_hit_rank)\n",
        "    \n",
        "    # Average metrics\n",
        "    results = {}\n",
        "    for k in k_values:\n",
        "        results[f\"R@{k}\"] = float(np.mean(recalls[f\"R@{k}\"])) if recalls[f\"R@{k}\"] else 0.0\n",
        "    \n",
        "    if should_compute_mrr and mrr_scores:\n",
        "        results[\"MRR\"] = float(np.mean(mrr_scores))\n",
        "    \n",
        "    if should_compute_median_rank and median_ranks:\n",
        "        results[\"MedianRank_first_hit\"] = float(np.median(median_ranks))\n",
        "        results[\"MeanRank_first_hit\"] = float(np.mean(median_ranks))\n",
        "    \n",
        "    return results\n",
        "\n",
        "def evaluate_image_to_text_by_recipe(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    k_values: List[int],\n",
        "    should_compute_mrr: bool = True,\n",
        "    should_compute_median_rank: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate Image → Text retrieval by recipe_id.\n",
        "    \n",
        "    For each image, retrieve captions. Ground truth: ALL captions from the same recipe_id.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating Image → Text retrieval (by recipe_id)...\")\n",
        "    \n",
        "    unique_images = test_df[\"image_key\"].unique()\n",
        "    \n",
        "    recalls = {f\"R@{k}\": [] for k in k_values}\n",
        "    mrr_scores = [] if should_compute_mrr else None\n",
        "    median_ranks = [] if should_compute_median_rank else None\n",
        "    \n",
        "    for img_key in tqdm(unique_images, desc=\"Processing images\"):\n",
        "        # Get image embedding\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        \n",
        "        # Compute similarities with all captions\n",
        "        similarities = text_embeddings @ img_emb  # (n_texts,)\n",
        "        \n",
        "        # Get recipe_id for this image\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        \n",
        "        # Get ground truth: ALL captions from the same recipe_id (more fair)\n",
        "        ground_truth_indices = recipe_to_all_caption_indices.get(recipe_id_str, [])\n",
        "        ground_truth_indices = [idx for idx in ground_truth_indices if idx is not None]\n",
        "        \n",
        "        if len(ground_truth_indices) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Compute metrics\n",
        "        for k in k_values:\n",
        "            recall = compute_recall_at_k(similarities, ground_truth_indices, k)\n",
        "            recalls[f\"R@{k}\"].append(recall)\n",
        "        \n",
        "        if should_compute_mrr:\n",
        "            mrr = compute_mrr(similarities, ground_truth_indices)\n",
        "            mrr_scores.append(mrr)\n",
        "        \n",
        "        if should_compute_median_rank:\n",
        "            first_hit_rank = compute_first_hit_rank(similarities, ground_truth_indices)\n",
        "            median_ranks.append(first_hit_rank)\n",
        "    \n",
        "    # Average metrics\n",
        "    results = {}\n",
        "    for k in k_values:\n",
        "        results[f\"R@{k}\"] = float(np.mean(recalls[f\"R@{k}\"])) if recalls[f\"R@{k}\"] else 0.0\n",
        "    \n",
        "    if should_compute_mrr and mrr_scores:\n",
        "        results[\"MRR\"] = float(np.mean(mrr_scores))\n",
        "    \n",
        "    if should_compute_median_rank and median_ranks:\n",
        "        results[\"MedianRank_first_hit\"] = float(np.median(median_ranks))\n",
        "        results[\"MeanRank_first_hit\"] = float(np.mean(median_ranks))\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run evaluation\n",
        "print(\"=\" * 60)\n",
        "print(\"Running Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "eval_start_time = time.time()\n",
        "\n",
        "# Text → Image retrieval (by recipe_id)\n",
        "text_to_image_results = evaluate_text_to_image_by_recipe(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_indices,\n",
        "    recipe_to_image_indices,\n",
        "    image_key_to_idx,\n",
        "    K_VALUES,\n",
        "    COMPUTE_MRR,\n",
        "    should_compute_median_rank=True\n",
        ")\n",
        "\n",
        "# Image → Text retrieval (by recipe_id)\n",
        "image_to_text_results = evaluate_image_to_text_by_recipe(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    K_VALUES,\n",
        "    COMPUTE_MRR,\n",
        "    should_compute_median_rank=True\n",
        ")\n",
        "\n",
        "eval_time = time.time() - eval_start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Evaluation Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Mapping for display names (only for specific metrics)\n",
        "METRIC_DISPLAY_NAMES = {\n",
        "    \"MedianRank_first_hit\": \"MedianRank\",\n",
        "    \"MeanRank_first_hit\": \"MeanRank\"\n",
        "}\n",
        "\n",
        "print(\"\\n📊 Text → Image Retrieval:\")\n",
        "for metric, value in text_to_image_results.items():\n",
        "    # Display with shorter names for readability (only for specific metrics)\n",
        "    display_name = METRIC_DISPLAY_NAMES.get(metric, metric)\n",
        "    # Format ranks with 2 decimals, other metrics with 4 decimals\n",
        "    if metric in (\"MedianRank_first_hit\", \"MeanRank_first_hit\"):\n",
        "        print(f\"  {display_name}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {display_name}: {value:.4f}\")\n",
        "\n",
        "print(\"\\n📊 Image → Text Retrieval:\")\n",
        "for metric, value in image_to_text_results.items():\n",
        "    # Display with shorter names for readability (only for specific metrics)\n",
        "    display_name = METRIC_DISPLAY_NAMES.get(metric, metric)\n",
        "    # Format ranks with 2 decimals, other metrics with 4 decimals\n",
        "    if metric in (\"MedianRank_first_hit\", \"MeanRank_first_hit\"):\n",
        "        print(f\"  {display_name}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {display_name}: {value:.4f}\")\n",
        "\n",
        "print(f\"\\n⏱ Evaluation time: {eval_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.5. Visualize Retrieval Results\n",
        "\n",
        "Visualize the retrieval results to see how well the fine-tuned model performs:\n",
        "- **Text → Image**: For each recipe, show the top-5 retrieved images\n",
        "- **Image → Text**: For each image, show the top-5 retrieved captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "print(\"=\" * 60)\n",
        "print(\"Visualizing Retrieval Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verify required variables exist (safety check)\n",
        "assert \"image_key_to_recipe_id\" in globals(), \"image_key_to_recipe_id not found. Run evaluation section first.\"\n",
        "assert \"text_embeddings\" in globals(), \"text_embeddings not found. Run evaluation section first.\"\n",
        "assert \"image_embeddings\" in globals(), \"image_embeddings not found. Run evaluation section first.\"\n",
        "assert \"unique_captions_list\" in globals(), \"unique_captions_list not found. Run evaluation section first.\"\n",
        "assert \"image_keys\" in globals(), \"image_keys not found. Run evaluation section first.\"\n",
        "\n",
        "print(\"\\n📝 Text → Image Retrieval (Top-5 images for each recipe):\")\n",
        "print(\"-\" * 60)\n",
        "visualize_text_to_image_retrieval(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_text,\n",
        "    recipe_to_image_indices,\n",
        "    caption_to_idx,\n",
        "    image_keys,\n",
        "    image_key_to_recipe_id,\n",
        "    n_examples=3\n",
        ")\n",
        "\n",
        "print(\"\\n🖼️ Image → Text Retrieval (Top-5 captions for each image):\")\n",
        "print(\"-\" * 60)\n",
        "visualize_image_to_text_retrieval(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    recipe_to_caption_text,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    unique_captions_list,\n",
        "    n_examples=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Generate Qualitative Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_top5_examples(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    caption_to_idx: Dict[str, int],\n",
        "    image_keys: List[str],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    n_examples: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate qualitative examples showing top-5 retrievals by recipe_id.\n",
        "    \"\"\"\n",
        "    \n",
        "    examples = []\n",
        "    \n",
        "    # Filter recipes with valid ground truth before sampling (avoid bias)\n",
        "    valid_recipe_ids = []\n",
        "    for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        gt_image_indices = recipe_to_image_indices.get(recipe_id_str, [])\n",
        "        if len(gt_image_indices) > 0:  # Only include if has valid GT\n",
        "            valid_recipe_ids.append(recipe_id)\n",
        "    \n",
        "    # Sample random recipes from valid ones\n",
        "    if len(valid_recipe_ids) == 0:\n",
        "        print(\"⚠ No valid recipes with ground truth images found\")\n",
        "        return examples\n",
        "    \n",
        "    sample_recipe_ids = pd.Series(valid_recipe_ids).sample(\n",
        "        min(n_examples, len(valid_recipe_ids)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for recipe_id in sample_recipe_ids:\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption for this recipe\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        \n",
        "        # Get caption embedding\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        \n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        similarities = image_embeddings @ text_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        \n",
        "        # Get ground truth: images from this recipe_id\n",
        "        gt_image_indices = set(recipe_to_image_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Get top-5 retrieved images\n",
        "        top5_images = []\n",
        "        for i, idx in enumerate(top5_indices):\n",
        "            img_key = image_keys[idx]\n",
        "            img_recipe_id = image_key_to_recipe_id.get(img_key)\n",
        "            is_correct = idx in gt_image_indices\n",
        "            \n",
        "            top5_images.append({\n",
        "                \"rank\": i + 1,\n",
        "                \"image_key\": img_key,\n",
        "                \"recipe_id\": img_recipe_id,\n",
        "                \"similarity\": float(similarities[idx]),\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "        \n",
        "        examples.append({\n",
        "            \"query_type\": \"text_to_image\",\n",
        "            \"recipe_id\": recipe_id_str,\n",
        "            \"query_caption\": caption,\n",
        "            \"ground_truth_recipe_id\": recipe_id_str,\n",
        "            \"top5_results\": top5_images\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "def generate_image_to_text_examples(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    unique_captions: List[str],\n",
        "    n_examples: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate qualitative examples showing top-5 captions for images (Image→Text retrieval).\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    \n",
        "    # Filter images with valid ground truth before sampling (avoid bias)\n",
        "    valid_images = []\n",
        "    for img_key in test_df[\"image_key\"].unique():\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        gt_caption_indices = recipe_to_all_caption_indices.get(recipe_id_str, [])\n",
        "        if len(gt_caption_indices) > 0:  # Only include if has valid GT\n",
        "            valid_images.append(img_key)\n",
        "    \n",
        "    # Sample random images from valid ones\n",
        "    if len(valid_images) == 0:\n",
        "        print(\"⚠ No valid images with ground truth captions found\")\n",
        "        return examples\n",
        "    \n",
        "    sample_images = pd.Series(valid_images).sample(\n",
        "        min(n_examples, len(valid_images)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for img_key in sample_images:\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        similarities = text_embeddings @ img_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        \n",
        "        # Get ground truth: recipe_id for this image\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        \n",
        "        # Get ground truth: all captions from this recipe_id\n",
        "        gt_caption_indices = set(recipe_to_all_caption_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Get top-5 retrieved captions\n",
        "        top5_captions = []\n",
        "        for i, idx in enumerate(top5_indices):\n",
        "            caption = unique_captions[idx]\n",
        "            is_correct = idx in gt_caption_indices\n",
        "            \n",
        "            top5_captions.append({\n",
        "                \"rank\": i + 1,\n",
        "                \"caption\": caption,\n",
        "                \"caption_index\": int(idx),\n",
        "                \"similarity\": float(similarities[idx]),\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "        \n",
        "        examples.append({\n",
        "            \"query_type\": \"image_to_text\",\n",
        "            \"recipe_id\": recipe_id_str,\n",
        "            \"query_image_key\": img_key,\n",
        "            \"recipe_representative_caption\": recipe_to_caption_text.get(recipe_id_str),\n",
        "            \"ground_truth_recipe_id\": recipe_id_str,\n",
        "            \"top5_results\": top5_captions\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "# Generate examples\n",
        "N_QUALITATIVE_EXAMPLES = 5\n",
        "print(\"Generating qualitative examples...\")\n",
        "\n",
        "# Verify unique_captions_list exists (should be from evaluation section)\n",
        "assert \"unique_captions_list\" in globals(), \"unique_captions_list not found. Run evaluation section first.\"\n",
        "\n",
        "# Text → Image examples\n",
        "text_to_image_examples = generate_top5_examples(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_text,\n",
        "    recipe_to_image_indices,\n",
        "    caption_to_idx,\n",
        "    image_keys,\n",
        "    image_key_to_recipe_id,\n",
        "    n_examples=N_QUALITATIVE_EXAMPLES\n",
        ")\n",
        "\n",
        "# Image → Text examples\n",
        "image_to_text_examples = generate_image_to_text_examples(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    recipe_to_caption_text,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    unique_captions=unique_captions_list,\n",
        "    n_examples=N_QUALITATIVE_EXAMPLES\n",
        ")\n",
        "\n",
        "print(f\"✓ Generated {len(text_to_image_examples)} text→image examples (requested: {N_QUALITATIVE_EXAMPLES})\")\n",
        "print(f\"✓ Generated {len(image_to_text_examples)} image→text examples (requested: {N_QUALITATIVE_EXAMPLES})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results dictionary\n",
        "results = {\n",
        "    \"metadata\": {\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"method\": \"lora\",\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"device\": str(main_device),\n",
        "        \"train_dataset\": TRAIN_AUGMENTED_KEY,\n",
        "        \"test_dataset\": TEST_MANIFEST_KEY,\n",
        "        \"filter_applied\": \"label == 1 (positive pairs only) for training\",\n",
        "    },\n",
        "    \"model_info\": {\n",
        "        \"total_parameters\": int(total_params),\n",
        "        \"trainable_parameters\": int(trainable_params),\n",
        "        \"trainable_ratio\": float(trainable_params / total_params),\n",
        "        \"frozen\": False,\n",
        "    },\n",
        "    \"training_info\": {\n",
        "        \"total_time_seconds\": training_logs[\"total_time_seconds\"],\n",
        "        \"peak_memory_mb\": training_logs[\"peak_memory_mb\"],\n",
        "        \"total_steps\": training_logs[\"total_steps\"],\n",
        "        \"num_epochs\": TRAINING_CONFIG[\"num_epochs\"],\n",
        "        \"final_loss\": float(training_logs[\"epochs\"][-1][\"avg_loss\"]) if training_logs[\"epochs\"] else None,\n",
        "    },\n",
        "    \"dataset_info\": {\n",
        "        \"test_pairs\": len(test_df),\n",
        "        \"test_recipes\": test_df[\"recipe_id\"].nunique(),\n",
        "        \"test_images\": test_df[\"image_key\"].nunique(),\n",
        "        \"test_captions\": test_df[\"caption\"].nunique(),\n",
        "    },\n",
        "    \"metrics\": {\n",
        "        \"text_to_image\": text_to_image_results,\n",
        "        \"image_to_text\": image_to_text_results,\n",
        "    },\n",
        "    \"timing\": {\n",
        "        \"embedding_generation_seconds\": embedding_time,\n",
        "        \"evaluation_seconds\": eval_time,\n",
        "        \"total_seconds\": embedding_time + eval_time,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Save results\n",
        "print(\"=\" * 60)\n",
        "print(\"Saving Results to MinIO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "save_json_to_minio(results, FINE_TUNING_BUCKET, RESULTS_KEY)\n",
        "save_json_to_minio({\n",
        "    \"text_to_image_examples\": text_to_image_examples,\n",
        "    \"image_to_text_examples\": image_to_text_examples\n",
        "}, FINE_TUNING_BUCKET, EXAMPLES_KEY)\n",
        "\n",
        "print(f\"\\n✅ Results saved successfully!\")\n",
        "print(f\"  Results: s3://{FINE_TUNING_BUCKET}/{RESULTS_KEY}\")\n",
        "print(f\"  Examples: s3://{FINE_TUNING_BUCKET}/{EXAMPLES_KEY}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Run ID: {RUN_ID}\")\n",
        "print(f\"Model: {MODEL_NAME} (LoRA fine-tuned)\")\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
        "print(f\"Training time: {training_time/60:.2f} minutes\")\n",
        "print(f\"Peak memory: {peak_memory:.2f} MB\")\n",
        "print(f\"\\nText → Image:\")\n",
        "print(f\"  R@1: {text_to_image_results.get('R@1', 0):.4f}\")\n",
        "print(f\"  R@5: {text_to_image_results.get('R@5', 0):.4f}\")\n",
        "print(f\"  R@10: {text_to_image_results.get('R@10', 0):.4f}\")\n",
        "if \"MRR\" in text_to_image_results:\n",
        "    print(f\"  MRR: {text_to_image_results['MRR']:.4f}\")\n",
        "print(f\"\\nImage → Text:\")\n",
        "print(f\"  R@1: {image_to_text_results.get('R@1', 0):.4f}\")\n",
        "print(f\"  R@5: {image_to_text_results.get('R@5', 0):.4f}\")\n",
        "print(f\"  R@10: {image_to_text_results.get('R@10', 0):.4f}\")\n",
        "if \"MRR\" in image_to_text_results:\n",
        "    print(f\"  MRR: {image_to_text_results['MRR']:.4f}\")\n",
        "print(f\"\\n📁 All outputs saved to: s3://{FINE_TUNING_BUCKET}/{RUN_DIR}/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_json_to_minio(data: Dict, bucket: str, key: str) -> bool:\n",
        "    \"\"\"Save dictionary as JSON to MinIO.\"\"\"\n",
        "    try:\n",
        "        json_bytes = json.dumps(data, indent=2).encode(\"utf-8\")\n",
        "        s3.put_object(\n",
        "            Bucket=bucket,\n",
        "            Key=key,\n",
        "            Body=json_bytes,\n",
        "            ContentType=\"application/json\",\n",
        "        )\n",
        "        size_kb = len(json_bytes) / 1024\n",
        "        print(f\"✓ Saved to s3://{bucket}/{key} ({size_kb:.1f} KB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to save: {e}\")\n",
        "        return False\n",
        "\n",
        "def save_yaml_to_minio(data: Dict, bucket: str, key: str) -> bool:\n",
        "    \"\"\"Save dictionary as YAML to MinIO.\"\"\"\n",
        "    try:\n",
        "        yaml_str = yaml.dump(data, default_flow_style=False, sort_keys=False)\n",
        "        yaml_bytes = yaml_str.encode(\"utf-8\")\n",
        "        s3.put_object(\n",
        "            Bucket=bucket,\n",
        "            Key=key,\n",
        "            Body=yaml_bytes,\n",
        "            ContentType=\"text/yaml\",\n",
        "        )\n",
        "        size_kb = len(yaml_bytes) / 1024\n",
        "        print(f\"✓ Saved to s3://{bucket}/{key} ({size_kb:.1f} KB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to save: {e}\")\n",
        "        return False\n",
        "\n",
        "# Save adapters locally first, then upload to MinIO\n",
        "print(\"Saving adapters...\")\n",
        "local_adapters_dir = Path(f\"adapters_{RUN_ID}\")\n",
        "model.save_pretrained(str(local_adapters_dir))\n",
        "\n",
        "# Upload adapters to MinIO (upload config and weights - supports both .bin and .safetensors)\n",
        "print(\"Uploading adapters to MinIO...\")\n",
        "adapter_files = [\"adapter_config.json\", \"adapter_model.bin\", \"adapter_model.safetensors\"]\n",
        "for file_name in adapter_files:\n",
        "    file_path = local_adapters_dir / file_name\n",
        "    if file_path.exists():\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            s3.put_object(\n",
        "                Bucket=FINE_TUNING_BUCKET,\n",
        "                Key=f\"{ADAPTERS_DIR}/{file_name}\",\n",
        "                Body=f.read(),\n",
        "            )\n",
        "        print(f\"  ✓ Uploaded {file_name}\")\n",
        "\n",
        "# Save training logs\n",
        "print(\"\\nSaving training logs...\")\n",
        "save_json_to_minio(training_logs, FINE_TUNING_BUCKET, LOGS_KEY)\n",
        "\n",
        "# Save configuration\n",
        "config_data = {\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"method\": \"lora\",\n",
        "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"lora_config\": LORA_CONFIG,\n",
        "    \"training_config\": TRAINING_CONFIG,\n",
        "    \"model_info\": {\n",
        "        \"total_parameters\": int(total_params),\n",
        "        \"trainable_parameters\": int(trainable_params),\n",
        "        \"trainable_ratio\": float(trainable_params / total_params),\n",
        "    },\n",
        "}\n",
        "save_yaml_to_minio(config_data, FINE_TUNING_BUCKET, CONFIG_KEY)\n",
        "\n",
        "print(f\"\\n✅ Adapters and logs saved successfully!\")\n",
        "print(f\"  Adapters: s3://{FINE_TUNING_BUCKET}/{ADAPTERS_DIR}/\")\n",
        "print(f\"  Logs: s3://{FINE_TUNING_BUCKET}/{LOGS_KEY}\")\n",
        "print(f\"  Config: s3://{FINE_TUNING_BUCKET}/{CONFIG_KEY}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluation (Same Protocol as Baseline)\n",
        "\n",
        "Now we evaluate the fine-tuned model using the same evaluation functions as the baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Generate embeddings for test set (same as baseline)\n",
        "print(\"=\" * 60)\n",
        "print(\"Generating Embeddings for Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def generate_image_embeddings(\n",
        "    df: pd.DataFrame, \n",
        "    model, \n",
        "    processor, \n",
        "    device: Union[str, torch.device], \n",
        "    batch_size: int = 32\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"Generate image embeddings for all images in the dataset.\"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    image_keys = []\n",
        "    failed_keys = []\n",
        "    \n",
        "    unique_images = df[\"image_key\"].unique()\n",
        "    print(f\"Generating embeddings for {len(unique_images)} unique images...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(unique_images), batch_size)):\n",
        "            batch_keys = unique_images[i:i+batch_size]\n",
        "            batch_images = []\n",
        "            valid_keys = []\n",
        "            \n",
        "            for img_key in batch_keys:\n",
        "                bucket, key = get_image_path_in_minio(img_key)\n",
        "                img = load_image_from_minio(bucket, key)\n",
        "                if img is not None:\n",
        "                    batch_images.append(img)\n",
        "                    valid_keys.append(img_key)\n",
        "                else:\n",
        "                    failed_keys.append(img_key)\n",
        "            \n",
        "            if not batch_images:\n",
        "                continue\n",
        "            \n",
        "            inputs = processor(images=batch_images, return_tensors=\"pt\", padding=True)\n",
        "            # Move inputs to device explicitly (more robust than .to(device) on BatchEncoding)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            outputs = model.get_image_features(**inputs)\n",
        "            batch_emb = F.normalize(outputs, p=2, dim=1)\n",
        "            \n",
        "            embeddings.append(batch_emb.cpu().numpy())\n",
        "            image_keys.extend(valid_keys)\n",
        "    \n",
        "    if failed_keys:\n",
        "        print(f\"⚠ Failed to load {len(failed_keys)} images\")\n",
        "    \n",
        "    embeddings = np.vstack(embeddings) if embeddings else np.array([])\n",
        "    print(f\"✓ Generated {len(image_keys)} image embeddings (shape: {embeddings.shape})\")\n",
        "    return embeddings, image_keys\n",
        "\n",
        "def generate_text_embeddings(\n",
        "    texts: List[str], \n",
        "    model, \n",
        "    processor, \n",
        "    device: Union[str, torch.device], \n",
        "    batch_size: int = 32\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Generate text embeddings for all captions.\"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    \n",
        "    print(f\"Generating embeddings for {len(texts)} texts...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(texts), batch_size)):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            inputs = processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            # Move inputs to device explicitly (more robust than .to(device) on BatchEncoding)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            outputs = model.get_text_features(**inputs)\n",
        "            batch_emb = F.normalize(outputs, p=2, dim=1)\n",
        "            embeddings.append(batch_emb.cpu().numpy())\n",
        "    \n",
        "    embeddings = np.vstack(embeddings) if embeddings else np.array([])\n",
        "    print(f\"✓ Generated {len(texts)} text embeddings (shape: {embeddings.shape})\")\n",
        "    return embeddings\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Generate embeddings\n",
        "image_embeddings, image_keys = generate_image_embeddings(\n",
        "    test_df, model, processor, main_device, batch_size=16  # Smaller batch for LoRA\n",
        ")\n",
        "\n",
        "unique_captions = test_df[\"caption\"].unique().tolist()\n",
        "text_embeddings = generate_text_embeddings(unique_captions, model, processor, main_device, batch_size=16)\n",
        "\n",
        "# Create official copy to ensure alignment with text_embeddings (never modify this)\n",
        "unique_captions_list = list(unique_captions)  # Explicit copy to prevent accidental modification\n",
        "\n",
        "embedding_time = time.time() - start_time\n",
        "print(f\"\\n✓ Embedding generation complete in {embedding_time:.2f} seconds\")\n",
        "\n",
        "# Create mapping dictionaries\n",
        "image_key_to_idx = {key: idx for idx, key in enumerate(image_keys)}\n",
        "caption_to_idx = {caption: idx for idx, caption in enumerate(unique_captions_list)}\n",
        "\n",
        "# Precompute lookup dictionaries for O(1) access (same as baseline)\n",
        "print(\"\\nPrecomputing lookup dictionaries...\")\n",
        "\n",
        "# recipe_id -> list of image indices\n",
        "recipe_to_image_indices: Dict[str, List[int]] = {}\n",
        "for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "    recipe_id_str = str(recipe_id)\n",
        "    recipe_images = test_df[test_df[\"recipe_id\"] == recipe_id][\"image_key\"].unique()\n",
        "    recipe_to_image_indices[recipe_id_str] = [\n",
        "        image_key_to_idx.get(img_key) \n",
        "        for img_key in recipe_images \n",
        "        if image_key_to_idx.get(img_key) is not None\n",
        "    ]\n",
        "\n",
        "# recipe_id -> list of caption indices (one caption per recipe for text→image query)\n",
        "# Use longest caption as representative (most informative)\n",
        "recipe_to_caption_indices: Dict[str, List[int]] = {}\n",
        "recipe_to_caption_text: Dict[str, str] = {}\n",
        "# recipe_id -> list of ALL caption indices (for image→text ground truth)\n",
        "recipe_to_all_caption_indices: Dict[str, List[int]] = {}\n",
        "for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "    recipe_id_str = str(recipe_id)\n",
        "    recipe_captions = test_df[test_df[\"recipe_id\"] == recipe_id][\"caption\"].unique()\n",
        "    if len(recipe_captions) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Use longest caption as representative (most informative)\n",
        "    # If multiple have same length, use first after sorting alphabetically (deterministic)\n",
        "    representative_caption = max(recipe_captions, key=lambda c: (len(c), c))\n",
        "    \n",
        "    # Store representative caption for text→image queries\n",
        "    cap_idx = caption_to_idx.get(representative_caption)\n",
        "    if cap_idx is not None:\n",
        "        recipe_to_caption_indices[recipe_id_str] = [cap_idx]\n",
        "        recipe_to_caption_text[recipe_id_str] = representative_caption\n",
        "    \n",
        "    # Store ALL captions for image→text ground truth (more fair if multiple captions per recipe)\n",
        "    all_caption_indices = []\n",
        "    for caption in recipe_captions:\n",
        "        cap_idx = caption_to_idx.get(caption)\n",
        "        if cap_idx is not None:\n",
        "            all_caption_indices.append(cap_idx)\n",
        "    recipe_to_all_caption_indices[recipe_id_str] = sorted(set(all_caption_indices))  # Remove duplicates, sorted for determinism\n",
        "\n",
        "# image_key -> recipe_id (as string for consistency)\n",
        "image_key_to_recipe_id: Dict[str, str] = {}\n",
        "for _, row in test_df.iterrows():\n",
        "    if row[\"image_key\"] not in image_key_to_recipe_id:\n",
        "        image_key_to_recipe_id[row[\"image_key\"]] = str(row[\"recipe_id\"])\n",
        "\n",
        "print(f\"✓ Precomputed lookups for {len(recipe_to_image_indices)} recipes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Evaluation Functions (Same as Baseline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy evaluation functions from baseline (same protocol by recipe_id)\n",
        "def compute_recall_at_k(scores: np.ndarray, ground_truth_indices: List[int], k: int) -> float:\n",
        "    \"\"\"\n",
        "    Compute Recall@K.\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "        k: top-K to consider\n",
        "    \n",
        "    Returns:\n",
        "        recall@k: 1.0 if any ground truth is in top-K, else 0.0\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    top_k_indices = np.argsort(scores)[-k:][::-1]  # Top K, highest first\n",
        "    top_k_set = set(top_k_indices)\n",
        "    \n",
        "    # Check if any ground truth is in top-K\n",
        "    for gt_idx in ground_truth_indices:\n",
        "        if gt_idx in top_k_set:\n",
        "            return 1.0\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "def compute_mrr(scores: np.ndarray, ground_truth_indices: List[int]) -> float:\n",
        "    \"\"\"\n",
        "    Compute Mean Reciprocal Rank.\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "    \n",
        "    Returns:\n",
        "        mrr: 1/rank of first correct result, or 0.0 if none found\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Sort by score (descending)\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    \n",
        "    # Find rank of first ground truth\n",
        "    for rank, idx in enumerate(sorted_indices, start=1):\n",
        "        if idx in ground_truth_indices:\n",
        "            return 1.0 / rank\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "def compute_first_hit_rank(scores: np.ndarray, ground_truth_indices: List[int]) -> float:\n",
        "    \"\"\"\n",
        "    Compute rank of first correct result (first hit rank).\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "    \n",
        "    Returns:\n",
        "        first_hit_rank: rank of first correct result (1-indexed), or len(scores)+1 if not found\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return float(len(scores) + 1)\n",
        "    \n",
        "    # Sort by score (descending)\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    \n",
        "    # Find rank of first ground truth\n",
        "    for rank, idx in enumerate(sorted_indices, start=1):\n",
        "        if idx in ground_truth_indices:\n",
        "            return float(rank)\n",
        "    \n",
        "    return float(len(scores) + 1)  # Not found\n",
        "\n",
        "def evaluate_text_to_image_by_recipe(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    k_values: List[int],\n",
        "    should_compute_mrr: bool = True,\n",
        "    should_compute_median_rank: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate Text → Image retrieval by recipe_id.\n",
        "    \n",
        "    For each recipe_id, use its representative caption to retrieve images.\n",
        "    Ground truth: all images from that recipe_id.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating Text → Image retrieval (by recipe_id)...\")\n",
        "    \n",
        "    unique_recipe_ids = test_df[\"recipe_id\"].unique()\n",
        "    \n",
        "    recalls = {f\"R@{k}\": [] for k in k_values}\n",
        "    mrr_scores = [] if should_compute_mrr else None\n",
        "    median_ranks = [] if should_compute_median_rank else None\n",
        "    \n",
        "    for recipe_id in tqdm(unique_recipe_ids, desc=\"Processing recipes\"):\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption embedding (first caption for this recipe)\n",
        "        caption_indices = recipe_to_caption_indices.get(recipe_id_str, [])\n",
        "        if not caption_indices:\n",
        "            continue\n",
        "        \n",
        "        # Use first caption (or could average multiple)\n",
        "        text_idx = caption_indices[0]\n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        \n",
        "        # Compute similarities with all images\n",
        "        similarities = image_embeddings @ text_emb  # (n_images,)\n",
        "        \n",
        "        # Get ground truth: all images from this recipe_id\n",
        "        ground_truth_indices = recipe_to_image_indices.get(recipe_id_str, [])\n",
        "        ground_truth_indices = [idx for idx in ground_truth_indices if idx is not None]\n",
        "        \n",
        "        if len(ground_truth_indices) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Compute metrics\n",
        "        for k in k_values:\n",
        "            recall = compute_recall_at_k(similarities, ground_truth_indices, k)\n",
        "            recalls[f\"R@{k}\"].append(recall)\n",
        "        \n",
        "        if should_compute_mrr:\n",
        "            mrr = compute_mrr(similarities, ground_truth_indices)\n",
        "            mrr_scores.append(mrr)\n",
        "        \n",
        "        if should_compute_median_rank:\n",
        "            first_hit_rank = compute_first_hit_rank(similarities, ground_truth_indices)\n",
        "            median_ranks.append(first_hit_rank)\n",
        "    \n",
        "    # Average metrics\n",
        "    results = {}\n",
        "    for k in k_values:\n",
        "        results[f\"R@{k}\"] = float(np.mean(recalls[f\"R@{k}\"])) if recalls[f\"R@{k}\"] else 0.0\n",
        "    \n",
        "    if should_compute_mrr and mrr_scores:\n",
        "        results[\"MRR\"] = float(np.mean(mrr_scores))\n",
        "    \n",
        "    if should_compute_median_rank and median_ranks:\n",
        "        results[\"MedianRank_first_hit\"] = float(np.median(median_ranks))\n",
        "        results[\"MeanRank_first_hit\"] = float(np.mean(median_ranks))\n",
        "    \n",
        "    return results\n",
        "\n",
        "def evaluate_image_to_text_by_recipe(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    k_values: List[int],\n",
        "    should_compute_mrr: bool = True,\n",
        "    should_compute_median_rank: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate Image → Text retrieval by recipe_id.\n",
        "    \n",
        "    For each image, retrieve captions. Ground truth: ALL captions from the same recipe_id.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating Image → Text retrieval (by recipe_id)...\")\n",
        "    \n",
        "    unique_images = test_df[\"image_key\"].unique()\n",
        "    \n",
        "    recalls = {f\"R@{k}\": [] for k in k_values}\n",
        "    mrr_scores = [] if should_compute_mrr else None\n",
        "    median_ranks = [] if should_compute_median_rank else None\n",
        "    \n",
        "    for img_key in tqdm(unique_images, desc=\"Processing images\"):\n",
        "        # Get image embedding\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        \n",
        "        # Compute similarities with all captions\n",
        "        similarities = text_embeddings @ img_emb  # (n_texts,)\n",
        "        \n",
        "        # Get recipe_id for this image\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        \n",
        "        # Get ground truth: ALL captions from the same recipe_id (more fair)\n",
        "        ground_truth_indices = recipe_to_all_caption_indices.get(recipe_id_str, [])\n",
        "        ground_truth_indices = [idx for idx in ground_truth_indices if idx is not None]\n",
        "        \n",
        "        if len(ground_truth_indices) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Compute metrics\n",
        "        for k in k_values:\n",
        "            recall = compute_recall_at_k(similarities, ground_truth_indices, k)\n",
        "            recalls[f\"R@{k}\"].append(recall)\n",
        "        \n",
        "        if should_compute_mrr:\n",
        "            mrr = compute_mrr(similarities, ground_truth_indices)\n",
        "            mrr_scores.append(mrr)\n",
        "        \n",
        "        if should_compute_median_rank:\n",
        "            first_hit_rank = compute_first_hit_rank(similarities, ground_truth_indices)\n",
        "            median_ranks.append(first_hit_rank)\n",
        "    \n",
        "    # Average metrics\n",
        "    results = {}\n",
        "    for k in k_values:\n",
        "        results[f\"R@{k}\"] = float(np.mean(recalls[f\"R@{k}\"])) if recalls[f\"R@{k}\"] else 0.0\n",
        "    \n",
        "    if should_compute_mrr and mrr_scores:\n",
        "        results[\"MRR\"] = float(np.mean(mrr_scores))\n",
        "    \n",
        "    if should_compute_median_rank and median_ranks:\n",
        "        results[\"MedianRank_first_hit\"] = float(np.median(median_ranks))\n",
        "        results[\"MeanRank_first_hit\"] = float(np.mean(median_ranks))\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run evaluation\n",
        "print(\"=\" * 60)\n",
        "print(\"Running Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "eval_start_time = time.time()\n",
        "\n",
        "# Text → Image retrieval (by recipe_id)\n",
        "text_to_image_results = evaluate_text_to_image_by_recipe(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_indices,\n",
        "    recipe_to_image_indices,\n",
        "    image_key_to_idx,\n",
        "    K_VALUES,\n",
        "    COMPUTE_MRR,\n",
        "    should_compute_median_rank=True\n",
        ")\n",
        "\n",
        "# Image → Text retrieval (by recipe_id)\n",
        "image_to_text_results = evaluate_image_to_text_by_recipe(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    K_VALUES,\n",
        "    COMPUTE_MRR,\n",
        "    should_compute_median_rank=True\n",
        ")\n",
        "\n",
        "eval_time = time.time() - eval_start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Evaluation Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Mapping for display names (only for specific metrics)\n",
        "METRIC_DISPLAY_NAMES = {\n",
        "    \"MedianRank_first_hit\": \"MedianRank\",\n",
        "    \"MeanRank_first_hit\": \"MeanRank\"\n",
        "}\n",
        "\n",
        "print(\"\\n📊 Text → Image Retrieval:\")\n",
        "for metric, value in text_to_image_results.items():\n",
        "    # Display with shorter names for readability (only for specific metrics)\n",
        "    display_name = METRIC_DISPLAY_NAMES.get(metric, metric)\n",
        "    # Format ranks with 2 decimals, other metrics with 4 decimals\n",
        "    if metric in (\"MedianRank_first_hit\", \"MeanRank_first_hit\"):\n",
        "        print(f\"  {display_name}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {display_name}: {value:.4f}\")\n",
        "\n",
        "print(\"\\n📊 Image → Text Retrieval:\")\n",
        "for metric, value in image_to_text_results.items():\n",
        "    # Display with shorter names for readability (only for specific metrics)\n",
        "    display_name = METRIC_DISPLAY_NAMES.get(metric, metric)\n",
        "    # Format ranks with 2 decimals, other metrics with 4 decimals\n",
        "    if metric in (\"MedianRank_first_hit\", \"MeanRank_first_hit\"):\n",
        "        print(f\"  {display_name}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {display_name}: {value:.4f}\")\n",
        "\n",
        "print(f\"\\n⏱ Evaluation time: {eval_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.5. Visualize Retrieval Results\n",
        "\n",
        "Visualize the retrieval results to see how well the fine-tuned model performs:\n",
        "- **Text → Image**: For each recipe, show the top-5 retrieved images\n",
        "- **Image → Text**: For each image, show the top-5 retrieved captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "print(\"=\" * 60)\n",
        "print(\"Visualizing Retrieval Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verify required variables exist (safety check)\n",
        "assert \"image_key_to_recipe_id\" in globals(), \"image_key_to_recipe_id not found. Run evaluation section first.\"\n",
        "assert \"text_embeddings\" in globals(), \"text_embeddings not found. Run evaluation section first.\"\n",
        "assert \"image_embeddings\" in globals(), \"image_embeddings not found. Run evaluation section first.\"\n",
        "assert \"unique_captions_list\" in globals(), \"unique_captions_list not found. Run evaluation section first.\"\n",
        "assert \"image_keys\" in globals(), \"image_keys not found. Run evaluation section first.\"\n",
        "\n",
        "print(\"\\n📝 Text → Image Retrieval (Top-5 images for each recipe):\")\n",
        "print(\"-\" * 60)\n",
        "visualize_text_to_image_retrieval(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_text,\n",
        "    recipe_to_image_indices,\n",
        "    caption_to_idx,\n",
        "    image_keys,\n",
        "    image_key_to_recipe_id,\n",
        "    n_examples=3\n",
        ")\n",
        "\n",
        "print(\"\\n🖼️ Image → Text Retrieval (Top-5 captions for each image):\")\n",
        "print(\"-\" * 60)\n",
        "visualize_image_to_text_retrieval(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    recipe_to_caption_text,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    unique_captions_list,\n",
        "    n_examples=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Generate Qualitative Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_top5_examples(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    caption_to_idx: Dict[str, int],\n",
        "    image_keys: List[str],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    n_examples: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate qualitative examples showing top-5 retrievals by recipe_id.\n",
        "    \"\"\"\n",
        "    \n",
        "    examples = []\n",
        "    \n",
        "    # Filter recipes with valid ground truth before sampling (avoid bias)\n",
        "    valid_recipe_ids = []\n",
        "    for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        gt_image_indices = recipe_to_image_indices.get(recipe_id_str, [])\n",
        "        if len(gt_image_indices) > 0:  # Only include if has valid GT\n",
        "            valid_recipe_ids.append(recipe_id)\n",
        "    \n",
        "    # Sample random recipes from valid ones\n",
        "    if len(valid_recipe_ids) == 0:\n",
        "        print(\"⚠ No valid recipes with ground truth images found\")\n",
        "        return examples\n",
        "    \n",
        "    sample_recipe_ids = pd.Series(valid_recipe_ids).sample(\n",
        "        min(n_examples, len(valid_recipe_ids)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for recipe_id in sample_recipe_ids:\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption for this recipe\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        \n",
        "        # Get caption embedding\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        \n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        similarities = image_embeddings @ text_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        \n",
        "        # Get ground truth: images from this recipe_id\n",
        "        gt_image_indices = set(recipe_to_image_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Get top-5 retrieved images\n",
        "        top5_images = []\n",
        "        for i, idx in enumerate(top5_indices):\n",
        "            img_key = image_keys[idx]\n",
        "            img_recipe_id = image_key_to_recipe_id.get(img_key)\n",
        "            is_correct = idx in gt_image_indices\n",
        "            \n",
        "            top5_images.append({\n",
        "                \"rank\": i + 1,\n",
        "                \"image_key\": img_key,\n",
        "                \"recipe_id\": img_recipe_id,\n",
        "                \"similarity\": float(similarities[idx]),\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "        \n",
        "        examples.append({\n",
        "            \"query_type\": \"text_to_image\",\n",
        "            \"recipe_id\": recipe_id_str,\n",
        "            \"query_caption\": caption,\n",
        "            \"ground_truth_recipe_id\": recipe_id_str,\n",
        "            \"top5_results\": top5_images\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "def generate_image_to_text_examples(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    unique_captions: List[str],\n",
        "    n_examples: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate qualitative examples showing top-5 captions for images (Image→Text retrieval).\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    \n",
        "    # Filter images with valid ground truth before sampling (avoid bias)\n",
        "    valid_images = []\n",
        "    for img_key in test_df[\"image_key\"].unique():\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        gt_caption_indices = recipe_to_all_caption_indices.get(recipe_id_str, [])\n",
        "        if len(gt_caption_indices) > 0:  # Only include if has valid GT\n",
        "            valid_images.append(img_key)\n",
        "    \n",
        "    # Sample random images from valid ones\n",
        "    if len(valid_images) == 0:\n",
        "        print(\"⚠ No valid images with ground truth captions found\")\n",
        "        return examples\n",
        "    \n",
        "    sample_images = pd.Series(valid_images).sample(\n",
        "        min(n_examples, len(valid_images)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for img_key in sample_images:\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        similarities = text_embeddings @ img_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        \n",
        "        # Get ground truth: recipe_id for this image\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        \n",
        "        # Get ground truth: all captions from this recipe_id\n",
        "        gt_caption_indices = set(recipe_to_all_caption_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Get top-5 retrieved captions\n",
        "        top5_captions = []\n",
        "        for i, idx in enumerate(top5_indices):\n",
        "            caption = unique_captions[idx]\n",
        "            is_correct = idx in gt_caption_indices\n",
        "            \n",
        "            top5_captions.append({\n",
        "                \"rank\": i + 1,\n",
        "                \"caption\": caption,\n",
        "                \"caption_index\": int(idx),\n",
        "                \"similarity\": float(similarities[idx]),\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "        \n",
        "        examples.append({\n",
        "            \"query_type\": \"image_to_text\",\n",
        "            \"recipe_id\": recipe_id_str,\n",
        "            \"query_image_key\": img_key,\n",
        "            \"recipe_representative_caption\": recipe_to_caption_text.get(recipe_id_str),\n",
        "            \"ground_truth_recipe_id\": recipe_id_str,\n",
        "            \"top5_results\": top5_captions\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "# Generate examples\n",
        "N_QUALITATIVE_EXAMPLES = 5\n",
        "print(\"Generating qualitative examples...\")\n",
        "\n",
        "# Verify unique_captions_list exists (should be from evaluation section)\n",
        "assert \"unique_captions_list\" in globals(), \"unique_captions_list not found. Run evaluation section first.\"\n",
        "\n",
        "# Text → Image examples\n",
        "text_to_image_examples = generate_top5_examples(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_text,\n",
        "    recipe_to_image_indices,\n",
        "    caption_to_idx,\n",
        "    image_keys,\n",
        "    image_key_to_recipe_id,\n",
        "    n_examples=N_QUALITATIVE_EXAMPLES\n",
        ")\n",
        "\n",
        "# Image → Text examples\n",
        "image_to_text_examples = generate_image_to_text_examples(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    recipe_to_caption_text,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    unique_captions=unique_captions_list,\n",
        "    n_examples=N_QUALITATIVE_EXAMPLES\n",
        ")\n",
        "\n",
        "print(f\"✓ Generated {len(text_to_image_examples)} text→image examples (requested: {N_QUALITATIVE_EXAMPLES})\")\n",
        "print(f\"✓ Generated {len(image_to_text_examples)} image→text examples (requested: {N_QUALITATIVE_EXAMPLES})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results dictionary\n",
        "results = {\n",
        "    \"metadata\": {\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"method\": \"lora\",\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"device\": str(main_device),\n",
        "        \"train_dataset\": TRAIN_AUGMENTED_KEY,\n",
        "        \"test_dataset\": TEST_MANIFEST_KEY,\n",
        "        \"filter_applied\": \"label == 1 (positive pairs only) for training\",\n",
        "    },\n",
        "    \"model_info\": {\n",
        "        \"total_parameters\": int(total_params),\n",
        "        \"trainable_parameters\": int(trainable_params),\n",
        "        \"trainable_ratio\": float(trainable_params / total_params),\n",
        "        \"frozen\": False,\n",
        "    },\n",
        "    \"training_info\": {\n",
        "        \"total_time_seconds\": training_logs[\"total_time_seconds\"],\n",
        "        \"peak_memory_mb\": training_logs[\"peak_memory_mb\"],\n",
        "        \"total_steps\": training_logs[\"total_steps\"],\n",
        "        \"num_epochs\": TRAINING_CONFIG[\"num_epochs\"],\n",
        "        \"final_loss\": float(training_logs[\"epochs\"][-1][\"avg_loss\"]) if training_logs[\"epochs\"] else None,\n",
        "    },\n",
        "    \"dataset_info\": {\n",
        "        \"test_pairs\": len(test_df),\n",
        "        \"test_recipes\": test_df[\"recipe_id\"].nunique(),\n",
        "        \"test_images\": test_df[\"image_key\"].nunique(),\n",
        "        \"test_captions\": test_df[\"caption\"].nunique(),\n",
        "    },\n",
        "    \"metrics\": {\n",
        "        \"text_to_image\": text_to_image_results,\n",
        "        \"image_to_text\": image_to_text_results,\n",
        "    },\n",
        "    \"timing\": {\n",
        "        \"embedding_generation_seconds\": embedding_time,\n",
        "        \"evaluation_seconds\": eval_time,\n",
        "        \"total_seconds\": embedding_time + eval_time,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Save results\n",
        "print(\"=\" * 60)\n",
        "print(\"Saving Results to MinIO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "save_json_to_minio(results, FINE_TUNING_BUCKET, RESULTS_KEY)\n",
        "save_json_to_minio({\n",
        "    \"text_to_image_examples\": text_to_image_examples,\n",
        "    \"image_to_text_examples\": image_to_text_examples\n",
        "}, FINE_TUNING_BUCKET, EXAMPLES_KEY)\n",
        "\n",
        "print(f\"\\n✅ Results saved successfully!\")\n",
        "print(f\"  Results: s3://{FINE_TUNING_BUCKET}/{RESULTS_KEY}\")\n",
        "print(f\"  Examples: s3://{FINE_TUNING_BUCKET}/{EXAMPLES_KEY}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Run ID: {RUN_ID}\")\n",
        "print(f\"Model: {MODEL_NAME} (LoRA fine-tuned)\")\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
        "print(f\"Training time: {training_time/60:.2f} minutes\")\n",
        "print(f\"Peak memory: {peak_memory:.2f} MB\")\n",
        "print(f\"\\nText → Image:\")\n",
        "print(f\"  R@1: {text_to_image_results.get('R@1', 0):.4f}\")\n",
        "print(f\"  R@5: {text_to_image_results.get('R@5', 0):.4f}\")\n",
        "print(f\"  R@10: {text_to_image_results.get('R@10', 0):.4f}\")\n",
        "if \"MRR\" in text_to_image_results:\n",
        "    print(f\"  MRR: {text_to_image_results['MRR']:.4f}\")\n",
        "print(f\"\\nImage → Text:\")\n",
        "print(f\"  R@1: {image_to_text_results.get('R@1', 0):.4f}\")\n",
        "print(f\"  R@5: {image_to_text_results.get('R@5', 0):.4f}\")\n",
        "print(f\"  R@10: {image_to_text_results.get('R@10', 0):.4f}\")\n",
        "if \"MRR\" in image_to_text_results:\n",
        "    print(f\"  MRR: {image_to_text_results['MRR']:.4f}\")\n",
        "print(f\"\\n📁 All outputs saved to: s3://{FINE_TUNING_BUCKET}/{RUN_DIR}/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize MinIO Client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize S3/MinIO client\n",
        "session = boto3.session.Session(\n",
        "    aws_access_key_id=MINIO_USER,\n",
        "    aws_secret_access_key=MINIO_PASSWORD,\n",
        "    region_name=\"us-east-1\"\n",
        ")\n",
        "s3 = session.client(\n",
        "    \"s3\",\n",
        "    endpoint_url=MINIO_ENDPOINT,\n",
        "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"})\n",
        ")\n",
        "\n",
        "def ensure_bucket_exists(bucket: str) -> bool:\n",
        "    \"\"\"Create bucket if it doesn't exist.\"\"\"\n",
        "    try:\n",
        "        s3.head_bucket(Bucket=bucket)\n",
        "        return True\n",
        "    except ClientError as e:\n",
        "        error_code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
        "        if error_code in (\"404\", \"NoSuchBucket\"):\n",
        "            try:\n",
        "                s3.create_bucket(Bucket=bucket)\n",
        "                print(f\"✓ Created bucket '{bucket}'\")\n",
        "                return True\n",
        "            except ClientError as create_error:\n",
        "                print(f\"✗ Failed to create bucket '{bucket}': {create_error}\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "# Verify buckets\n",
        "print(\"Checking buckets...\")\n",
        "ensure_bucket_exists(FINE_TUNING_BUCKET)\n",
        "print(\"✓ Buckets ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Training and Test Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_csv_from_minio(bucket: str, key: str) -> pd.DataFrame:\n",
        "    \"\"\"Load CSV file from MinIO into a DataFrame.\"\"\"\n",
        "    try:\n",
        "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "        df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
        "        print(f\"✓ Loaded {len(df)} rows from s3://{bucket}/{key}\")\n",
        "        return df\n",
        "    except ClientError as e:\n",
        "        print(f\"✗ Failed to load s3://{bucket}/{key}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Load training dataset (augmented, with negatives)\n",
        "print(\"Loading augmented training dataset...\")\n",
        "train_full_df = load_csv_from_minio(FINE_TUNING_BUCKET, TRAIN_AUGMENTED_KEY)\n",
        "\n",
        "if train_full_df.empty:\n",
        "    raise RuntimeError(f\"Could not load training data from s3://{FINE_TUNING_BUCKET}/{TRAIN_AUGMENTED_KEY}\")\n",
        "\n",
        "print(f\"\\nFull training dataset shape: {train_full_df.shape}\")\n",
        "print(f\"Columns: {list(train_full_df.columns)}\")\n",
        "if \"label\" in train_full_df.columns:\n",
        "    print(f\"\\nLabel distribution:\")\n",
        "    print(train_full_df[\"label\"].value_counts())\n",
        "    \n",
        "    # Filter only positive pairs (label=1)\n",
        "    train_df = train_full_df[train_full_df[\"label\"] == 1].copy()\n",
        "    print(f\"\\n✓ Filtered to {len(train_df)} positive pairs (label=1)\")\n",
        "    print(f\"  Removed {len(train_full_df) - len(train_df)} negative pairs\")\n",
        "else:\n",
        "    # If no label column, assume all are positive\n",
        "    train_df = train_full_df.copy()\n",
        "    print(f\"\\n✓ No label column found, using all {len(train_df)} pairs as positive\")\n",
        "\n",
        "# Load test dataset (from baseline split, already filtered)\n",
        "print(\"\\nLoading test dataset...\")\n",
        "test_df = load_csv_from_minio(FINE_TUNING_BUCKET, TEST_MANIFEST_KEY)\n",
        "\n",
        "if test_df.empty:\n",
        "    raise RuntimeError(f\"Could not load test data from s3://{FINE_TUNING_BUCKET}/{TEST_MANIFEST_KEY}\")\n",
        "\n",
        "print(f\"\\nTraining dataset:\")\n",
        "print(f\"  Pairs: {len(train_df)}\")\n",
        "print(f\"  Recipes: {train_df['recipe_id'].nunique()}\")\n",
        "print(f\"  Images: {train_df['image_key'].nunique()}\")\n",
        "print(f\"  Captions: {train_df['caption'].nunique()}\")\n",
        "\n",
        "print(f\"\\nTest dataset:\")\n",
        "print(f\"  Pairs: {len(test_df)}\")\n",
        "print(f\"  Recipes: {test_df['recipe_id'].nunique()}\")\n",
        "print(f\"  Images: {test_df['image_key'].nunique()}\")\n",
        "print(f\"  Captions: {test_df['caption'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Helper Functions for Image Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image_from_minio(bucket: str, key: str) -> Optional[Image.Image]:\n",
        "    \"\"\"Load an image from MinIO.\"\"\"\n",
        "    try:\n",
        "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "        img = Image.open(io.BytesIO(obj[\"Body\"].read()))\n",
        "        img.load()\n",
        "        return img.convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# Cache for image paths to avoid head_object per sample (performance optimization)\n",
        "_image_path_cache: Dict[str, Tuple[str, str]] = {}\n",
        "\n",
        "def build_image_path_cache(df: pd.DataFrame) -> Dict[str, Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Pre-build cache of image_key -> (bucket, full_key) to avoid head_object calls during training.\n",
        "    This significantly improves throughput when loading images from MinIO.\n",
        "    \"\"\"\n",
        "    print(\"Building image path cache...\")\n",
        "    cache = {}\n",
        "    bucket = FINE_TUNING_BUCKET\n",
        "    unique_keys = df[\"image_key\"].unique()\n",
        "    \n",
        "    for image_key in tqdm(unique_keys, desc=\"Caching image paths\"):\n",
        "        # Check if it's already a full path\n",
        "        if image_key.startswith(\"images/\") or image_key.startswith(\"augmented_images/\"):\n",
        "            cache[image_key] = (bucket, image_key)\n",
        "        else:\n",
        "            # Try images/ first\n",
        "            key = f\"{IMAGES_PREFIX}/{image_key}\"\n",
        "            # Check if exists, if not try augmented_images/\n",
        "            try:\n",
        "                s3.head_object(Bucket=bucket, Key=key)\n",
        "                cache[image_key] = (bucket, key)\n",
        "            except ClientError as e:\n",
        "                if e.response.get(\"Error\", {}).get(\"Code\") == \"404\":\n",
        "                    # Try augmented_images/\n",
        "                    key = f\"augmented_images/{image_key}\"\n",
        "                    cache[image_key] = (bucket, key)\n",
        "                else:\n",
        "                    raise\n",
        "    \n",
        "    print(f\"✓ Cached {len(cache)} image paths\")\n",
        "    return cache\n",
        "\n",
        "def get_image_path_in_minio(image_key: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Determine bucket and full key for an image (uses cache if available).\n",
        "    \n",
        "    Images can be in:\n",
        "    - fine-tuning-zone/images/...\n",
        "    - fine-tuning-zone/augmented_images/...\n",
        "    \n",
        "    If cache is built, uses it. Otherwise falls back to head_object (slower).\n",
        "    \"\"\"\n",
        "    # Use cache if available\n",
        "    if image_key in _image_path_cache:\n",
        "        return _image_path_cache[image_key]\n",
        "    \n",
        "    # Fallback: check if it's already a full path\n",
        "    bucket = FINE_TUNING_BUCKET\n",
        "    if image_key.startswith(\"images/\") or image_key.startswith(\"augmented_images/\"):\n",
        "        key = image_key\n",
        "    else:\n",
        "        # Try images/ first\n",
        "        key = f\"{IMAGES_PREFIX}/{image_key}\"\n",
        "        # Check if exists, if not try augmented_images/\n",
        "        try:\n",
        "            s3.head_object(Bucket=bucket, Key=key)\n",
        "        except ClientError as e:\n",
        "            if e.response.get(\"Error\", {}).get(\"Code\") == \"404\":\n",
        "                # Try augmented_images/\n",
        "                key = f\"augmented_images/{image_key}\"\n",
        "            else:\n",
        "                raise\n",
        "    \n",
        "    return bucket, key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load CLIP Model with LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not TRANSFORMERS_AVAILABLE or not PEFT_AVAILABLE:\n",
        "    raise ImportError(\"Required libraries not available. Install: pip install transformers peft\")\n",
        "\n",
        "print(f\"Loading CLIP model with LoRA: {MODEL_NAME}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "model = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "\n",
        "# Load processor\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(\"Model ready for training (LoRA)\")\n",
        "\n",
        "# Validate target_modules match actual CLIP module names (guardrail)\n",
        "print(\"\\nValidating target_modules against actual CLIP architecture...\")\n",
        "all_module_names = [name for name, _ in model.named_modules()]\n",
        "target_modules = LORA_CONFIG[\"target_modules\"]\n",
        "matched_modules = []\n",
        "for target in target_modules:\n",
        "    matches = [name for name in all_module_names if target in name]\n",
        "    if matches:\n",
        "        matched_modules.extend(matches)\n",
        "        print(f\"  ✓ '{target}' matches: {matches[:3]}...\")  # Show first 3\n",
        "    else:\n",
        "        print(f\"  ⚠ '{target}' not found in model modules!\")\n",
        "\n",
        "if not matched_modules:\n",
        "    print(\"\\n⚠ WARNING: No target_modules matched! LoRA may not be applied.\")\n",
        "    print(\"Available attention-related modules (sample):\")\n",
        "    attn_modules = [name for name in all_module_names if \"attn\" in name.lower() or \"proj\" in name.lower()]\n",
        "    for mod in attn_modules[:10]:  # Show first 10\n",
        "        print(f\"  - {mod}\")\n",
        "    print(\"  ... (use these to adjust target_modules)\")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_CONFIG[\"r\"],\n",
        "    lora_alpha=LORA_CONFIG[\"lora_alpha\"],\n",
        "    lora_dropout=LORA_CONFIG[\"lora_dropout\"],\n",
        "    bias=LORA_CONFIG[\"bias\"],\n",
        "    task_type=LORA_CONFIG[\"task_type\"],\n",
        "    target_modules=LORA_CONFIG[\"target_modules\"],\n",
        ")\n",
        "\n",
        "# Apply LoRA adapters\n",
        "print(\"Applying LoRA adapters...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Model Information\")\n",
        "print(\"=\" * 60)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Check logit_scale trainability\n",
        "print(f\"\\nlogit_scale requires_grad: {model.logit_scale.requires_grad}\")\n",
        "if not model.logit_scale.requires_grad:\n",
        "    print(\"⚠ logit_scale is frozen. Enabling it for better fine-tuning (1 extra parameter, almost free).\")\n",
        "    model.logit_scale.requires_grad_(True)\n",
        "    print(\"✓ logit_scale is now trainable\")\n",
        "\n",
        "# Get model info\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n✓ Model loaded successfully\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  Trainable ratio: {trainable_params/total_params*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create DataLoader for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "class CLIPDataset(Dataset):\n",
        "    \"\"\"Dataset for CLIP training with image-text pairs.\"\"\"\n",
        "    \n",
        "    def __init__(self, df: pd.DataFrame, processor, s3_client, bucket: str):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.s3 = s3_client\n",
        "        self.bucket = bucket\n",
        "        self.failed_image_count = 0\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        \n",
        "        # Load image\n",
        "        bucket, key = get_image_path_in_minio(row[\"image_key\"])\n",
        "        img = load_image_from_minio(bucket, key)\n",
        "        \n",
        "        # Get caption\n",
        "        caption = row[\"caption\"]\n",
        "        \n",
        "        if img is None:\n",
        "            # Return a placeholder if image fails to load\n",
        "            self.failed_image_count += 1\n",
        "            return {\"image\": None, \"text\": caption, \"is_valid\": False}\n",
        "        \n",
        "        return {\n",
        "            \"image\": img,\n",
        "            \"text\": caption, \n",
        "            \"is_valid\": True\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for DataLoader.\"\"\"\n",
        "    # Filter out invalid samples (images that failed to load)\n",
        "    batch = [b for b in batch if b.get(\"is_valid\", True) and b[\"image\"] is not None]\n",
        "    \n",
        "    if len(batch) == 0:\n",
        "        return None  # Signal to skip this batch\n",
        "    \n",
        "    images = [item[\"image\"] for item in batch]\n",
        "    texts = [item[\"text\"] for item in batch]\n",
        "    \n",
        "    # Process with CLIP processor\n",
        "    inputs = processor(\n",
        "        text=texts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "    )\n",
        "    \n",
        "    # Filter to only include what CLIPModel.forward() accepts\n",
        "    # CLIPModel expects: input_ids, attention_mask, pixel_values\n",
        "    allowed_keys = {\"input_ids\", \"attention_mask\", \"pixel_values\"}\n",
        "    return {k: v for k, v in inputs.items() if k in allowed_keys}\n",
        "\n",
        "# Build image path cache (performance optimization: avoid head_object per sample)\n",
        "print(\"Building image path cache for training set...\")\n",
        "_image_path_cache.update(build_image_path_cache(train_df))\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "print(\"\\nCreating datasets and dataloaders...\")\n",
        "\n",
        "train_dataset = CLIPDataset(train_df, processor, s3, FINE_TUNING_BUCKET)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,  # Set to 0 to avoid issues with MinIO\n",
        ")\n",
        "\n",
        "print(f\"✓ Created training dataloader\")\n",
        "print(f\"  Total batches: {len(train_dataloader)}\")\n",
        "print(f\"  Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
        "print(f\"  Gradient accumulation steps: {TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
        "print(f\"  Effective batch size: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.5. Visualize Retrieval Results\n",
        "\n",
        "Visualize the retrieval results to see how well the fine-tuned model performs:\n",
        "- **Text → Image**: For each caption, show the top-5 retrieved images\n",
        "- **Image → Text**: For each image, show the top-5 retrieved captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "\n",
        "# Use random seed from training config\n",
        "RANDOM_SEED = TRAINING_CONFIG[\"random_seed\"]\n",
        "\n",
        "def visualize_text_to_image_retrieval(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    caption_to_idx: Dict[str, int],\n",
        "    image_keys: List[str],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    n_examples: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize Text → Image retrieval: for each recipe, show top-5 images.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Sample random recipes\n",
        "    unique_recipe_ids = test_df[\"recipe_id\"].unique()\n",
        "    sample_recipe_ids = pd.Series(unique_recipe_ids).sample(\n",
        "        min(n_examples, len(unique_recipe_ids)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for recipe_idx, recipe_id in enumerate(sample_recipe_ids):\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption for this recipe\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        \n",
        "        # Get caption embedding\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        \n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        similarities = image_embeddings @ text_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]  # Top 5, highest first\n",
        "        \n",
        "        # Get ground truth: images from this recipe_id\n",
        "        gt_image_indices = set(recipe_to_image_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Create figure with more vertical space for captions\n",
        "        fig = plt.figure(figsize=(16, 5))\n",
        "        gs = gridspec.GridSpec(1, 6, width_ratios=[2, 1, 1, 1, 1, 1], hspace=0.3, bottom=0.15)\n",
        "        \n",
        "        # Show query caption\n",
        "        ax0 = fig.add_subplot(gs[0])\n",
        "        ax0.text(0.5, 0.5, f'Query:\\n\"{caption}\"', \n",
        "                ha='center', va='center', fontsize=14, fontweight='bold',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
        "        ax0.axis('off')\n",
        "        \n",
        "        # Show top-5 images\n",
        "        for i, img_idx in enumerate(top5_indices):\n",
        "            img_key = image_keys[img_idx]\n",
        "            bucket, key = get_image_path_in_minio(img_key)\n",
        "            img = load_image_from_minio(bucket, key)\n",
        "            \n",
        "            if img is None:\n",
        "                continue\n",
        "            \n",
        "            # Check if this image is from the ground truth recipe\n",
        "            is_correct = img_idx in gt_image_indices\n",
        "            \n",
        "            # Get recipe_id for this image to show its representative caption\n",
        "            img_recipe_id = image_key_to_recipe_id.get(img_key, \"Unknown\")\n",
        "            # Note: Shows representative caption of the recipe (longest caption), not necessarily the exact caption for this image\n",
        "            img_caption = recipe_to_caption_text.get(img_recipe_id, \"Unknown\")\n",
        "            \n",
        "            ax = fig.add_subplot(gs[i + 1])\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "            \n",
        "            # Color border: green if correct, red if incorrect\n",
        "            border_color = 'green' if is_correct else 'red'\n",
        "            border_width = 3 if is_correct else 2\n",
        "            for spine in ax.spines.values():\n",
        "                spine.set_edgecolor(border_color)\n",
        "                spine.set_linewidth(border_width)\n",
        "            \n",
        "            # Title with similarity score\n",
        "            similarity_score = similarities[img_idx]\n",
        "            rank = i + 1\n",
        "            title = f\"Rank {rank}\\n{similarity_score:.3f}\"\n",
        "            if is_correct:\n",
        "                title += \"\\n✓ Correct\"\n",
        "            ax.set_title(title, fontsize=10, color=border_color, fontweight='bold')\n",
        "            \n",
        "            # Add recipe caption below the image\n",
        "            # Truncate long captions for display\n",
        "            display_caption = img_caption if len(img_caption) <= 40 else img_caption[:37] + \"...\"\n",
        "            ax.text(0.5, -0.15, f'\"{display_caption}\"', \n",
        "                   ha='center', va='top', fontsize=9, \n",
        "                   transform=ax.transAxes,\n",
        "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'),\n",
        "                   wrap=True)\n",
        "        \n",
        "        plt.suptitle(f'Text → Image Retrieval Example {recipe_idx + 1} (Recipe: {recipe_id_str})', \n",
        "                     fontsize=16, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print()\n",
        "\n",
        "def visualize_image_to_text_retrieval(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    unique_captions: List[str],\n",
        "    n_examples: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize Image → Text retrieval: for each image, show top-5 captions.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Sample random images\n",
        "    unique_images = test_df[\"image_key\"].unique()\n",
        "    sample_images = pd.Series(unique_images).sample(\n",
        "        min(n_examples, len(unique_images)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for img_idx_example, img_key in enumerate(sample_images):\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        similarities = text_embeddings @ img_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]  # Top 5, highest first\n",
        "        \n",
        "        # Get ground truth: recipe_id for this image\n",
        "        gt_recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        # Use all captions for ground truth (consistent with evaluation)\n",
        "        gt_caption_indices = set(recipe_to_all_caption_indices.get(gt_recipe_id_str, [])) if gt_recipe_id_str else set()\n",
        "        \n",
        "        # Load query image\n",
        "        bucket, key = get_image_path_in_minio(img_key)\n",
        "        query_img = load_image_from_minio(bucket, key)\n",
        "        \n",
        "        if query_img is None:\n",
        "            continue\n",
        "        \n",
        "        # Create figure with more vertical space for recipe name\n",
        "        fig = plt.figure(figsize=(16, 4))\n",
        "        gs = gridspec.GridSpec(1, 6, width_ratios=[1, 1, 1, 1, 1, 1], hspace=0.3, bottom=0.15)\n",
        "        \n",
        "        # Show query image\n",
        "        ax0 = fig.add_subplot(gs[0])\n",
        "        ax0.imshow(query_img)\n",
        "        ax0.axis('off')\n",
        "        ax0.set_title('Query Image', fontsize=12, fontweight='bold')\n",
        "        \n",
        "        # Add recipe name below the query image\n",
        "        if gt_recipe_id_str:\n",
        "            recipe_caption = recipe_to_caption_text.get(gt_recipe_id_str, \"Unknown\")\n",
        "            # Truncate long captions for display\n",
        "            display_recipe = recipe_caption if len(recipe_caption) <= 40 else recipe_caption[:37] + \"...\"\n",
        "            ax0.text(0.5, -0.15, f'Recipe: {gt_recipe_id_str}\\n\"{display_recipe}\"', \n",
        "                   ha='center', va='top', fontsize=9, \n",
        "                   transform=ax0.transAxes,\n",
        "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'),\n",
        "                   wrap=True)\n",
        "        \n",
        "        # Show top-5 captions\n",
        "        for i, text_idx in enumerate(top5_indices):\n",
        "            caption = unique_captions[text_idx]\n",
        "            \n",
        "            # Check if this caption is from the ground truth recipe\n",
        "            is_correct = text_idx in gt_caption_indices\n",
        "            \n",
        "            ax = fig.add_subplot(gs[i + 1])\n",
        "            ax.axis('off')\n",
        "            \n",
        "            # Color border: green if correct, red if incorrect\n",
        "            border_color = 'green' if is_correct else 'red'\n",
        "            border_width = 3 if is_correct else 2\n",
        "            \n",
        "            # Create text box\n",
        "            similarity_score = similarities[text_idx]\n",
        "            rank = i + 1\n",
        "            text_content = f\"Rank {rank}\\n\\n\\\"{caption}\\\"\\n\\nSimilarity: {similarity_score:.3f}\"\n",
        "            if is_correct:\n",
        "                text_content += \"\\n\\n✓ Correct\"\n",
        "            \n",
        "            ax.text(0.5, 0.5, text_content,\n",
        "                   ha='center', va='center', fontsize=10,\n",
        "                   bbox=dict(boxstyle='round', facecolor='lightyellow', \n",
        "                            edgecolor=border_color, linewidth=border_width, alpha=0.8),\n",
        "                   wrap=True)\n",
        "        \n",
        "        plt.suptitle(f'Image → Text Retrieval Example {img_idx_example + 1} (Recipe: {gt_recipe_id_str})', \n",
        "                     fontsize=16, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print()\n",
        "\n",
        "# Visualization functions defined above\n",
        "# They will be called after embeddings are generated (see section after evaluation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# This section has been moved to after the evaluation (section 11.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell will be moved to after the evaluation section\n",
        "# The visualization functions are now in the correct location (after section 11)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clip_contrastive_loss(image_embeds, text_embeds, logit_scale):\n",
        "    \"\"\"\n",
        "    CLIP contrastive loss with in-batch negatives.\n",
        "    \n",
        "    Args:\n",
        "        image_embeds: Image embeddings (batch_size, embed_dim)\n",
        "        text_embeds: Text embeddings (batch_size, embed_dim)\n",
        "        logit_scale: Learnable temperature parameter\n",
        "    \n",
        "    Returns:\n",
        "        loss: Contrastive loss\n",
        "    \"\"\"\n",
        "    # Normalize embeddings\n",
        "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
        "    text_embeds = F.normalize(text_embeds, dim=-1)\n",
        "    \n",
        "    # Compute logits\n",
        "    logit_scale = logit_scale.exp()\n",
        "    logits_per_text = logit_scale * (text_embeds @ image_embeds.T)  # (B, B)\n",
        "    logits_per_image = logits_per_text.T  # (B, B)\n",
        "    \n",
        "    # Labels: diagonal (each text matches its corresponding image)\n",
        "    batch_size = image_embeds.size(0)\n",
        "    labels = torch.arange(batch_size, device=image_embeds.device)\n",
        "    \n",
        "    # Cross-entropy losses\n",
        "    loss_t = F.cross_entropy(logits_per_text, labels)\n",
        "    loss_i = F.cross_entropy(logits_per_image, labels)\n",
        "    \n",
        "    # Average\n",
        "    loss = (loss_t + loss_i) / 2.0\n",
        "    \n",
        "    return loss\n",
        "\n",
        "# Setup optimizer (only trainable parameters)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=TRAINING_CONFIG[\"learning_rate\"],\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Setup learning rate scheduler (linear warmup + cosine decay)\n",
        "# Calculate steps per epoch accounting for gradient accumulation\n",
        "# Defensive check: ensure we have batches to train on\n",
        "if len(train_dataloader) == 0:\n",
        "    raise RuntimeError(\"train_dataloader has 0 batches. Check dataset filtering / loading.\")\n",
        "\n",
        "steps_per_epoch = int(np.ceil(len(train_dataloader) / TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "steps_per_epoch = max(1, steps_per_epoch)  # Ensure at least 1 step\n",
        "total_steps = steps_per_epoch * TRAINING_CONFIG[\"num_epochs\"]\n",
        "# Ensure warmup_steps doesn't exceed total_steps\n",
        "warmup_steps = min(TRAINING_CONFIG[\"warmup_steps\"], total_steps)\n",
        "\n",
        "def get_lr_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    \"\"\"Create learning rate scheduler with warmup.\"\"\"\n",
        "    from torch.optim.lr_scheduler import LambdaLR\n",
        "    \n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
        "    \n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "scheduler = get_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "print(f\"✓ Training setup complete\")\n",
        "print(f\"  Optimizer: AdamW (lr={TRAINING_CONFIG['learning_rate']})\")\n",
        "print(f\"  Scheduler: Linear warmup + Cosine decay\")\n",
        "print(f\"  Total steps: {total_steps}\")\n",
        "print(f\"  Warmup steps: {warmup_steps}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training logs\n",
        "training_logs = {\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"start_time\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"config\": {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"lora\": LORA_CONFIG,\n",
        "        \"training\": TRAINING_CONFIG,\n",
        "    },\n",
        "    \"epochs\": [],\n",
        "    \"total_time_seconds\": 0,\n",
        "    \"peak_memory_mb\": 0,\n",
        "}\n",
        "\n",
        "# Get device for inputs\n",
        "if DEVICE == \"cuda\":\n",
        "    main_device = next(model.parameters()).device\n",
        "else:\n",
        "    main_device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Device: {main_device}\")\n",
        "print(f\"Total epochs: {TRAINING_CONFIG['num_epochs']}\")\n",
        "batches_per_epoch = len(train_dataloader)\n",
        "optimizer_steps_per_epoch = int(np.ceil(batches_per_epoch / TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "print(f\"Batches per epoch: {batches_per_epoch}\")\n",
        "print(f\"Optimizer steps per epoch: {optimizer_steps_per_epoch}\")\n",
        "print(f\"Total optimizer steps: {total_steps}\")\n",
        "print()\n",
        "\n",
        "model.train()\n",
        "training_start_time = time.time()\n",
        "global_step = 0\n",
        "peak_memory = 0\n",
        "step_losses = []  # Track losses per step (not per batch)\n",
        "\n",
        "# Setup mixed precision training (AMP) for GPU\n",
        "# Use generic torch.amp API (works correctly on both CPU and CUDA)\n",
        "use_amp = (DEVICE == \"cuda\")\n",
        "if use_amp:\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=True)\n",
        "    print(\"✓ Mixed precision (AMP) enabled for GPU training\")\n",
        "else:\n",
        "    scaler = None  # No scaler needed for CPU\n",
        "    print(\"ℹ Mixed precision disabled (CPU training)\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(TRAINING_CONFIG[\"random_seed\"])\n",
        "np.random.seed(TRAINING_CONFIG[\"random_seed\"])\n",
        "random.seed(TRAINING_CONFIG[\"random_seed\"])\n",
        "\n",
        "for epoch in range(TRAINING_CONFIG[\"num_epochs\"]):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_losses = []\n",
        "    running_loss = 0.0  # Accumulate loss across gradient accumulation steps\n",
        "    \n",
        "    print(f\"\\nEpoch {epoch + 1}/{TRAINING_CONFIG['num_epochs']}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Reset failed image counter for this epoch\n",
        "    train_dataset.failed_image_count = 0\n",
        "    \n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
        "    \n",
        "    # Track last batch_idx explicitly (more robust than using locals())\n",
        "    last_batch_idx = -1\n",
        "    for batch_idx, inputs in enumerate(progress_bar):\n",
        "        # Skip batches with no valid samples (filtered out in collate_fn)\n",
        "        if inputs is None:\n",
        "            continue\n",
        "        \n",
        "        last_batch_idx = batch_idx\n",
        "        # Move inputs to device\n",
        "        input_ids = inputs[\"input_ids\"].to(main_device)\n",
        "        attention_mask = inputs.get(\"attention_mask\", None)\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.to(main_device)\n",
        "        pixel_values = inputs[\"pixel_values\"].to(main_device)\n",
        "        \n",
        "        # Forward pass using specific methods (avoids PEFT wrapper issues)\n",
        "        # Use get_image_features and get_text_features instead of forward()\n",
        "        text_inputs = {\n",
        "            \"input_ids\": input_ids,\n",
        "        }\n",
        "        if attention_mask is not None:\n",
        "            text_inputs[\"attention_mask\"] = attention_mask\n",
        "        \n",
        "        image_inputs = {\n",
        "            \"pixel_values\": pixel_values,\n",
        "        }\n",
        "        \n",
        "        # Forward pass with mixed precision (AMP) when on GPU\n",
        "        # Using enabled=use_amp for robustness (works even if refactored)\n",
        "        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
        "            # Get embeddings separately\n",
        "            text_embeds = model.get_text_features(**text_inputs)\n",
        "            image_embeds = model.get_image_features(**image_inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = clip_contrastive_loss(\n",
        "                image_embeds,\n",
        "                text_embeds,\n",
        "                model.logit_scale\n",
        "            )\n",
        "            \n",
        "            # Scale loss for gradient accumulation\n",
        "            loss = loss / TRAINING_CONFIG[\"gradient_accumulation_steps\"]\n",
        "        \n",
        "        # Backward pass (with scaler for AMP, direct for CPU)\n",
        "        if use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "        \n",
        "        # Accumulate unscaled loss for this accumulation step\n",
        "        running_loss += float(loss.item() * TRAINING_CONFIG[\"gradient_accumulation_steps\"])\n",
        "        \n",
        "        # Store unscaled loss for epoch average\n",
        "        epoch_losses.append(float(loss.item() * TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "        \n",
        "        # Update weights (only after accumulation steps)\n",
        "        if (batch_idx + 1) % TRAINING_CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
        "            if use_amp:\n",
        "                # Gradient clipping (especially important with trainable logit_scale)\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    [p for p in model.parameters() if p.requires_grad],\n",
        "                    max_norm=1.0\n",
        "                )\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                # Gradient clipping for CPU\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    [p for p in model.parameters() if p.requires_grad],\n",
        "                    max_norm=1.0\n",
        "                )\n",
        "                optimizer.step()\n",
        "            \n",
        "            # Clamp logit_scale to prevent temperature explosion (CLIP best practice)\n",
        "            with torch.no_grad():\n",
        "                model.logit_scale.clamp_(LOGIT_SCALE_MIN, LOGIT_SCALE_MAX)\n",
        "            \n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            # Store average loss for this step (across accumulation)\n",
        "            step_losses.append(running_loss / TRAINING_CONFIG[\"gradient_accumulation_steps\"])\n",
        "            running_loss = 0.0\n",
        "        \n",
        "        # Logging (by step, not by batch)\n",
        "        if global_step > 0 and global_step % TRAINING_CONFIG[\"logging_steps\"] == 0:\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            # Use step_losses for accurate logging\n",
        "            avg_loss = np.mean(step_losses[-TRAINING_CONFIG[\"logging_steps\"]:]) if len(step_losses) >= TRAINING_CONFIG[\"logging_steps\"] else np.mean(step_losses)\n",
        "            # Log logit_scale (temperature) to monitor stability\n",
        "            logit_scale_value = float(model.logit_scale.exp().item())\n",
        "            progress_bar.set_postfix({\n",
        "                \"loss\": f\"{avg_loss:.4f}\",\n",
        "                \"lr\": f\"{current_lr:.2e}\",\n",
        "                \"temp\": f\"{logit_scale_value:.2f}\",  # Temperature (exp of logit_scale)\n",
        "                \"step\": global_step\n",
        "            })\n",
        "            \n",
        "            # Store in training logs for analysis\n",
        "            if \"step_logs\" not in training_logs:\n",
        "                training_logs[\"step_logs\"] = []\n",
        "            training_logs[\"step_logs\"].append({\n",
        "                \"global_step\": global_step,\n",
        "                \"loss\": float(avg_loss),\n",
        "                \"learning_rate\": float(current_lr),\n",
        "                \"logit_scale\": float(model.logit_scale.item()),\n",
        "                \"temperature\": logit_scale_value\n",
        "            })\n",
        "        \n",
        "        # Track peak memory\n",
        "        if DEVICE == \"cuda\":\n",
        "            current_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
        "            peak_memory = max(peak_memory, current_memory)\n",
        "    \n",
        "    # Process remaining gradients at end of epoch if not aligned with accumulation steps\n",
        "    # Protection: ensure last_batch_idx is valid (should be >= 0 if dataloader has batches)\n",
        "    if last_batch_idx >= 0:\n",
        "        remaining_steps = (last_batch_idx + 1) % TRAINING_CONFIG[\"gradient_accumulation_steps\"]\n",
        "    else:\n",
        "        remaining_steps = 0  # No batches processed\n",
        "    \n",
        "    if remaining_steps > 0:\n",
        "        if use_amp:\n",
        "            # Gradient clipping (especially important with trainable logit_scale)\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in model.parameters() if p.requires_grad],\n",
        "                max_norm=1.0\n",
        "            )\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # Gradient clipping for CPU\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in model.parameters() if p.requires_grad],\n",
        "                max_norm=1.0\n",
        "            )\n",
        "            optimizer.step()\n",
        "        \n",
        "        # Clamp logit_scale to prevent temperature explosion (CLIP best practice)\n",
        "        with torch.no_grad():\n",
        "            model.logit_scale.clamp_(LOGIT_SCALE_MIN, LOGIT_SCALE_MAX)\n",
        "        \n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        global_step += 1\n",
        "        # Store average loss for remaining accumulation steps\n",
        "        step_losses.append(running_loss / remaining_steps)\n",
        "        running_loss = 0.0\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    avg_epoch_loss = np.mean(epoch_losses)\n",
        "    \n",
        "    # Calculate optimizer steps for this epoch\n",
        "    batches = len(train_dataloader)\n",
        "    optimizer_steps_this_epoch = int(np.ceil(batches / TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "    \n",
        "    # Report failed image loads (guardrail: detect if training is contaminated)\n",
        "    failed_images_this_epoch = train_dataset.failed_image_count\n",
        "    total_samples = len(train_dataloader) * TRAINING_CONFIG[\"batch_size\"]\n",
        "    failure_rate = (failed_images_this_epoch / total_samples * 100) if total_samples > 0 else 0\n",
        "    \n",
        "    if failed_images_this_epoch > 0:\n",
        "        print(f\"  ⚠ Failed image loads: {failed_images_this_epoch} ({failure_rate:.2f}% of samples)\")\n",
        "        \n",
        "        # Abort if contamination is too high (>0.5% threshold)\n",
        "        CONTAMINATION_THRESHOLD = 0.5\n",
        "        if failure_rate > CONTAMINATION_THRESHOLD:\n",
        "            raise RuntimeError(\n",
        "                f\"Training aborted: image load failure rate ({failure_rate:.2f}%) exceeds threshold ({CONTAMINATION_THRESHOLD}%). \"\n",
        "                f\"Training would be contaminated. Check image paths and MinIO connectivity.\"\n",
        "            )\n",
        "    \n",
        "    epoch_log = {\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"avg_loss\": float(avg_epoch_loss),\n",
        "        \"time_seconds\": float(epoch_time),\n",
        "        \"batches\": batches,\n",
        "        \"optimizer_steps\": optimizer_steps_this_epoch,\n",
        "        \"failed_image_loads\": failed_images_this_epoch,  # Track for analysis\n",
        "    }\n",
        "    training_logs[\"epochs\"].append(epoch_log)\n",
        "    \n",
        "    print(f\"  Average loss: {avg_epoch_loss:.4f}\")\n",
        "    print(f\"  Time: {epoch_time:.2f} seconds\")\n",
        "\n",
        "training_time = time.time() - training_start_time\n",
        "training_logs[\"total_time_seconds\"] = float(training_time)\n",
        "training_logs[\"peak_memory_mb\"] = float(peak_memory)\n",
        "training_logs[\"end_time\"] = datetime.utcnow().isoformat() + \"Z\"\n",
        "training_logs[\"total_steps\"] = global_step\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training Complete\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "print(f\"Peak memory: {peak_memory:.2f} MB\")\n",
        "print(f\"Total steps: {global_step}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoRA Fine-Tuning for CLIP\n",
        "\n",
        "This notebook fine-tunes CLIP using **LoRA** (Low-Rank Adaptation) on the food recipe dataset.\n",
        "\n",
        "**Hypothesis H1:** The fine-tuned model improves text↔image alignment compared to the CLIP baseline, measured with retrieval metrics (R@K, MRR).\n",
        "\n",
        "**This notebook (M1 LoRA):**\n",
        "- Loads pre-trained CLIP model (no quantization)\n",
        "- Applies LoRA adapters to trainable modules\n",
        "- Fine-tunes on training set using CLIP contrastive loss\n",
        "- Evaluates on test set with **same protocol as baseline** (by recipe_id)\n",
        "- Saves adapters, logs, and results for comparison\n",
        "\n",
        "**Inputs:**\n",
        "- `fine-tuning-zone/datasets/train_pairs_augmented_with_negatives.csv` (for training)\n",
        "- `fine-tuning-zone/datasets/test_pairs_positive.csv` (for evaluation)\n",
        "- `fine-tuning-zone/images/` and `fine-tuning-zone/augmented_images/` (images)\n",
        "\n",
        "**Outputs:**\n",
        "- `fine-tuning-zone/experiments/lora/run_{run_id}/` — Complete run directory with:\n",
        "  - `config.yaml` — Training configuration\n",
        "  - `adapters/` — LoRA adapter weights\n",
        "  - `training_logs.json` — Loss and timing logs\n",
        "  - `results_lora.json` — Evaluation metrics\n",
        "  - `examples_top5.json` — Qualitative examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import math\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from datetime import datetime\n",
        "\n",
        "# Check Python environment\n",
        "print(\"=\" * 60)\n",
        "print(\"Python Environment Check\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Python executable: {sys.executable}\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Check if we're in a virtual environment\n",
        "venv_path = Path(sys.executable).parent.parent\n",
        "if (venv_path / \"pyvenv.cfg\").exists() or \"venv\" in str(sys.executable).lower():\n",
        "    print(f\"✓ Running in virtual environment: {venv_path}\")\n",
        "else:\n",
        "    print(f\"⚠ Not in a virtual environment\")\n",
        "    print(f\"  Current Python: {sys.executable}\")\n",
        "    print(f\"  If you have a venv, make sure Jupyter is using it!\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import boto3\n",
        "from botocore.config import Config\n",
        "from botocore.exceptions import ClientError\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "import yaml\n",
        "\n",
        "# CLIP and LoRA imports\n",
        "TRANSFORMERS_AVAILABLE = False\n",
        "PEFT_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from transformers import CLIPModel, CLIPProcessor\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "    print(\"✓ transformers imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Failed to import transformers: {e}\")\n",
        "\n",
        "try:\n",
        "    from peft import LoraConfig, get_peft_model\n",
        "    PEFT_AVAILABLE = True\n",
        "    print(\"✓ peft imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Failed to import peft: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Library Status\")\n",
        "print(\"=\" * 60)\n",
        "if not TRANSFORMERS_AVAILABLE:\n",
        "    print(\"  ✗ transformers: NOT AVAILABLE\")\n",
        "    print(\"    Install with: pip install transformers\")\n",
        "else:\n",
        "    print(\"  ✓ transformers: AVAILABLE\")\n",
        "    \n",
        "if not PEFT_AVAILABLE:\n",
        "    print(\"  ✗ peft: NOT AVAILABLE\")\n",
        "    print(\"    Install with: pip install peft\")\n",
        "else:\n",
        "    print(\"  ✓ peft: AVAILABLE\")\n",
        "\n",
        "if TRANSFORMERS_AVAILABLE and PEFT_AVAILABLE:\n",
        "    print(\"\\n✓ All required libraries are available!\")\n",
        "    print(\"  You can proceed with training.\")\n",
        "elif not TRANSFORMERS_AVAILABLE or not PEFT_AVAILABLE:\n",
        "    print(\"\\n✗ Some required libraries are missing!\")\n",
        "    print(\"  Please install missing libraries before continuing.\")\n",
        "\n",
        "# Load environment variables\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent\n",
        "\n",
        "ENV_PATHS = [\n",
        "    PROJECT_ROOT / \"notebooks\" / \".env\",\n",
        "    PROJECT_ROOT / \"app\" / \".env\",\n",
        "    PROJECT_ROOT / \".env\",\n",
        "]\n",
        "\n",
        "env_loaded = False\n",
        "for env_path in ENV_PATHS:\n",
        "    if env_path.exists():\n",
        "        load_dotenv(env_path)\n",
        "        print(f\"✓ Loaded .env from: {env_path}\")\n",
        "        env_loaded = True\n",
        "        break\n",
        "\n",
        "if not env_loaded:\n",
        "    print(\"⚠ No .env file found, trying default load_dotenv()...\")\n",
        "    load_dotenv()\n",
        "\n",
        "# MinIO Configuration\n",
        "MINIO_USER = os.getenv(\"MINIO_USER\")\n",
        "MINIO_PASSWORD = os.getenv(\"MINIO_PASSWORD\")\n",
        "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\n",
        "\n",
        "# Bucket configuration\n",
        "FINE_TUNING_BUCKET = \"fine-tuning-zone\"\n",
        "DATASETS_PREFIX = \"datasets\"\n",
        "IMAGES_PREFIX = \"images\"\n",
        "AUGMENTED_IMAGES_PREFIX = \"augmented_images\"\n",
        "EXPERIMENTS_PREFIX = \"experiments\"\n",
        "\n",
        "# Input/Output paths\n",
        "TRAIN_AUGMENTED_KEY = f\"{DATASETS_PREFIX}/train_pairs_augmented_with_negatives.csv\"\n",
        "TEST_MANIFEST_KEY = f\"{DATASETS_PREFIX}/test_pairs_positive.csv\"  # Test set from baseline split\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"  # Same as baseline\n",
        "\n",
        "def pick_device():\n",
        "    \"\"\"Pick device, testing if CUDA actually works (not just available).\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return \"cpu\"\n",
        "    try:\n",
        "        _ = torch.randn(1, device=\"cuda\")\n",
        "        return \"cuda\"\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ CUDA visible pero no usable: {e}\")\n",
        "        return \"cpu\"\n",
        "\n",
        "DEVICE = pick_device()\n",
        "\n",
        "# LoRA configuration\n",
        "LORA_CONFIG = {\n",
        "    \"r\": 8,\n",
        "    \"lora_alpha\": 16,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \"bias\": \"none\",\n",
        "    \"task_type\": \"FEATURE_EXTRACTION\",\n",
        "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],  # Only attention modules (safer for CLIP)\n",
        "}\n",
        "\n",
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"num_epochs\": 3,\n",
        "    \"batch_size\": 2,  # Smaller batch for CPU training\n",
        "    \"gradient_accumulation_steps\": 16,  # Effective batch size = 2 * 16 = 32\n",
        "    \"warmup_steps\": 100,\n",
        "    \"save_steps\": 500,\n",
        "    \"logging_steps\": 50,\n",
        "    \"random_seed\": 42,\n",
        "}\n",
        "\n",
        "# Evaluation configuration (same as baseline)\n",
        "K_VALUES = [1, 5, 10]\n",
        "COMPUTE_MRR = True\n",
        "\n",
        "# Generate run ID\n",
        "RUN_ID = f\"lora_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
        "RUN_DIR = f\"{EXPERIMENTS_PREFIX}/lora/run_{RUN_ID}\"\n",
        "\n",
        "# Output paths\n",
        "CONFIG_KEY = f\"{RUN_DIR}/config.yaml\"\n",
        "ADAPTERS_DIR = f\"{RUN_DIR}/adapters\"\n",
        "LOGS_KEY = f\"{RUN_DIR}/training_logs.json\"\n",
        "RESULTS_KEY = f\"{RUN_DIR}/results_lora.json\"\n",
        "EXAMPLES_KEY = f\"{RUN_DIR}/examples_top5.json\"\n",
        "\n",
        "# Update config to include dataset info\n",
        "TRAINING_CONFIG[\"train_dataset\"] = TRAIN_AUGMENTED_KEY\n",
        "TRAINING_CONFIG[\"test_dataset\"] = TEST_MANIFEST_KEY\n",
        "TRAINING_CONFIG[\"filter_applied\"] = \"label == 1 (positive pairs only)\"\n",
        "\n",
        "# Logit scale clamping constants (CLIP best practice)\n",
        "LOGIT_SCALE_MIN = math.log(1/100)  # Minimum temperature: 0.01\n",
        "LOGIT_SCALE_MAX = math.log(100)     # Maximum temperature: 100\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "print(f\"  Method: LoRA\")\n",
        "print(f\"  LoRA r: {LORA_CONFIG['r']}, alpha: {LORA_CONFIG['lora_alpha']}\")\n",
        "print(f\"  Training: {TRAINING_CONFIG['num_epochs']} epochs, LR={TRAINING_CONFIG['learning_rate']}\")\n",
        "print(f\"  Batch size: {TRAINING_CONFIG['batch_size']} (effective: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']})\")\n",
        "print(f\"  Run ID: {RUN_ID}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize MinIO Client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize S3/MinIO client\n",
        "session = boto3.session.Session(\n",
        "    aws_access_key_id=MINIO_USER,\n",
        "    aws_secret_access_key=MINIO_PASSWORD,\n",
        "    region_name=\"us-east-1\"\n",
        ")\n",
        "s3 = session.client(\n",
        "    \"s3\",\n",
        "    endpoint_url=MINIO_ENDPOINT,\n",
        "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"})\n",
        ")\n",
        "\n",
        "def ensure_bucket_exists(bucket: str) -> bool:\n",
        "    \"\"\"Create bucket if it doesn't exist.\"\"\"\n",
        "    try:\n",
        "        s3.head_bucket(Bucket=bucket)\n",
        "        return True\n",
        "    except ClientError as e:\n",
        "        error_code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
        "        if error_code in (\"404\", \"NoSuchBucket\"):\n",
        "            try:\n",
        "                s3.create_bucket(Bucket=bucket)\n",
        "                print(f\"✓ Created bucket '{bucket}'\")\n",
        "                return True\n",
        "            except ClientError as create_error:\n",
        "                print(f\"✗ Failed to create bucket '{bucket}': {create_error}\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "# Verify buckets\n",
        "print(\"Checking buckets...\")\n",
        "ensure_bucket_exists(FINE_TUNING_BUCKET)\n",
        "print(\"✓ Buckets ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Training and Test Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_csv_from_minio(bucket: str, key: str) -> pd.DataFrame:\n",
        "    \"\"\"Load CSV file from MinIO into a DataFrame.\"\"\"\n",
        "    try:\n",
        "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "        df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
        "        print(f\"✓ Loaded {len(df)} rows from s3://{bucket}/{key}\")\n",
        "        return df\n",
        "    except ClientError as e:\n",
        "        print(f\"✗ Failed to load s3://{bucket}/{key}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Load training dataset (augmented, with negatives)\n",
        "print(\"Loading augmented training dataset...\")\n",
        "train_full_df = load_csv_from_minio(FINE_TUNING_BUCKET, TRAIN_AUGMENTED_KEY)\n",
        "\n",
        "if train_full_df.empty:\n",
        "    raise RuntimeError(f\"Could not load training data from s3://{FINE_TUNING_BUCKET}/{TRAIN_AUGMENTED_KEY}\")\n",
        "\n",
        "print(f\"\\nFull training dataset shape: {train_full_df.shape}\")\n",
        "print(f\"Columns: {list(train_full_df.columns)}\")\n",
        "if \"label\" in train_full_df.columns:\n",
        "    print(f\"\\nLabel distribution:\")\n",
        "    print(train_full_df[\"label\"].value_counts())\n",
        "    \n",
        "    # Filter only positive pairs (label=1)\n",
        "    train_df = train_full_df[train_full_df[\"label\"] == 1].copy()\n",
        "    print(f\"\\n✓ Filtered to {len(train_df)} positive pairs (label=1)\")\n",
        "    print(f\"  Removed {len(train_full_df) - len(train_df)} negative pairs\")\n",
        "else:\n",
        "    # If no label column, assume all are positive\n",
        "    train_df = train_full_df.copy()\n",
        "    print(f\"\\n✓ No label column found, using all {len(train_df)} pairs as positive\")\n",
        "\n",
        "# Load test dataset (from baseline split, already filtered)\n",
        "print(\"\\nLoading test dataset...\")\n",
        "test_df = load_csv_from_minio(FINE_TUNING_BUCKET, TEST_MANIFEST_KEY)\n",
        "\n",
        "if test_df.empty:\n",
        "    raise RuntimeError(f\"Could not load test data from s3://{FINE_TUNING_BUCKET}/{TEST_MANIFEST_KEY}\")\n",
        "\n",
        "print(f\"\\nTraining dataset:\")\n",
        "print(f\"  Pairs: {len(train_df)}\")\n",
        "print(f\"  Recipes: {train_df['recipe_id'].nunique()}\")\n",
        "print(f\"  Images: {train_df['image_key'].nunique()}\")\n",
        "print(f\"  Captions: {train_df['caption'].nunique()}\")\n",
        "\n",
        "print(f\"\\nTest dataset:\")\n",
        "print(f\"  Pairs: {len(test_df)}\")\n",
        "print(f\"  Recipes: {test_df['recipe_id'].nunique()}\")\n",
        "print(f\"  Images: {test_df['image_key'].nunique()}\")\n",
        "print(f\"  Captions: {test_df['caption'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Helper Functions for Image Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image_from_minio(bucket: str, key: str) -> Optional[Image.Image]:\n",
        "    \"\"\"Load an image from MinIO.\"\"\"\n",
        "    try:\n",
        "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "        img = Image.open(io.BytesIO(obj[\"Body\"].read()))\n",
        "        img.load()\n",
        "        return img.convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def get_image_path_in_minio(image_key: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Determine bucket and full key for an image.\n",
        "    \n",
        "    Images can be in:\n",
        "    - fine-tuning-zone/images/...\n",
        "    - fine-tuning-zone/augmented_images/...\n",
        "    \n",
        "    Tries images/ first, then augmented_images/ if not found.\n",
        "    \"\"\"\n",
        "    bucket = FINE_TUNING_BUCKET\n",
        "    \n",
        "    # Check if it's already a full path\n",
        "    if image_key.startswith(\"images/\") or image_key.startswith(\"augmented_images/\"):\n",
        "        key = image_key\n",
        "    else:\n",
        "        # Try images/ first\n",
        "        key = f\"{IMAGES_PREFIX}/{image_key}\"\n",
        "        # Check if exists, if not try augmented_images/\n",
        "        try:\n",
        "            s3.head_object(Bucket=bucket, Key=key)\n",
        "        except ClientError as e:\n",
        "            if e.response.get(\"Error\", {}).get(\"Code\") == \"404\":\n",
        "                # Try augmented_images/\n",
        "                key = f\"augmented_images/{image_key}\"\n",
        "            else:\n",
        "                raise\n",
        "    \n",
        "    return bucket, key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load CLIP Model with LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not TRANSFORMERS_AVAILABLE or not PEFT_AVAILABLE:\n",
        "    raise ImportError(\"Required libraries not available. Install: pip install transformers peft\")\n",
        "\n",
        "print(f\"Loading CLIP model with LoRA: {MODEL_NAME}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "model = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "\n",
        "# Load processor\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(\"Model ready for training (LoRA)\")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_CONFIG[\"r\"],\n",
        "    lora_alpha=LORA_CONFIG[\"lora_alpha\"],\n",
        "    lora_dropout=LORA_CONFIG[\"lora_dropout\"],\n",
        "    bias=LORA_CONFIG[\"bias\"],\n",
        "    task_type=LORA_CONFIG[\"task_type\"],\n",
        "    target_modules=LORA_CONFIG[\"target_modules\"],\n",
        ")\n",
        "\n",
        "# Apply LoRA adapters\n",
        "print(\"Applying LoRA adapters...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Print trainable parameters\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Model Information\")\n",
        "print(\"=\" * 60)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Check logit_scale trainability\n",
        "print(f\"\\nlogit_scale requires_grad: {model.logit_scale.requires_grad}\")\n",
        "if not model.logit_scale.requires_grad:\n",
        "    print(\"⚠ logit_scale is frozen. Enabling it for better fine-tuning (1 extra parameter, almost free).\")\n",
        "    model.logit_scale.requires_grad_(True)\n",
        "    print(\"✓ logit_scale is now trainable\")\n",
        "\n",
        "# Get model info\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n✓ Model loaded successfully\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  Trainable ratio: {trainable_params/total_params*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create DataLoader for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "class CLIPDataset(Dataset):\n",
        "    \"\"\"Dataset for CLIP training with image-text pairs.\"\"\"\n",
        "    \n",
        "    def __init__(self, df: pd.DataFrame, processor, s3_client, bucket: str):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.s3 = s3_client\n",
        "        self.bucket = bucket\n",
        "        self.failed_image_count = 0\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        \n",
        "        # Load image\n",
        "        bucket, key = get_image_path_in_minio(row[\"image_key\"])\n",
        "        img = load_image_from_minio(bucket, key)\n",
        "        \n",
        "        if img is None:\n",
        "            # Return a placeholder if image fails to load\n",
        "            self.failed_image_count += 1\n",
        "            img = Image.new(\"RGB\", (224, 224), color=\"black\")\n",
        "        \n",
        "        # Get caption\n",
        "        caption = row[\"caption\"]\n",
        "        \n",
        "        return {\n",
        "            \"image\": img,\n",
        "            \"text\": caption,\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for DataLoader.\"\"\"\n",
        "    images = [item[\"image\"] for item in batch]\n",
        "    texts = [item[\"text\"] for item in batch]\n",
        "    \n",
        "    # Process with CLIP processor\n",
        "    # Process text and images together (CLIPProcessor handles this correctly)\n",
        "    inputs = processor(\n",
        "        text=texts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "    )\n",
        "    \n",
        "    # Filter to only include what CLIPModel.forward() accepts\n",
        "    # CLIPModel expects: input_ids, attention_mask, pixel_values\n",
        "    # Remove any other keys like inputs_embeds, etc.\n",
        "    filtered_inputs = {}\n",
        "    allowed_keys = {\"input_ids\", \"attention_mask\", \"pixel_values\"}\n",
        "    \n",
        "    for key, value in inputs.items():\n",
        "        if key in allowed_keys:\n",
        "            filtered_inputs[key] = value\n",
        "    \n",
        "    return filtered_inputs\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "print(\"Creating datasets and dataloaders...\")\n",
        "\n",
        "train_dataset = CLIPDataset(train_df, processor, s3, FINE_TUNING_BUCKET)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,  # Set to 0 to avoid issues with MinIO\n",
        ")\n",
        "\n",
        "print(f\"✓ Created training dataloader\")\n",
        "print(f\"  Total batches: {len(train_dataloader)}\")\n",
        "print(f\"  Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
        "print(f\"  Gradient accumulation steps: {TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
        "print(f\"  Effective batch size: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.5. Visualization Functions\n",
        "\n",
        "Define functions to visualize retrieval results. These will be called after embeddings are generated (see section 11.5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "\n",
        "# Use random seed from training config\n",
        "RANDOM_SEED = TRAINING_CONFIG[\"random_seed\"]\n",
        "\n",
        "def visualize_text_to_image_retrieval(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    caption_to_idx: Dict[str, int],\n",
        "    image_keys: List[str],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    n_examples: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize Text → Image retrieval: for each recipe, show top-5 images.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Sample random recipes\n",
        "    unique_recipe_ids = test_df[\"recipe_id\"].unique()\n",
        "    sample_recipe_ids = pd.Series(unique_recipe_ids).sample(\n",
        "        min(n_examples, len(unique_recipe_ids)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for recipe_idx, recipe_id in enumerate(sample_recipe_ids):\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption for this recipe\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        \n",
        "        # Get caption embedding\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        \n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        similarities = image_embeddings @ text_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]  # Top 5, highest first\n",
        "        \n",
        "        # Get ground truth: images from this recipe_id\n",
        "        gt_image_indices = set(recipe_to_image_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Create figure with more vertical space for captions\n",
        "        fig = plt.figure(figsize=(16, 5))\n",
        "        gs = gridspec.GridSpec(1, 6, width_ratios=[2, 1, 1, 1, 1, 1], hspace=0.3, bottom=0.15)\n",
        "        \n",
        "        # Show query caption\n",
        "        ax0 = fig.add_subplot(gs[0])\n",
        "        ax0.text(0.5, 0.5, f'Query:\\n\"{caption}\"', \n",
        "                ha='center', va='center', fontsize=14, fontweight='bold',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
        "        ax0.axis('off')\n",
        "        \n",
        "        # Show top-5 images\n",
        "        for i, img_idx in enumerate(top5_indices):\n",
        "            img_key = image_keys[img_idx]\n",
        "            bucket, key = get_image_path_in_minio(img_key)\n",
        "            img = load_image_from_minio(bucket, key)\n",
        "            \n",
        "            if img is None:\n",
        "                continue\n",
        "            \n",
        "            # Check if this image is from the ground truth recipe\n",
        "            is_correct = img_idx in gt_image_indices\n",
        "            \n",
        "            # Get recipe_id for this image to show its representative caption\n",
        "            img_recipe_id = image_key_to_recipe_id.get(img_key, \"Unknown\")\n",
        "            # Note: Shows representative caption of the recipe (longest caption), not necessarily the exact caption for this image\n",
        "            img_caption = recipe_to_caption_text.get(img_recipe_id, \"Unknown\")\n",
        "            \n",
        "            ax = fig.add_subplot(gs[i + 1])\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "            \n",
        "            # Color border: green if correct, red if incorrect\n",
        "            border_color = 'green' if is_correct else 'red'\n",
        "            border_width = 3 if is_correct else 2\n",
        "            for spine in ax.spines.values():\n",
        "                spine.set_edgecolor(border_color)\n",
        "                spine.set_linewidth(border_width)\n",
        "            \n",
        "            # Title with similarity score\n",
        "            similarity_score = similarities[img_idx]\n",
        "            rank = i + 1\n",
        "            title = f\"Rank {rank}\\n{similarity_score:.3f}\"\n",
        "            if is_correct:\n",
        "                title += \"\\n✓ Correct\"\n",
        "            ax.set_title(title, fontsize=10, color=border_color, fontweight='bold')\n",
        "            \n",
        "            # Add recipe caption below the image\n",
        "            # Truncate long captions for display\n",
        "            display_caption = img_caption if len(img_caption) <= 40 else img_caption[:37] + \"...\"\n",
        "            ax.text(0.5, -0.15, f'\"{display_caption}\"', \n",
        "                   ha='center', va='top', fontsize=9, \n",
        "                   transform=ax.transAxes,\n",
        "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'),\n",
        "                   wrap=True)\n",
        "        \n",
        "        plt.suptitle(f'Text → Image Retrieval Example {recipe_idx + 1} (Recipe: {recipe_id_str})', \n",
        "                     fontsize=16, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print()\n",
        "\n",
        "def visualize_image_to_text_retrieval(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    unique_captions: List[str],\n",
        "    n_examples: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize Image → Text retrieval: for each image, show top-5 captions.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Sample random images\n",
        "    unique_images = test_df[\"image_key\"].unique()\n",
        "    sample_images = pd.Series(unique_images).sample(\n",
        "        min(n_examples, len(unique_images)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for img_idx_example, img_key in enumerate(sample_images):\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        similarities = text_embeddings @ img_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]  # Top 5, highest first\n",
        "        \n",
        "        # Get ground truth: recipe_id for this image\n",
        "        gt_recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        # Use all captions for ground truth (consistent with evaluation)\n",
        "        gt_caption_indices = set(recipe_to_all_caption_indices.get(gt_recipe_id_str, [])) if gt_recipe_id_str else set()\n",
        "        \n",
        "        # Load query image\n",
        "        bucket, key = get_image_path_in_minio(img_key)\n",
        "        query_img = load_image_from_minio(bucket, key)\n",
        "        \n",
        "        if query_img is None:\n",
        "            continue\n",
        "        \n",
        "        # Create figure with more vertical space for recipe name\n",
        "        fig = plt.figure(figsize=(16, 4))\n",
        "        gs = gridspec.GridSpec(1, 6, width_ratios=[1, 1, 1, 1, 1, 1], hspace=0.3, bottom=0.15)\n",
        "        \n",
        "        # Show query image\n",
        "        ax0 = fig.add_subplot(gs[0])\n",
        "        ax0.imshow(query_img)\n",
        "        ax0.axis('off')\n",
        "        ax0.set_title('Query Image', fontsize=12, fontweight='bold')\n",
        "        \n",
        "        # Add recipe name below the query image\n",
        "        if gt_recipe_id_str:\n",
        "            recipe_caption = recipe_to_caption_text.get(gt_recipe_id_str, \"Unknown\")\n",
        "            # Truncate long captions for display\n",
        "            display_recipe = recipe_caption if len(recipe_caption) <= 40 else recipe_caption[:37] + \"...\"\n",
        "            ax0.text(0.5, -0.15, f'Recipe: {gt_recipe_id_str}\\n\"{display_recipe}\"', \n",
        "                   ha='center', va='top', fontsize=9, \n",
        "                   transform=ax0.transAxes,\n",
        "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'),\n",
        "                   wrap=True)\n",
        "        \n",
        "        # Show top-5 captions\n",
        "        for i, text_idx in enumerate(top5_indices):\n",
        "            caption = unique_captions[text_idx]\n",
        "            \n",
        "            # Check if this caption is from the ground truth recipe\n",
        "            is_correct = text_idx in gt_caption_indices\n",
        "            \n",
        "            ax = fig.add_subplot(gs[i + 1])\n",
        "            ax.axis('off')\n",
        "            \n",
        "            # Color border: green if correct, red if incorrect\n",
        "            border_color = 'green' if is_correct else 'red'\n",
        "            border_width = 3 if is_correct else 2\n",
        "            \n",
        "            # Create text box\n",
        "            similarity_score = similarities[text_idx]\n",
        "            rank = i + 1\n",
        "            text_content = f\"Rank {rank}\\n\\n\\\"{caption}\\\"\\n\\nSimilarity: {similarity_score:.3f}\"\n",
        "            if is_correct:\n",
        "                text_content += \"\\n\\n✓ Correct\"\n",
        "            \n",
        "            ax.text(0.5, 0.5, text_content,\n",
        "                   ha='center', va='center', fontsize=10,\n",
        "                   bbox=dict(boxstyle='round', facecolor='lightyellow', \n",
        "                            edgecolor=border_color, linewidth=border_width, alpha=0.8),\n",
        "                   wrap=True)\n",
        "        \n",
        "        plt.suptitle(f'Image → Text Retrieval Example {img_idx_example + 1} (Recipe: {gt_recipe_id_str})', \n",
        "                     fontsize=16, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print()\n",
        "\n",
        "# Visualization functions defined above\n",
        "# They will be called after embeddings are generated (see section after evaluation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# This section has been moved to after the evaluation (section 11.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell will be moved to after the evaluation section\n",
        "# The visualization functions are now in the correct location (after section 11)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clip_contrastive_loss(image_embeds, text_embeds, logit_scale):\n",
        "    \"\"\"\n",
        "    CLIP contrastive loss with in-batch negatives.\n",
        "    \n",
        "    Args:\n",
        "        image_embeds: Image embeddings (batch_size, embed_dim)\n",
        "        text_embeds: Text embeddings (batch_size, embed_dim)\n",
        "        logit_scale: Learnable temperature parameter\n",
        "    \n",
        "    Returns:\n",
        "        loss: Contrastive loss\n",
        "    \"\"\"\n",
        "    # Normalize embeddings\n",
        "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
        "    text_embeds = F.normalize(text_embeds, dim=-1)\n",
        "    \n",
        "    # Compute logits\n",
        "    logit_scale = logit_scale.exp()\n",
        "    logits_per_text = logit_scale * (text_embeds @ image_embeds.T)  # (B, B)\n",
        "    logits_per_image = logits_per_text.T  # (B, B)\n",
        "    \n",
        "    # Labels: diagonal (each text matches its corresponding image)\n",
        "    batch_size = image_embeds.size(0)\n",
        "    labels = torch.arange(batch_size, device=image_embeds.device)\n",
        "    \n",
        "    # Cross-entropy losses\n",
        "    loss_t = F.cross_entropy(logits_per_text, labels)\n",
        "    loss_i = F.cross_entropy(logits_per_image, labels)\n",
        "    \n",
        "    # Average\n",
        "    loss = (loss_t + loss_i) / 2.0\n",
        "    \n",
        "    return loss\n",
        "\n",
        "# Setup optimizer (only trainable parameters)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=TRAINING_CONFIG[\"learning_rate\"],\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Setup learning rate scheduler (linear warmup + cosine decay)\n",
        "# Calculate steps per epoch accounting for gradient accumulation\n",
        "# Defensive check: ensure we have batches to train on\n",
        "if len(train_dataloader) == 0:\n",
        "    raise RuntimeError(\"train_dataloader has 0 batches. Check dataset filtering / loading.\")\n",
        "\n",
        "steps_per_epoch = int(np.ceil(len(train_dataloader) / TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "steps_per_epoch = max(1, steps_per_epoch)  # Ensure at least 1 step\n",
        "total_steps = steps_per_epoch * TRAINING_CONFIG[\"num_epochs\"]\n",
        "# Ensure warmup_steps doesn't exceed total_steps\n",
        "warmup_steps = min(TRAINING_CONFIG[\"warmup_steps\"], total_steps)\n",
        "\n",
        "def get_lr_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    \"\"\"Create learning rate scheduler with warmup.\"\"\"\n",
        "    from torch.optim.lr_scheduler import LambdaLR\n",
        "    \n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
        "    \n",
        "    return LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "scheduler = get_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "print(f\"✓ Training setup complete\")\n",
        "print(f\"  Optimizer: AdamW (lr={TRAINING_CONFIG['learning_rate']})\")\n",
        "print(f\"  Scheduler: Linear warmup + Cosine decay\")\n",
        "print(f\"  Total steps: {total_steps}\")\n",
        "print(f\"  Warmup steps: {warmup_steps}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training logs\n",
        "training_logs = {\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"start_time\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"config\": {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"lora\": LORA_CONFIG,\n",
        "        \"training\": TRAINING_CONFIG,\n",
        "    },\n",
        "    \"epochs\": [],\n",
        "    \"total_time_seconds\": 0,\n",
        "    \"peak_memory_mb\": 0,\n",
        "}\n",
        "\n",
        "# Get device for inputs\n",
        "if DEVICE == \"cuda\":\n",
        "    main_device = next(model.parameters()).device\n",
        "else:\n",
        "    main_device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Device: {main_device}\")\n",
        "print(f\"Total epochs: {TRAINING_CONFIG['num_epochs']}\")\n",
        "batches_per_epoch = len(train_dataloader)\n",
        "optimizer_steps_per_epoch = int(np.ceil(batches_per_epoch / TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "print(f\"Batches per epoch: {batches_per_epoch}\")\n",
        "print(f\"Optimizer steps per epoch: {optimizer_steps_per_epoch}\")\n",
        "print(f\"Total optimizer steps: {total_steps}\")\n",
        "print()\n",
        "\n",
        "model.train()\n",
        "training_start_time = time.time()\n",
        "global_step = 0\n",
        "peak_memory = 0\n",
        "step_losses = []  # Track losses per step (not per batch)\n",
        "\n",
        "# Setup mixed precision training (AMP) for GPU\n",
        "# Use generic torch.amp API (works correctly on both CPU and CUDA)\n",
        "use_amp = (DEVICE == \"cuda\")\n",
        "if use_amp:\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=True)\n",
        "    print(\"✓ Mixed precision (AMP) enabled for GPU training\")\n",
        "else:\n",
        "    scaler = None  # No scaler needed for CPU\n",
        "    print(\"ℹ Mixed precision disabled (CPU training)\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(TRAINING_CONFIG[\"random_seed\"])\n",
        "np.random.seed(TRAINING_CONFIG[\"random_seed\"])\n",
        "random.seed(TRAINING_CONFIG[\"random_seed\"])\n",
        "\n",
        "for epoch in range(TRAINING_CONFIG[\"num_epochs\"]):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_losses = []\n",
        "    running_loss = 0.0  # Accumulate loss across gradient accumulation steps\n",
        "    \n",
        "    print(f\"\\nEpoch {epoch + 1}/{TRAINING_CONFIG['num_epochs']}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Reset failed image counter for this epoch\n",
        "    train_dataset.failed_image_count = 0\n",
        "    \n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
        "    \n",
        "    # Track last batch_idx explicitly (more robust than using locals())\n",
        "    last_batch_idx = -1\n",
        "    for batch_idx, inputs in enumerate(progress_bar):\n",
        "        last_batch_idx = batch_idx\n",
        "        # Move inputs to device\n",
        "        input_ids = inputs[\"input_ids\"].to(main_device)\n",
        "        attention_mask = inputs.get(\"attention_mask\", None)\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.to(main_device)\n",
        "        pixel_values = inputs[\"pixel_values\"].to(main_device)\n",
        "        \n",
        "        # Forward pass using specific methods (avoids PEFT wrapper issues)\n",
        "        # Use get_image_features and get_text_features instead of forward()\n",
        "        text_inputs = {\n",
        "            \"input_ids\": input_ids,\n",
        "        }\n",
        "        if attention_mask is not None:\n",
        "            text_inputs[\"attention_mask\"] = attention_mask\n",
        "        \n",
        "        image_inputs = {\n",
        "            \"pixel_values\": pixel_values,\n",
        "        }\n",
        "        \n",
        "        # Forward pass with mixed precision (AMP) when on GPU\n",
        "        # Using enabled=use_amp for robustness (works even if refactored)\n",
        "        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
        "            # Get embeddings separately\n",
        "            text_embeds = model.get_text_features(**text_inputs)\n",
        "            image_embeds = model.get_image_features(**image_inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = clip_contrastive_loss(\n",
        "                image_embeds,\n",
        "                text_embeds,\n",
        "                model.logit_scale\n",
        "            )\n",
        "            \n",
        "            # Scale loss for gradient accumulation\n",
        "            loss = loss / TRAINING_CONFIG[\"gradient_accumulation_steps\"]\n",
        "        \n",
        "        # Backward pass (with scaler for AMP, direct for CPU)\n",
        "        if use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "        \n",
        "        # Accumulate unscaled loss for this accumulation step\n",
        "        running_loss += float(loss.item() * TRAINING_CONFIG[\"gradient_accumulation_steps\"])\n",
        "        \n",
        "        # Store unscaled loss for epoch average\n",
        "        epoch_losses.append(float(loss.item() * TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "        \n",
        "        # Update weights (only after accumulation steps)\n",
        "        if (batch_idx + 1) % TRAINING_CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
        "            if use_amp:\n",
        "                # Gradient clipping (especially important with trainable logit_scale)\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    [p for p in model.parameters() if p.requires_grad],\n",
        "                    max_norm=1.0\n",
        "                )\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                # Gradient clipping for CPU\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    [p for p in model.parameters() if p.requires_grad],\n",
        "                    max_norm=1.0\n",
        "                )\n",
        "                optimizer.step()\n",
        "            \n",
        "            # Clamp logit_scale to prevent temperature explosion (CLIP best practice)\n",
        "            with torch.no_grad():\n",
        "                model.logit_scale.clamp_(LOGIT_SCALE_MIN, LOGIT_SCALE_MAX)\n",
        "            \n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            # Store average loss for this step (across accumulation)\n",
        "            step_losses.append(running_loss / TRAINING_CONFIG[\"gradient_accumulation_steps\"])\n",
        "            running_loss = 0.0\n",
        "        \n",
        "        # Logging (by step, not by batch)\n",
        "        if global_step > 0 and global_step % TRAINING_CONFIG[\"logging_steps\"] == 0:\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            # Use step_losses for accurate logging\n",
        "            avg_loss = np.mean(step_losses[-TRAINING_CONFIG[\"logging_steps\"]:]) if len(step_losses) >= TRAINING_CONFIG[\"logging_steps\"] else np.mean(step_losses)\n",
        "            progress_bar.set_postfix({\n",
        "                \"loss\": f\"{avg_loss:.4f}\",\n",
        "                \"lr\": f\"{current_lr:.2e}\",\n",
        "                \"step\": global_step\n",
        "            })\n",
        "        \n",
        "        # Track peak memory\n",
        "        if DEVICE == \"cuda\":\n",
        "            current_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
        "            peak_memory = max(peak_memory, current_memory)\n",
        "    \n",
        "    # Process remaining gradients at end of epoch if not aligned with accumulation steps\n",
        "    # Protection: ensure last_batch_idx is valid (should be >= 0 if dataloader has batches)\n",
        "    if last_batch_idx >= 0:\n",
        "        remaining_steps = (last_batch_idx + 1) % TRAINING_CONFIG[\"gradient_accumulation_steps\"]\n",
        "    else:\n",
        "        remaining_steps = 0  # No batches processed\n",
        "    \n",
        "    if remaining_steps > 0:\n",
        "        if use_amp:\n",
        "            # Gradient clipping (especially important with trainable logit_scale)\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in model.parameters() if p.requires_grad],\n",
        "                max_norm=1.0\n",
        "            )\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # Gradient clipping for CPU\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in model.parameters() if p.requires_grad],\n",
        "                max_norm=1.0\n",
        "            )\n",
        "            optimizer.step()\n",
        "        \n",
        "        # Clamp logit_scale to prevent temperature explosion (CLIP best practice)\n",
        "        with torch.no_grad():\n",
        "            model.logit_scale.clamp_(LOGIT_SCALE_MIN, LOGIT_SCALE_MAX)\n",
        "        \n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        global_step += 1\n",
        "        # Store average loss for remaining accumulation steps\n",
        "        step_losses.append(running_loss / remaining_steps)\n",
        "        running_loss = 0.0\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    avg_epoch_loss = np.mean(epoch_losses)\n",
        "    \n",
        "    # Calculate optimizer steps for this epoch\n",
        "    batches = len(train_dataloader)\n",
        "    optimizer_steps_this_epoch = int(np.ceil(batches / TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "    \n",
        "    # Report failed image loads (guardrail: detect if training is contaminated)\n",
        "    failed_images_this_epoch = train_dataset.failed_image_count\n",
        "    if failed_images_this_epoch > 0:\n",
        "        print(f\"  ⚠ Failed image loads: {failed_images_this_epoch} (training may be contaminated)\")\n",
        "    \n",
        "    epoch_log = {\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"avg_loss\": float(avg_epoch_loss),\n",
        "        \"time_seconds\": float(epoch_time),\n",
        "        \"batches\": batches,\n",
        "        \"optimizer_steps\": optimizer_steps_this_epoch,\n",
        "        \"failed_image_loads\": failed_images_this_epoch,  # Track for analysis\n",
        "    }\n",
        "    training_logs[\"epochs\"].append(epoch_log)\n",
        "    \n",
        "    print(f\"  Average loss: {avg_epoch_loss:.4f}\")\n",
        "    print(f\"  Time: {epoch_time:.2f} seconds\")\n",
        "\n",
        "training_time = time.time() - training_start_time\n",
        "training_logs[\"total_time_seconds\"] = float(training_time)\n",
        "training_logs[\"peak_memory_mb\"] = float(peak_memory)\n",
        "training_logs[\"end_time\"] = datetime.utcnow().isoformat() + \"Z\"\n",
        "training_logs[\"total_steps\"] = global_step\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training Complete\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "print(f\"Peak memory: {peak_memory:.2f} MB\")\n",
        "print(f\"Total steps: {global_step}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_json_to_minio(data: Dict, bucket: str, key: str) -> bool:\n",
        "    \"\"\"Save dictionary as JSON to MinIO.\"\"\"\n",
        "    try:\n",
        "        json_bytes = json.dumps(data, indent=2).encode(\"utf-8\")\n",
        "        s3.put_object(\n",
        "            Bucket=bucket,\n",
        "            Key=key,\n",
        "            Body=json_bytes,\n",
        "            ContentType=\"application/json\",\n",
        "        )\n",
        "        size_kb = len(json_bytes) / 1024\n",
        "        print(f\"✓ Saved to s3://{bucket}/{key} ({size_kb:.1f} KB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to save: {e}\")\n",
        "        return False\n",
        "\n",
        "def save_yaml_to_minio(data: Dict, bucket: str, key: str) -> bool:\n",
        "    \"\"\"Save dictionary as YAML to MinIO.\"\"\"\n",
        "    try:\n",
        "        yaml_str = yaml.dump(data, default_flow_style=False, sort_keys=False)\n",
        "        yaml_bytes = yaml_str.encode(\"utf-8\")\n",
        "        s3.put_object(\n",
        "            Bucket=bucket,\n",
        "            Key=key,\n",
        "            Body=yaml_bytes,\n",
        "            ContentType=\"text/yaml\",\n",
        "        )\n",
        "        size_kb = len(yaml_bytes) / 1024\n",
        "        print(f\"✓ Saved to s3://{bucket}/{key} ({size_kb:.1f} KB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to save: {e}\")\n",
        "        return False\n",
        "\n",
        "# Save adapters locally first, then upload to MinIO\n",
        "print(\"Saving adapters...\")\n",
        "local_adapters_dir = Path(f\"adapters_{RUN_ID}\")\n",
        "model.save_pretrained(str(local_adapters_dir))\n",
        "\n",
        "# Upload adapters to MinIO (upload config and weights - supports both .bin and .safetensors)\n",
        "print(\"Uploading adapters to MinIO...\")\n",
        "adapter_files = [\"adapter_config.json\", \"adapter_model.bin\", \"adapter_model.safetensors\"]\n",
        "for file_name in adapter_files:\n",
        "    file_path = local_adapters_dir / file_name\n",
        "    if file_path.exists():\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            s3.put_object(\n",
        "                Bucket=FINE_TUNING_BUCKET,\n",
        "                Key=f\"{ADAPTERS_DIR}/{file_name}\",\n",
        "                Body=f.read(),\n",
        "            )\n",
        "        print(f\"  ✓ Uploaded {file_name}\")\n",
        "\n",
        "# Save training logs\n",
        "print(\"\\nSaving training logs...\")\n",
        "save_json_to_minio(training_logs, FINE_TUNING_BUCKET, LOGS_KEY)\n",
        "\n",
        "# Save configuration\n",
        "config_data = {\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"method\": \"lora\",\n",
        "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"lora_config\": LORA_CONFIG,\n",
        "    \"training_config\": TRAINING_CONFIG,\n",
        "    \"model_info\": {\n",
        "        \"total_parameters\": int(total_params),\n",
        "        \"trainable_parameters\": int(trainable_params),\n",
        "        \"trainable_ratio\": float(trainable_params / total_params),\n",
        "    },\n",
        "}\n",
        "save_yaml_to_minio(config_data, FINE_TUNING_BUCKET, CONFIG_KEY)\n",
        "\n",
        "print(f\"\\n✅ Adapters and logs saved successfully!\")\n",
        "print(f\"  Adapters: s3://{FINE_TUNING_BUCKET}/{ADAPTERS_DIR}/\")\n",
        "print(f\"  Logs: s3://{FINE_TUNING_BUCKET}/{LOGS_KEY}\")\n",
        "print(f\"  Config: s3://{FINE_TUNING_BUCKET}/{CONFIG_KEY}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluation (Same Protocol as Baseline)\n",
        "\n",
        "Now we evaluate the fine-tuned model using the same evaluation functions as the baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Generate embeddings for test set (same as baseline)\n",
        "print(\"=\" * 60)\n",
        "print(\"Generating Embeddings for Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def generate_image_embeddings(\n",
        "    df: pd.DataFrame, \n",
        "    model, \n",
        "    processor, \n",
        "    device: Union[str, torch.device], \n",
        "    batch_size: int = 32\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"Generate image embeddings for all images in the dataset.\"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    image_keys = []\n",
        "    failed_keys = []\n",
        "    \n",
        "    unique_images = df[\"image_key\"].unique()\n",
        "    print(f\"Generating embeddings for {len(unique_images)} unique images...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(unique_images), batch_size)):\n",
        "            batch_keys = unique_images[i:i+batch_size]\n",
        "            batch_images = []\n",
        "            valid_keys = []\n",
        "            \n",
        "            for img_key in batch_keys:\n",
        "                bucket, key = get_image_path_in_minio(img_key)\n",
        "                img = load_image_from_minio(bucket, key)\n",
        "                if img is not None:\n",
        "                    batch_images.append(img)\n",
        "                    valid_keys.append(img_key)\n",
        "                else:\n",
        "                    failed_keys.append(img_key)\n",
        "            \n",
        "            if not batch_images:\n",
        "                continue\n",
        "            \n",
        "            inputs = processor(images=batch_images, return_tensors=\"pt\", padding=True)\n",
        "            # Move inputs to device explicitly (more robust than .to(device) on BatchEncoding)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            outputs = model.get_image_features(**inputs)\n",
        "            batch_emb = F.normalize(outputs, p=2, dim=1)\n",
        "            \n",
        "            embeddings.append(batch_emb.cpu().numpy())\n",
        "            image_keys.extend(valid_keys)\n",
        "    \n",
        "    if failed_keys:\n",
        "        print(f\"⚠ Failed to load {len(failed_keys)} images\")\n",
        "    \n",
        "    embeddings = np.vstack(embeddings) if embeddings else np.array([])\n",
        "    print(f\"✓ Generated {len(image_keys)} image embeddings (shape: {embeddings.shape})\")\n",
        "    return embeddings, image_keys\n",
        "\n",
        "def generate_text_embeddings(\n",
        "    texts: List[str], \n",
        "    model, \n",
        "    processor, \n",
        "    device: Union[str, torch.device], \n",
        "    batch_size: int = 32\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Generate text embeddings for all captions.\"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    \n",
        "    print(f\"Generating embeddings for {len(texts)} texts...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(texts), batch_size)):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            inputs = processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            # Move inputs to device explicitly (more robust than .to(device) on BatchEncoding)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            outputs = model.get_text_features(**inputs)\n",
        "            batch_emb = F.normalize(outputs, p=2, dim=1)\n",
        "            embeddings.append(batch_emb.cpu().numpy())\n",
        "    \n",
        "    embeddings = np.vstack(embeddings) if embeddings else np.array([])\n",
        "    print(f\"✓ Generated {len(texts)} text embeddings (shape: {embeddings.shape})\")\n",
        "    return embeddings\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Generate embeddings\n",
        "image_embeddings, image_keys = generate_image_embeddings(\n",
        "    test_df, model, processor, main_device, batch_size=16  # Smaller batch for LoRA\n",
        ")\n",
        "\n",
        "unique_captions = test_df[\"caption\"].unique().tolist()\n",
        "text_embeddings = generate_text_embeddings(unique_captions, model, processor, main_device, batch_size=16)\n",
        "\n",
        "# Create official copy to ensure alignment with text_embeddings (never modify this)\n",
        "unique_captions_list = list(unique_captions)  # Explicit copy to prevent accidental modification\n",
        "\n",
        "embedding_time = time.time() - start_time\n",
        "print(f\"\\n✓ Embedding generation complete in {embedding_time:.2f} seconds\")\n",
        "\n",
        "# Create mapping dictionaries\n",
        "image_key_to_idx = {key: idx for idx, key in enumerate(image_keys)}\n",
        "caption_to_idx = {caption: idx for idx, caption in enumerate(unique_captions_list)}\n",
        "\n",
        "# Precompute lookup dictionaries for O(1) access (same as baseline)\n",
        "print(\"\\nPrecomputing lookup dictionaries...\")\n",
        "\n",
        "# recipe_id -> list of image indices\n",
        "recipe_to_image_indices: Dict[str, List[int]] = {}\n",
        "for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "    recipe_id_str = str(recipe_id)\n",
        "    recipe_images = test_df[test_df[\"recipe_id\"] == recipe_id][\"image_key\"].unique()\n",
        "    recipe_to_image_indices[recipe_id_str] = [\n",
        "        image_key_to_idx.get(img_key) \n",
        "        for img_key in recipe_images \n",
        "        if image_key_to_idx.get(img_key) is not None\n",
        "    ]\n",
        "\n",
        "# recipe_id -> list of caption indices (one caption per recipe for text→image query)\n",
        "# Use longest caption as representative (most informative)\n",
        "recipe_to_caption_indices: Dict[str, List[int]] = {}\n",
        "recipe_to_caption_text: Dict[str, str] = {}\n",
        "# recipe_id -> list of ALL caption indices (for image→text ground truth)\n",
        "recipe_to_all_caption_indices: Dict[str, List[int]] = {}\n",
        "for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "    recipe_id_str = str(recipe_id)\n",
        "    recipe_captions = test_df[test_df[\"recipe_id\"] == recipe_id][\"caption\"].unique()\n",
        "    if len(recipe_captions) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Use longest caption as representative (most informative)\n",
        "    # If multiple have same length, use first after sorting alphabetically (deterministic)\n",
        "    representative_caption = max(recipe_captions, key=lambda c: (len(c), c))\n",
        "    \n",
        "    # Store representative caption for text→image queries\n",
        "    cap_idx = caption_to_idx.get(representative_caption)\n",
        "    if cap_idx is not None:\n",
        "        recipe_to_caption_indices[recipe_id_str] = [cap_idx]\n",
        "        recipe_to_caption_text[recipe_id_str] = representative_caption\n",
        "    \n",
        "    # Store ALL captions for image→text ground truth (more fair if multiple captions per recipe)\n",
        "    all_caption_indices = []\n",
        "    for caption in recipe_captions:\n",
        "        cap_idx = caption_to_idx.get(caption)\n",
        "        if cap_idx is not None:\n",
        "            all_caption_indices.append(cap_idx)\n",
        "    recipe_to_all_caption_indices[recipe_id_str] = sorted(set(all_caption_indices))  # Remove duplicates, sorted for determinism\n",
        "\n",
        "# image_key -> recipe_id (as string for consistency)\n",
        "image_key_to_recipe_id: Dict[str, str] = {}\n",
        "for _, row in test_df.iterrows():\n",
        "    if row[\"image_key\"] not in image_key_to_recipe_id:\n",
        "        image_key_to_recipe_id[row[\"image_key\"]] = str(row[\"recipe_id\"])\n",
        "\n",
        "print(f\"✓ Precomputed lookups for {len(recipe_to_image_indices)} recipes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Evaluation Functions (Same as Baseline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy evaluation functions from baseline (same protocol by recipe_id)\n",
        "def compute_recall_at_k(scores: np.ndarray, ground_truth_indices: List[int], k: int) -> float:\n",
        "    \"\"\"\n",
        "    Compute Recall@K.\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "        k: top-K to consider\n",
        "    \n",
        "    Returns:\n",
        "        recall@k: 1.0 if any ground truth is in top-K, else 0.0\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    top_k_indices = np.argsort(scores)[-k:][::-1]  # Top K, highest first\n",
        "    top_k_set = set(top_k_indices)\n",
        "    \n",
        "    # Check if any ground truth is in top-K\n",
        "    for gt_idx in ground_truth_indices:\n",
        "        if gt_idx in top_k_set:\n",
        "            return 1.0\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "def compute_mrr(scores: np.ndarray, ground_truth_indices: List[int]) -> float:\n",
        "    \"\"\"\n",
        "    Compute Mean Reciprocal Rank.\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "    \n",
        "    Returns:\n",
        "        mrr: 1/rank of first correct result, or 0.0 if none found\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Sort by score (descending)\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    \n",
        "    # Find rank of first ground truth\n",
        "    for rank, idx in enumerate(sorted_indices, start=1):\n",
        "        if idx in ground_truth_indices:\n",
        "            return 1.0 / rank\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "def compute_first_hit_rank(scores: np.ndarray, ground_truth_indices: List[int]) -> float:\n",
        "    \"\"\"\n",
        "    Compute rank of first correct result (first hit rank).\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "    \n",
        "    Returns:\n",
        "        first_hit_rank: rank of first correct result (1-indexed), or len(scores)+1 if not found\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return float(len(scores) + 1)\n",
        "    \n",
        "    # Sort by score (descending)\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    \n",
        "    # Find rank of first ground truth\n",
        "    for rank, idx in enumerate(sorted_indices, start=1):\n",
        "        if idx in ground_truth_indices:\n",
        "            return float(rank)\n",
        "    \n",
        "    return float(len(scores) + 1)  # Not found\n",
        "\n",
        "def evaluate_text_to_image_by_recipe(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    k_values: List[int],\n",
        "    should_compute_mrr: bool = True,\n",
        "    should_compute_median_rank: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate Text → Image retrieval by recipe_id.\n",
        "    \n",
        "    For each recipe_id, use its representative caption to retrieve images.\n",
        "    Ground truth: all images from that recipe_id.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating Text → Image retrieval (by recipe_id)...\")\n",
        "    \n",
        "    unique_recipe_ids = test_df[\"recipe_id\"].unique()\n",
        "    \n",
        "    recalls = {f\"R@{k}\": [] for k in k_values}\n",
        "    mrr_scores = [] if should_compute_mrr else None\n",
        "    median_ranks = [] if should_compute_median_rank else None\n",
        "    \n",
        "    for recipe_id in tqdm(unique_recipe_ids, desc=\"Processing recipes\"):\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption embedding (first caption for this recipe)\n",
        "        caption_indices = recipe_to_caption_indices.get(recipe_id_str, [])\n",
        "        if not caption_indices:\n",
        "            continue\n",
        "        \n",
        "        # Use first caption (or could average multiple)\n",
        "        text_idx = caption_indices[0]\n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        \n",
        "        # Compute similarities with all images\n",
        "        similarities = image_embeddings @ text_emb  # (n_images,)\n",
        "        \n",
        "        # Get ground truth: all images from this recipe_id\n",
        "        ground_truth_indices = recipe_to_image_indices.get(recipe_id_str, [])\n",
        "        ground_truth_indices = [idx for idx in ground_truth_indices if idx is not None]\n",
        "        \n",
        "        if len(ground_truth_indices) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Compute metrics\n",
        "        for k in k_values:\n",
        "            recall = compute_recall_at_k(similarities, ground_truth_indices, k)\n",
        "            recalls[f\"R@{k}\"].append(recall)\n",
        "        \n",
        "        if should_compute_mrr:\n",
        "            mrr = compute_mrr(similarities, ground_truth_indices)\n",
        "            mrr_scores.append(mrr)\n",
        "        \n",
        "        if should_compute_median_rank:\n",
        "            first_hit_rank = compute_first_hit_rank(similarities, ground_truth_indices)\n",
        "            median_ranks.append(first_hit_rank)\n",
        "    \n",
        "    # Average metrics\n",
        "    results = {}\n",
        "    for k in k_values:\n",
        "        results[f\"R@{k}\"] = float(np.mean(recalls[f\"R@{k}\"])) if recalls[f\"R@{k}\"] else 0.0\n",
        "    \n",
        "    if should_compute_mrr and mrr_scores:\n",
        "        results[\"MRR\"] = float(np.mean(mrr_scores))\n",
        "    \n",
        "    if should_compute_median_rank and median_ranks:\n",
        "        results[\"MedianRank_first_hit\"] = float(np.median(median_ranks))\n",
        "        results[\"MeanRank_first_hit\"] = float(np.mean(median_ranks))\n",
        "    \n",
        "    return results\n",
        "\n",
        "def evaluate_image_to_text_by_recipe(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    k_values: List[int],\n",
        "    should_compute_mrr: bool = True,\n",
        "    should_compute_median_rank: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate Image → Text retrieval by recipe_id.\n",
        "    \n",
        "    For each image, retrieve captions. Ground truth: ALL captions from the same recipe_id.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating Image → Text retrieval (by recipe_id)...\")\n",
        "    \n",
        "    unique_images = test_df[\"image_key\"].unique()\n",
        "    \n",
        "    recalls = {f\"R@{k}\": [] for k in k_values}\n",
        "    mrr_scores = [] if should_compute_mrr else None\n",
        "    median_ranks = [] if should_compute_median_rank else None\n",
        "    \n",
        "    for img_key in tqdm(unique_images, desc=\"Processing images\"):\n",
        "        # Get image embedding\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        \n",
        "        # Compute similarities with all captions\n",
        "        similarities = text_embeddings @ img_emb  # (n_texts,)\n",
        "        \n",
        "        # Get recipe_id for this image\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        \n",
        "        # Get ground truth: ALL captions from the same recipe_id (more fair)\n",
        "        ground_truth_indices = recipe_to_all_caption_indices.get(recipe_id_str, [])\n",
        "        ground_truth_indices = [idx for idx in ground_truth_indices if idx is not None]\n",
        "        \n",
        "        if len(ground_truth_indices) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Compute metrics\n",
        "        for k in k_values:\n",
        "            recall = compute_recall_at_k(similarities, ground_truth_indices, k)\n",
        "            recalls[f\"R@{k}\"].append(recall)\n",
        "        \n",
        "        if should_compute_mrr:\n",
        "            mrr = compute_mrr(similarities, ground_truth_indices)\n",
        "            mrr_scores.append(mrr)\n",
        "        \n",
        "        if should_compute_median_rank:\n",
        "            first_hit_rank = compute_first_hit_rank(similarities, ground_truth_indices)\n",
        "            median_ranks.append(first_hit_rank)\n",
        "    \n",
        "    # Average metrics\n",
        "    results = {}\n",
        "    for k in k_values:\n",
        "        results[f\"R@{k}\"] = float(np.mean(recalls[f\"R@{k}\"])) if recalls[f\"R@{k}\"] else 0.0\n",
        "    \n",
        "    if should_compute_mrr and mrr_scores:\n",
        "        results[\"MRR\"] = float(np.mean(mrr_scores))\n",
        "    \n",
        "    if should_compute_median_rank and median_ranks:\n",
        "        results[\"MedianRank_first_hit\"] = float(np.median(median_ranks))\n",
        "        results[\"MeanRank_first_hit\"] = float(np.mean(median_ranks))\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run evaluation\n",
        "print(\"=\" * 60)\n",
        "print(\"Running Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "eval_start_time = time.time()\n",
        "\n",
        "# Text → Image retrieval (by recipe_id)\n",
        "text_to_image_results = evaluate_text_to_image_by_recipe(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_indices,\n",
        "    recipe_to_image_indices,\n",
        "    image_key_to_idx,\n",
        "    K_VALUES,\n",
        "    COMPUTE_MRR,\n",
        "    should_compute_median_rank=True\n",
        ")\n",
        "\n",
        "# Image → Text retrieval (by recipe_id)\n",
        "image_to_text_results = evaluate_image_to_text_by_recipe(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    K_VALUES,\n",
        "    COMPUTE_MRR,\n",
        "    should_compute_median_rank=True\n",
        ")\n",
        "\n",
        "eval_time = time.time() - eval_start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Evaluation Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Mapping for display names (only for specific metrics)\n",
        "METRIC_DISPLAY_NAMES = {\n",
        "    \"MedianRank_first_hit\": \"MedianRank\",\n",
        "    \"MeanRank_first_hit\": \"MeanRank\"\n",
        "}\n",
        "\n",
        "print(\"\\n📊 Text → Image Retrieval:\")\n",
        "for metric, value in text_to_image_results.items():\n",
        "    # Display with shorter names for readability (only for specific metrics)\n",
        "    display_name = METRIC_DISPLAY_NAMES.get(metric, metric)\n",
        "    # Format ranks with 2 decimals, other metrics with 4 decimals\n",
        "    if metric in (\"MedianRank_first_hit\", \"MeanRank_first_hit\"):\n",
        "        print(f\"  {display_name}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {display_name}: {value:.4f}\")\n",
        "\n",
        "print(\"\\n📊 Image → Text Retrieval:\")\n",
        "for metric, value in image_to_text_results.items():\n",
        "    # Display with shorter names for readability (only for specific metrics)\n",
        "    display_name = METRIC_DISPLAY_NAMES.get(metric, metric)\n",
        "    # Format ranks with 2 decimals, other metrics with 4 decimals\n",
        "    if metric in (\"MedianRank_first_hit\", \"MeanRank_first_hit\"):\n",
        "        print(f\"  {display_name}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {display_name}: {value:.4f}\")\n",
        "\n",
        "print(f\"\\n⏱ Evaluation time: {eval_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.5. Visualize Retrieval Results\n",
        "\n",
        "Visualize the retrieval results to see how well the fine-tuned model performs:\n",
        "- **Text → Image**: For each recipe, show the top-5 retrieved images\n",
        "- **Image → Text**: For each image, show the top-5 retrieved captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "print(\"=\" * 60)\n",
        "print(\"Visualizing Retrieval Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verify required variables exist (safety check)\n",
        "assert \"image_key_to_recipe_id\" in globals(), \"image_key_to_recipe_id not found. Run evaluation section first.\"\n",
        "assert \"text_embeddings\" in globals(), \"text_embeddings not found. Run evaluation section first.\"\n",
        "assert \"image_embeddings\" in globals(), \"image_embeddings not found. Run evaluation section first.\"\n",
        "assert \"unique_captions_list\" in globals(), \"unique_captions_list not found. Run evaluation section first.\"\n",
        "assert \"image_keys\" in globals(), \"image_keys not found. Run evaluation section first.\"\n",
        "\n",
        "print(\"\\n📝 Text → Image Retrieval (Top-5 images for each recipe):\")\n",
        "print(\"-\" * 60)\n",
        "visualize_text_to_image_retrieval(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_text,\n",
        "    recipe_to_image_indices,\n",
        "    caption_to_idx,\n",
        "    image_keys,\n",
        "    image_key_to_recipe_id,\n",
        "    n_examples=3\n",
        ")\n",
        "\n",
        "print(\"\\n🖼️ Image → Text Retrieval (Top-5 captions for each image):\")\n",
        "print(\"-\" * 60)\n",
        "visualize_image_to_text_retrieval(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    recipe_to_caption_text,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    unique_captions_list,\n",
        "    n_examples=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Generate Qualitative Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_top5_examples(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    caption_to_idx: Dict[str, int],\n",
        "    image_keys: List[str],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    n_examples: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate qualitative examples showing top-5 retrievals by recipe_id.\n",
        "    \"\"\"\n",
        "    \n",
        "    examples = []\n",
        "    \n",
        "    # Filter recipes with valid ground truth before sampling (avoid bias)\n",
        "    valid_recipe_ids = []\n",
        "    for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        gt_image_indices = recipe_to_image_indices.get(recipe_id_str, [])\n",
        "        if len(gt_image_indices) > 0:  # Only include if has valid GT\n",
        "            valid_recipe_ids.append(recipe_id)\n",
        "    \n",
        "    # Sample random recipes from valid ones\n",
        "    if len(valid_recipe_ids) == 0:\n",
        "        print(\"⚠ No valid recipes with ground truth images found\")\n",
        "        return examples\n",
        "    \n",
        "    sample_recipe_ids = pd.Series(valid_recipe_ids).sample(\n",
        "        min(n_examples, len(valid_recipe_ids)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for recipe_id in sample_recipe_ids:\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption for this recipe\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        \n",
        "        # Get caption embedding\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        \n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        similarities = image_embeddings @ text_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        \n",
        "        # Get ground truth: images from this recipe_id\n",
        "        gt_image_indices = set(recipe_to_image_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Get top-5 retrieved images\n",
        "        top5_images = []\n",
        "        for i, idx in enumerate(top5_indices):\n",
        "            img_key = image_keys[idx]\n",
        "            img_recipe_id = image_key_to_recipe_id.get(img_key)\n",
        "            is_correct = idx in gt_image_indices\n",
        "            \n",
        "            top5_images.append({\n",
        "                \"rank\": i + 1,\n",
        "                \"image_key\": img_key,\n",
        "                \"recipe_id\": img_recipe_id,\n",
        "                \"similarity\": float(similarities[idx]),\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "        \n",
        "        examples.append({\n",
        "            \"query_type\": \"text_to_image\",\n",
        "            \"recipe_id\": recipe_id_str,\n",
        "            \"query_caption\": caption,\n",
        "            \"ground_truth_recipe_id\": recipe_id_str,\n",
        "            \"top5_results\": top5_images\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "def generate_image_to_text_examples(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    unique_captions: List[str],\n",
        "    n_examples: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate qualitative examples showing top-5 captions for images (Image→Text retrieval).\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    \n",
        "    # Filter images with valid ground truth before sampling (avoid bias)\n",
        "    valid_images = []\n",
        "    for img_key in test_df[\"image_key\"].unique():\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        gt_caption_indices = recipe_to_all_caption_indices.get(recipe_id_str, [])\n",
        "        if len(gt_caption_indices) > 0:  # Only include if has valid GT\n",
        "            valid_images.append(img_key)\n",
        "    \n",
        "    # Sample random images from valid ones\n",
        "    if len(valid_images) == 0:\n",
        "        print(\"⚠ No valid images with ground truth captions found\")\n",
        "        return examples\n",
        "    \n",
        "    sample_images = pd.Series(valid_images).sample(\n",
        "        min(n_examples, len(valid_images)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for img_key in sample_images:\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        similarities = text_embeddings @ img_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        \n",
        "        # Get ground truth: recipe_id for this image\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        \n",
        "        # Get ground truth: all captions from this recipe_id\n",
        "        gt_caption_indices = set(recipe_to_all_caption_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Get top-5 retrieved captions\n",
        "        top5_captions = []\n",
        "        for i, idx in enumerate(top5_indices):\n",
        "            caption = unique_captions[idx]\n",
        "            is_correct = idx in gt_caption_indices\n",
        "            \n",
        "            top5_captions.append({\n",
        "                \"rank\": i + 1,\n",
        "                \"caption\": caption,\n",
        "                \"caption_index\": int(idx),\n",
        "                \"similarity\": float(similarities[idx]),\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "        \n",
        "        examples.append({\n",
        "            \"query_type\": \"image_to_text\",\n",
        "            \"recipe_id\": recipe_id_str,\n",
        "            \"query_image_key\": img_key,\n",
        "            \"recipe_representative_caption\": recipe_to_caption_text.get(recipe_id_str),\n",
        "            \"ground_truth_recipe_id\": recipe_id_str,\n",
        "            \"top5_results\": top5_captions\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "# Generate examples\n",
        "N_QUALITATIVE_EXAMPLES = 5\n",
        "print(\"Generating qualitative examples...\")\n",
        "\n",
        "# Verify unique_captions_list exists (should be from evaluation section)\n",
        "assert \"unique_captions_list\" in globals(), \"unique_captions_list not found. Run evaluation section first.\"\n",
        "\n",
        "# Text → Image examples\n",
        "text_to_image_examples = generate_top5_examples(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_text,\n",
        "    recipe_to_image_indices,\n",
        "    caption_to_idx,\n",
        "    image_keys,\n",
        "    image_key_to_recipe_id,\n",
        "    n_examples=N_QUALITATIVE_EXAMPLES\n",
        ")\n",
        "\n",
        "# Image → Text examples\n",
        "image_to_text_examples = generate_image_to_text_examples(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    recipe_to_caption_text,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    unique_captions=unique_captions_list,\n",
        "    n_examples=N_QUALITATIVE_EXAMPLES\n",
        ")\n",
        "\n",
        "print(f\"✓ Generated {len(text_to_image_examples)} text→image examples (requested: {N_QUALITATIVE_EXAMPLES})\")\n",
        "print(f\"✓ Generated {len(image_to_text_examples)} image→text examples (requested: {N_QUALITATIVE_EXAMPLES})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results dictionary\n",
        "results = {\n",
        "    \"metadata\": {\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"method\": \"lora\",\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"device\": str(main_device),\n",
        "        \"train_dataset\": TRAIN_AUGMENTED_KEY,\n",
        "        \"test_dataset\": TEST_MANIFEST_KEY,\n",
        "        \"filter_applied\": \"label == 1 (positive pairs only) for training\",\n",
        "    },\n",
        "    \"model_info\": {\n",
        "        \"total_parameters\": int(total_params),\n",
        "        \"trainable_parameters\": int(trainable_params),\n",
        "        \"trainable_ratio\": float(trainable_params / total_params),\n",
        "        \"frozen\": False,\n",
        "    },\n",
        "    \"training_info\": {\n",
        "        \"total_time_seconds\": training_logs[\"total_time_seconds\"],\n",
        "        \"peak_memory_mb\": training_logs[\"peak_memory_mb\"],\n",
        "        \"total_steps\": training_logs[\"total_steps\"],\n",
        "        \"num_epochs\": TRAINING_CONFIG[\"num_epochs\"],\n",
        "        \"final_loss\": float(training_logs[\"epochs\"][-1][\"avg_loss\"]) if training_logs[\"epochs\"] else None,\n",
        "    },\n",
        "    \"dataset_info\": {\n",
        "        \"test_pairs\": len(test_df),\n",
        "        \"test_recipes\": test_df[\"recipe_id\"].nunique(),\n",
        "        \"test_images\": test_df[\"image_key\"].nunique(),\n",
        "        \"test_captions\": test_df[\"caption\"].nunique(),\n",
        "    },\n",
        "    \"metrics\": {\n",
        "        \"text_to_image\": text_to_image_results,\n",
        "        \"image_to_text\": image_to_text_results,\n",
        "    },\n",
        "    \"timing\": {\n",
        "        \"embedding_generation_seconds\": embedding_time,\n",
        "        \"evaluation_seconds\": eval_time,\n",
        "        \"total_seconds\": embedding_time + eval_time,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Save results\n",
        "print(\"=\" * 60)\n",
        "print(\"Saving Results to MinIO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "save_json_to_minio(results, FINE_TUNING_BUCKET, RESULTS_KEY)\n",
        "save_json_to_minio({\n",
        "    \"text_to_image_examples\": text_to_image_examples,\n",
        "    \"image_to_text_examples\": image_to_text_examples\n",
        "}, FINE_TUNING_BUCKET, EXAMPLES_KEY)\n",
        "\n",
        "print(f\"\\n✅ Results saved successfully!\")\n",
        "print(f\"  Results: s3://{FINE_TUNING_BUCKET}/{RESULTS_KEY}\")\n",
        "print(f\"  Examples: s3://{FINE_TUNING_BUCKET}/{EXAMPLES_KEY}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Run ID: {RUN_ID}\")\n",
        "print(f\"Model: {MODEL_NAME} (LoRA fine-tuned)\")\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
        "print(f\"Training time: {training_time/60:.2f} minutes\")\n",
        "print(f\"Peak memory: {peak_memory:.2f} MB\")\n",
        "print(f\"\\nText → Image:\")\n",
        "print(f\"  R@1: {text_to_image_results.get('R@1', 0):.4f}\")\n",
        "print(f\"  R@5: {text_to_image_results.get('R@5', 0):.4f}\")\n",
        "print(f\"  R@10: {text_to_image_results.get('R@10', 0):.4f}\")\n",
        "if \"MRR\" in text_to_image_results:\n",
        "    print(f\"  MRR: {text_to_image_results['MRR']:.4f}\")\n",
        "print(f\"\\nImage → Text:\")\n",
        "print(f\"  R@1: {image_to_text_results.get('R@1', 0):.4f}\")\n",
        "print(f\"  R@5: {image_to_text_results.get('R@5', 0):.4f}\")\n",
        "print(f\"  R@10: {image_to_text_results.get('R@10', 0):.4f}\")\n",
        "if \"MRR\" in image_to_text_results:\n",
        "    print(f\"  MRR: {image_to_text_results['MRR']:.4f}\")\n",
        "print(f\"\\n📁 All outputs saved to: s3://{FINE_TUNING_BUCKET}/{RUN_DIR}/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_json_to_minio(data: Dict, bucket: str, key: str) -> bool:\n",
        "    \"\"\"Save dictionary as JSON to MinIO.\"\"\"\n",
        "    try:\n",
        "        json_bytes = json.dumps(data, indent=2).encode(\"utf-8\")\n",
        "        s3.put_object(\n",
        "            Bucket=bucket,\n",
        "            Key=key,\n",
        "            Body=json_bytes,\n",
        "            ContentType=\"application/json\",\n",
        "        )\n",
        "        size_kb = len(json_bytes) / 1024\n",
        "        print(f\"✓ Saved to s3://{bucket}/{key} ({size_kb:.1f} KB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to save: {e}\")\n",
        "        return False\n",
        "\n",
        "def save_yaml_to_minio(data: Dict, bucket: str, key: str) -> bool:\n",
        "    \"\"\"Save dictionary as YAML to MinIO.\"\"\"\n",
        "    try:\n",
        "        yaml_str = yaml.dump(data, default_flow_style=False, sort_keys=False)\n",
        "        yaml_bytes = yaml_str.encode(\"utf-8\")\n",
        "        s3.put_object(\n",
        "            Bucket=bucket,\n",
        "            Key=key,\n",
        "            Body=yaml_bytes,\n",
        "            ContentType=\"text/yaml\",\n",
        "        )\n",
        "        size_kb = len(yaml_bytes) / 1024\n",
        "        print(f\"✓ Saved to s3://{bucket}/{key} ({size_kb:.1f} KB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to save: {e}\")\n",
        "        return False\n",
        "\n",
        "# Save adapters locally first, then upload to MinIO\n",
        "print(\"Saving adapters...\")\n",
        "local_adapters_dir = Path(f\"adapters_{RUN_ID}\")\n",
        "model.save_pretrained(str(local_adapters_dir))\n",
        "\n",
        "# Upload adapters to MinIO (upload config and weights - supports both .bin and .safetensors)\n",
        "print(\"Uploading adapters to MinIO...\")\n",
        "adapter_files = [\"adapter_config.json\", \"adapter_model.bin\", \"adapter_model.safetensors\"]\n",
        "for file_name in adapter_files:\n",
        "    file_path = local_adapters_dir / file_name\n",
        "    if file_path.exists():\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            s3.put_object(\n",
        "                Bucket=FINE_TUNING_BUCKET,\n",
        "                Key=f\"{ADAPTERS_DIR}/{file_name}\",\n",
        "                Body=f.read(),\n",
        "            )\n",
        "        print(f\"  ✓ Uploaded {file_name}\")\n",
        "\n",
        "# Save training logs\n",
        "print(\"\\nSaving training logs...\")\n",
        "save_json_to_minio(training_logs, FINE_TUNING_BUCKET, LOGS_KEY)\n",
        "\n",
        "# Save configuration\n",
        "config_data = {\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"method\": \"lora\",\n",
        "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"lora_config\": LORA_CONFIG,\n",
        "    \"training_config\": TRAINING_CONFIG,\n",
        "    \"model_info\": {\n",
        "        \"total_parameters\": int(total_params),\n",
        "        \"trainable_parameters\": int(trainable_params),\n",
        "        \"trainable_ratio\": float(trainable_params / total_params),\n",
        "    },\n",
        "}\n",
        "save_yaml_to_minio(config_data, FINE_TUNING_BUCKET, CONFIG_KEY)\n",
        "\n",
        "print(f\"\\n✅ Adapters and logs saved successfully!\")\n",
        "print(f\"  Adapters: s3://{FINE_TUNING_BUCKET}/{ADAPTERS_DIR}/\")\n",
        "print(f\"  Logs: s3://{FINE_TUNING_BUCKET}/{LOGS_KEY}\")\n",
        "print(f\"  Config: s3://{FINE_TUNING_BUCKET}/{CONFIG_KEY}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluation (Same Protocol as Baseline)\n",
        "\n",
        "Now we evaluate the fine-tuned model using the same evaluation functions as the baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Generate embeddings for test set (same as baseline)\n",
        "print(\"=\" * 60)\n",
        "print(\"Generating Embeddings for Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def generate_image_embeddings(\n",
        "    df: pd.DataFrame, \n",
        "    model, \n",
        "    processor, \n",
        "    device: Union[str, torch.device], \n",
        "    batch_size: int = 32\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"Generate image embeddings for all images in the dataset.\"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    image_keys = []\n",
        "    failed_keys = []\n",
        "    \n",
        "    unique_images = df[\"image_key\"].unique()\n",
        "    print(f\"Generating embeddings for {len(unique_images)} unique images...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(unique_images), batch_size)):\n",
        "            batch_keys = unique_images[i:i+batch_size]\n",
        "            batch_images = []\n",
        "            valid_keys = []\n",
        "            \n",
        "            for img_key in batch_keys:\n",
        "                bucket, key = get_image_path_in_minio(img_key)\n",
        "                img = load_image_from_minio(bucket, key)\n",
        "                if img is not None:\n",
        "                    batch_images.append(img)\n",
        "                    valid_keys.append(img_key)\n",
        "                else:\n",
        "                    failed_keys.append(img_key)\n",
        "            \n",
        "            if not batch_images:\n",
        "                continue\n",
        "            \n",
        "            inputs = processor(images=batch_images, return_tensors=\"pt\", padding=True)\n",
        "            # Move inputs to device explicitly (more robust than .to(device) on BatchEncoding)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            outputs = model.get_image_features(**inputs)\n",
        "            batch_emb = F.normalize(outputs, p=2, dim=1)\n",
        "            \n",
        "            embeddings.append(batch_emb.cpu().numpy())\n",
        "            image_keys.extend(valid_keys)\n",
        "    \n",
        "    if failed_keys:\n",
        "        print(f\"⚠ Failed to load {len(failed_keys)} images\")\n",
        "    \n",
        "    embeddings = np.vstack(embeddings) if embeddings else np.array([])\n",
        "    print(f\"✓ Generated {len(image_keys)} image embeddings (shape: {embeddings.shape})\")\n",
        "    return embeddings, image_keys\n",
        "\n",
        "def generate_text_embeddings(\n",
        "    texts: List[str], \n",
        "    model, \n",
        "    processor, \n",
        "    device: Union[str, torch.device], \n",
        "    batch_size: int = 32\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Generate text embeddings for all captions.\"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    \n",
        "    print(f\"Generating embeddings for {len(texts)} texts...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(texts), batch_size)):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            inputs = processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            # Move inputs to device explicitly (more robust than .to(device) on BatchEncoding)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            outputs = model.get_text_features(**inputs)\n",
        "            batch_emb = F.normalize(outputs, p=2, dim=1)\n",
        "            embeddings.append(batch_emb.cpu().numpy())\n",
        "    \n",
        "    embeddings = np.vstack(embeddings) if embeddings else np.array([])\n",
        "    print(f\"✓ Generated {len(texts)} text embeddings (shape: {embeddings.shape})\")\n",
        "    return embeddings\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Generate embeddings\n",
        "image_embeddings, image_keys = generate_image_embeddings(\n",
        "    test_df, model, processor, main_device, batch_size=16  # Smaller batch for LoRA\n",
        ")\n",
        "\n",
        "unique_captions = test_df[\"caption\"].unique().tolist()\n",
        "text_embeddings = generate_text_embeddings(unique_captions, model, processor, main_device, batch_size=16)\n",
        "\n",
        "# Create official copy to ensure alignment with text_embeddings (never modify this)\n",
        "unique_captions_list = list(unique_captions)  # Explicit copy to prevent accidental modification\n",
        "\n",
        "embedding_time = time.time() - start_time\n",
        "print(f\"\\n✓ Embedding generation complete in {embedding_time:.2f} seconds\")\n",
        "\n",
        "# Create mapping dictionaries\n",
        "image_key_to_idx = {key: idx for idx, key in enumerate(image_keys)}\n",
        "caption_to_idx = {caption: idx for idx, caption in enumerate(unique_captions_list)}\n",
        "\n",
        "# Precompute lookup dictionaries for O(1) access (same as baseline)\n",
        "print(\"\\nPrecomputing lookup dictionaries...\")\n",
        "\n",
        "# recipe_id -> list of image indices\n",
        "recipe_to_image_indices: Dict[str, List[int]] = {}\n",
        "for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "    recipe_id_str = str(recipe_id)\n",
        "    recipe_images = test_df[test_df[\"recipe_id\"] == recipe_id][\"image_key\"].unique()\n",
        "    recipe_to_image_indices[recipe_id_str] = [\n",
        "        image_key_to_idx.get(img_key) \n",
        "        for img_key in recipe_images \n",
        "        if image_key_to_idx.get(img_key) is not None\n",
        "    ]\n",
        "\n",
        "# recipe_id -> list of caption indices (one caption per recipe for text→image query)\n",
        "# Use longest caption as representative (most informative)\n",
        "recipe_to_caption_indices: Dict[str, List[int]] = {}\n",
        "recipe_to_caption_text: Dict[str, str] = {}\n",
        "# recipe_id -> list of ALL caption indices (for image→text ground truth)\n",
        "recipe_to_all_caption_indices: Dict[str, List[int]] = {}\n",
        "for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "    recipe_id_str = str(recipe_id)\n",
        "    recipe_captions = test_df[test_df[\"recipe_id\"] == recipe_id][\"caption\"].unique()\n",
        "    if len(recipe_captions) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Use longest caption as representative (most informative)\n",
        "    # If multiple have same length, use first after sorting alphabetically (deterministic)\n",
        "    representative_caption = max(recipe_captions, key=lambda c: (len(c), c))\n",
        "    \n",
        "    # Store representative caption for text→image queries\n",
        "    cap_idx = caption_to_idx.get(representative_caption)\n",
        "    if cap_idx is not None:\n",
        "        recipe_to_caption_indices[recipe_id_str] = [cap_idx]\n",
        "        recipe_to_caption_text[recipe_id_str] = representative_caption\n",
        "    \n",
        "    # Store ALL captions for image→text ground truth (more fair if multiple captions per recipe)\n",
        "    all_caption_indices = []\n",
        "    for caption in recipe_captions:\n",
        "        cap_idx = caption_to_idx.get(caption)\n",
        "        if cap_idx is not None:\n",
        "            all_caption_indices.append(cap_idx)\n",
        "    recipe_to_all_caption_indices[recipe_id_str] = sorted(set(all_caption_indices))  # Remove duplicates, sorted for determinism\n",
        "\n",
        "# image_key -> recipe_id (as string for consistency)\n",
        "image_key_to_recipe_id: Dict[str, str] = {}\n",
        "for _, row in test_df.iterrows():\n",
        "    if row[\"image_key\"] not in image_key_to_recipe_id:\n",
        "        image_key_to_recipe_id[row[\"image_key\"]] = str(row[\"recipe_id\"])\n",
        "\n",
        "print(f\"✓ Precomputed lookups for {len(recipe_to_image_indices)} recipes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Evaluation Functions (Same as Baseline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy evaluation functions from baseline (same protocol by recipe_id)\n",
        "def compute_recall_at_k(scores: np.ndarray, ground_truth_indices: List[int], k: int) -> float:\n",
        "    \"\"\"\n",
        "    Compute Recall@K.\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "        k: top-K to consider\n",
        "    \n",
        "    Returns:\n",
        "        recall@k: 1.0 if any ground truth is in top-K, else 0.0\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    top_k_indices = np.argsort(scores)[-k:][::-1]  # Top K, highest first\n",
        "    top_k_set = set(top_k_indices)\n",
        "    \n",
        "    # Check if any ground truth is in top-K\n",
        "    for gt_idx in ground_truth_indices:\n",
        "        if gt_idx in top_k_set:\n",
        "            return 1.0\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "def compute_mrr(scores: np.ndarray, ground_truth_indices: List[int]) -> float:\n",
        "    \"\"\"\n",
        "    Compute Mean Reciprocal Rank.\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "    \n",
        "    Returns:\n",
        "        mrr: 1/rank of first correct result, or 0.0 if none found\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Sort by score (descending)\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    \n",
        "    # Find rank of first ground truth\n",
        "    for rank, idx in enumerate(sorted_indices, start=1):\n",
        "        if idx in ground_truth_indices:\n",
        "            return 1.0 / rank\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "def compute_first_hit_rank(scores: np.ndarray, ground_truth_indices: List[int]) -> float:\n",
        "    \"\"\"\n",
        "    Compute rank of first correct result (first hit rank).\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "    \n",
        "    Returns:\n",
        "        first_hit_rank: rank of first correct result (1-indexed), or len(scores)+1 if not found\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return float(len(scores) + 1)\n",
        "    \n",
        "    # Sort by score (descending)\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    \n",
        "    # Find rank of first ground truth\n",
        "    for rank, idx in enumerate(sorted_indices, start=1):\n",
        "        if idx in ground_truth_indices:\n",
        "            return float(rank)\n",
        "    \n",
        "    return float(len(scores) + 1)  # Not found\n",
        "\n",
        "def evaluate_text_to_image_by_recipe(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    k_values: List[int],\n",
        "    should_compute_mrr: bool = True,\n",
        "    should_compute_median_rank: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate Text → Image retrieval by recipe_id.\n",
        "    \n",
        "    For each recipe_id, use its representative caption to retrieve images.\n",
        "    Ground truth: all images from that recipe_id.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating Text → Image retrieval (by recipe_id)...\")\n",
        "    \n",
        "    unique_recipe_ids = test_df[\"recipe_id\"].unique()\n",
        "    \n",
        "    recalls = {f\"R@{k}\": [] for k in k_values}\n",
        "    mrr_scores = [] if should_compute_mrr else None\n",
        "    median_ranks = [] if should_compute_median_rank else None\n",
        "    \n",
        "    for recipe_id in tqdm(unique_recipe_ids, desc=\"Processing recipes\"):\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption embedding (first caption for this recipe)\n",
        "        caption_indices = recipe_to_caption_indices.get(recipe_id_str, [])\n",
        "        if not caption_indices:\n",
        "            continue\n",
        "        \n",
        "        # Use first caption (or could average multiple)\n",
        "        text_idx = caption_indices[0]\n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        \n",
        "        # Compute similarities with all images\n",
        "        similarities = image_embeddings @ text_emb  # (n_images,)\n",
        "        \n",
        "        # Get ground truth: all images from this recipe_id\n",
        "        ground_truth_indices = recipe_to_image_indices.get(recipe_id_str, [])\n",
        "        ground_truth_indices = [idx for idx in ground_truth_indices if idx is not None]\n",
        "        \n",
        "        if len(ground_truth_indices) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Compute metrics\n",
        "        for k in k_values:\n",
        "            recall = compute_recall_at_k(similarities, ground_truth_indices, k)\n",
        "            recalls[f\"R@{k}\"].append(recall)\n",
        "        \n",
        "        if should_compute_mrr:\n",
        "            mrr = compute_mrr(similarities, ground_truth_indices)\n",
        "            mrr_scores.append(mrr)\n",
        "        \n",
        "        if should_compute_median_rank:\n",
        "            first_hit_rank = compute_first_hit_rank(similarities, ground_truth_indices)\n",
        "            median_ranks.append(first_hit_rank)\n",
        "    \n",
        "    # Average metrics\n",
        "    results = {}\n",
        "    for k in k_values:\n",
        "        results[f\"R@{k}\"] = float(np.mean(recalls[f\"R@{k}\"])) if recalls[f\"R@{k}\"] else 0.0\n",
        "    \n",
        "    if should_compute_mrr and mrr_scores:\n",
        "        results[\"MRR\"] = float(np.mean(mrr_scores))\n",
        "    \n",
        "    if should_compute_median_rank and median_ranks:\n",
        "        results[\"MedianRank_first_hit\"] = float(np.median(median_ranks))\n",
        "        results[\"MeanRank_first_hit\"] = float(np.mean(median_ranks))\n",
        "    \n",
        "    return results\n",
        "\n",
        "def evaluate_image_to_text_by_recipe(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    k_values: List[int],\n",
        "    should_compute_mrr: bool = True,\n",
        "    should_compute_median_rank: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate Image → Text retrieval by recipe_id.\n",
        "    \n",
        "    For each image, retrieve captions. Ground truth: ALL captions from the same recipe_id.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating Image → Text retrieval (by recipe_id)...\")\n",
        "    \n",
        "    unique_images = test_df[\"image_key\"].unique()\n",
        "    \n",
        "    recalls = {f\"R@{k}\": [] for k in k_values}\n",
        "    mrr_scores = [] if should_compute_mrr else None\n",
        "    median_ranks = [] if should_compute_median_rank else None\n",
        "    \n",
        "    for img_key in tqdm(unique_images, desc=\"Processing images\"):\n",
        "        # Get image embedding\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        \n",
        "        # Compute similarities with all captions\n",
        "        similarities = text_embeddings @ img_emb  # (n_texts,)\n",
        "        \n",
        "        # Get recipe_id for this image\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        \n",
        "        # Get ground truth: ALL captions from the same recipe_id (more fair)\n",
        "        ground_truth_indices = recipe_to_all_caption_indices.get(recipe_id_str, [])\n",
        "        ground_truth_indices = [idx for idx in ground_truth_indices if idx is not None]\n",
        "        \n",
        "        if len(ground_truth_indices) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Compute metrics\n",
        "        for k in k_values:\n",
        "            recall = compute_recall_at_k(similarities, ground_truth_indices, k)\n",
        "            recalls[f\"R@{k}\"].append(recall)\n",
        "        \n",
        "        if should_compute_mrr:\n",
        "            mrr = compute_mrr(similarities, ground_truth_indices)\n",
        "            mrr_scores.append(mrr)\n",
        "        \n",
        "        if should_compute_median_rank:\n",
        "            first_hit_rank = compute_first_hit_rank(similarities, ground_truth_indices)\n",
        "            median_ranks.append(first_hit_rank)\n",
        "    \n",
        "    # Average metrics\n",
        "    results = {}\n",
        "    for k in k_values:\n",
        "        results[f\"R@{k}\"] = float(np.mean(recalls[f\"R@{k}\"])) if recalls[f\"R@{k}\"] else 0.0\n",
        "    \n",
        "    if should_compute_mrr and mrr_scores:\n",
        "        results[\"MRR\"] = float(np.mean(mrr_scores))\n",
        "    \n",
        "    if should_compute_median_rank and median_ranks:\n",
        "        results[\"MedianRank_first_hit\"] = float(np.median(median_ranks))\n",
        "        results[\"MeanRank_first_hit\"] = float(np.mean(median_ranks))\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run evaluation\n",
        "print(\"=\" * 60)\n",
        "print(\"Running Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "eval_start_time = time.time()\n",
        "\n",
        "# Text → Image retrieval (by recipe_id)\n",
        "text_to_image_results = evaluate_text_to_image_by_recipe(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_indices,\n",
        "    recipe_to_image_indices,\n",
        "    image_key_to_idx,\n",
        "    K_VALUES,\n",
        "    COMPUTE_MRR,\n",
        "    should_compute_median_rank=True\n",
        ")\n",
        "\n",
        "# Image → Text retrieval (by recipe_id)\n",
        "image_to_text_results = evaluate_image_to_text_by_recipe(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    K_VALUES,\n",
        "    COMPUTE_MRR,\n",
        "    should_compute_median_rank=True\n",
        ")\n",
        "\n",
        "eval_time = time.time() - eval_start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Evaluation Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Mapping for display names (only for specific metrics)\n",
        "METRIC_DISPLAY_NAMES = {\n",
        "    \"MedianRank_first_hit\": \"MedianRank\",\n",
        "    \"MeanRank_first_hit\": \"MeanRank\"\n",
        "}\n",
        "\n",
        "print(\"\\n📊 Text → Image Retrieval:\")\n",
        "for metric, value in text_to_image_results.items():\n",
        "    # Display with shorter names for readability (only for specific metrics)\n",
        "    display_name = METRIC_DISPLAY_NAMES.get(metric, metric)\n",
        "    # Format ranks with 2 decimals, other metrics with 4 decimals\n",
        "    if metric in (\"MedianRank_first_hit\", \"MeanRank_first_hit\"):\n",
        "        print(f\"  {display_name}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {display_name}: {value:.4f}\")\n",
        "\n",
        "print(\"\\n📊 Image → Text Retrieval:\")\n",
        "for metric, value in image_to_text_results.items():\n",
        "    # Display with shorter names for readability (only for specific metrics)\n",
        "    display_name = METRIC_DISPLAY_NAMES.get(metric, metric)\n",
        "    # Format ranks with 2 decimals, other metrics with 4 decimals\n",
        "    if metric in (\"MedianRank_first_hit\", \"MeanRank_first_hit\"):\n",
        "        print(f\"  {display_name}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {display_name}: {value:.4f}\")\n",
        "\n",
        "print(f\"\\n⏱ Evaluation time: {eval_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.5. Visualize Retrieval Results\n",
        "\n",
        "Visualize the retrieval results to see how well the fine-tuned model performs:\n",
        "- **Text → Image**: For each recipe, show the top-5 retrieved images\n",
        "- **Image → Text**: For each image, show the top-5 retrieved captions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "print(\"=\" * 60)\n",
        "print(\"Visualizing Retrieval Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verify required variables exist (safety check)\n",
        "assert \"image_key_to_recipe_id\" in globals(), \"image_key_to_recipe_id not found. Run evaluation section first.\"\n",
        "assert \"text_embeddings\" in globals(), \"text_embeddings not found. Run evaluation section first.\"\n",
        "assert \"image_embeddings\" in globals(), \"image_embeddings not found. Run evaluation section first.\"\n",
        "assert \"unique_captions_list\" in globals(), \"unique_captions_list not found. Run evaluation section first.\"\n",
        "assert \"image_keys\" in globals(), \"image_keys not found. Run evaluation section first.\"\n",
        "\n",
        "print(\"\\n📝 Text → Image Retrieval (Top-5 images for each recipe):\")\n",
        "print(\"-\" * 60)\n",
        "visualize_text_to_image_retrieval(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_text,\n",
        "    recipe_to_image_indices,\n",
        "    caption_to_idx,\n",
        "    image_keys,\n",
        "    image_key_to_recipe_id,\n",
        "    n_examples=3\n",
        ")\n",
        "\n",
        "print(\"\\n🖼️ Image → Text Retrieval (Top-5 captions for each image):\")\n",
        "print(\"-\" * 60)\n",
        "visualize_image_to_text_retrieval(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    recipe_to_caption_text,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    unique_captions_list,\n",
        "    n_examples=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Generate Qualitative Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_top5_examples(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    caption_to_idx: Dict[str, int],\n",
        "    image_keys: List[str],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    n_examples: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate qualitative examples showing top-5 retrievals by recipe_id.\n",
        "    \"\"\"\n",
        "    \n",
        "    examples = []\n",
        "    \n",
        "    # Filter recipes with valid ground truth before sampling (avoid bias)\n",
        "    valid_recipe_ids = []\n",
        "    for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        gt_image_indices = recipe_to_image_indices.get(recipe_id_str, [])\n",
        "        if len(gt_image_indices) > 0:  # Only include if has valid GT\n",
        "            valid_recipe_ids.append(recipe_id)\n",
        "    \n",
        "    # Sample random recipes from valid ones\n",
        "    if len(valid_recipe_ids) == 0:\n",
        "        print(\"⚠ No valid recipes with ground truth images found\")\n",
        "        return examples\n",
        "    \n",
        "    sample_recipe_ids = pd.Series(valid_recipe_ids).sample(\n",
        "        min(n_examples, len(valid_recipe_ids)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for recipe_id in sample_recipe_ids:\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption for this recipe\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        \n",
        "        # Get caption embedding\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        \n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        similarities = image_embeddings @ text_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        \n",
        "        # Get ground truth: images from this recipe_id\n",
        "        gt_image_indices = set(recipe_to_image_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Get top-5 retrieved images\n",
        "        top5_images = []\n",
        "        for i, idx in enumerate(top5_indices):\n",
        "            img_key = image_keys[idx]\n",
        "            img_recipe_id = image_key_to_recipe_id.get(img_key)\n",
        "            is_correct = idx in gt_image_indices\n",
        "            \n",
        "            top5_images.append({\n",
        "                \"rank\": i + 1,\n",
        "                \"image_key\": img_key,\n",
        "                \"recipe_id\": img_recipe_id,\n",
        "                \"similarity\": float(similarities[idx]),\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "        \n",
        "        examples.append({\n",
        "            \"query_type\": \"text_to_image\",\n",
        "            \"recipe_id\": recipe_id_str,\n",
        "            \"query_caption\": caption,\n",
        "            \"ground_truth_recipe_id\": recipe_id_str,\n",
        "            \"top5_results\": top5_images\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "def generate_image_to_text_examples(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    unique_captions: List[str],\n",
        "    n_examples: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate qualitative examples showing top-5 captions for images (Image→Text retrieval).\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    \n",
        "    # Filter images with valid ground truth before sampling (avoid bias)\n",
        "    valid_images = []\n",
        "    for img_key in test_df[\"image_key\"].unique():\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        gt_caption_indices = recipe_to_all_caption_indices.get(recipe_id_str, [])\n",
        "        if len(gt_caption_indices) > 0:  # Only include if has valid GT\n",
        "            valid_images.append(img_key)\n",
        "    \n",
        "    # Sample random images from valid ones\n",
        "    if len(valid_images) == 0:\n",
        "        print(\"⚠ No valid images with ground truth captions found\")\n",
        "        return examples\n",
        "    \n",
        "    sample_images = pd.Series(valid_images).sample(\n",
        "        min(n_examples, len(valid_images)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for img_key in sample_images:\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        similarities = text_embeddings @ img_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        \n",
        "        # Get ground truth: recipe_id for this image\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        \n",
        "        # Get ground truth: all captions from this recipe_id\n",
        "        gt_caption_indices = set(recipe_to_all_caption_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Get top-5 retrieved captions\n",
        "        top5_captions = []\n",
        "        for i, idx in enumerate(top5_indices):\n",
        "            caption = unique_captions[idx]\n",
        "            is_correct = idx in gt_caption_indices\n",
        "            \n",
        "            top5_captions.append({\n",
        "                \"rank\": i + 1,\n",
        "                \"caption\": caption,\n",
        "                \"caption_index\": int(idx),\n",
        "                \"similarity\": float(similarities[idx]),\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "        \n",
        "        examples.append({\n",
        "            \"query_type\": \"image_to_text\",\n",
        "            \"recipe_id\": recipe_id_str,\n",
        "            \"query_image_key\": img_key,\n",
        "            \"recipe_representative_caption\": recipe_to_caption_text.get(recipe_id_str),\n",
        "            \"ground_truth_recipe_id\": recipe_id_str,\n",
        "            \"top5_results\": top5_captions\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "# Generate examples\n",
        "N_QUALITATIVE_EXAMPLES = 5\n",
        "print(\"Generating qualitative examples...\")\n",
        "\n",
        "# Verify unique_captions_list exists (should be from evaluation section)\n",
        "assert \"unique_captions_list\" in globals(), \"unique_captions_list not found. Run evaluation section first.\"\n",
        "\n",
        "# Text → Image examples\n",
        "text_to_image_examples = generate_top5_examples(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_text,\n",
        "    recipe_to_image_indices,\n",
        "    caption_to_idx,\n",
        "    image_keys,\n",
        "    image_key_to_recipe_id,\n",
        "    n_examples=N_QUALITATIVE_EXAMPLES\n",
        ")\n",
        "\n",
        "# Image → Text examples\n",
        "image_to_text_examples = generate_image_to_text_examples(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    recipe_to_caption_text,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    unique_captions=unique_captions_list,\n",
        "    n_examples=N_QUALITATIVE_EXAMPLES\n",
        ")\n",
        "\n",
        "print(f\"✓ Generated {len(text_to_image_examples)} text→image examples (requested: {N_QUALITATIVE_EXAMPLES})\")\n",
        "print(f\"✓ Generated {len(image_to_text_examples)} image→text examples (requested: {N_QUALITATIVE_EXAMPLES})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create results dictionary\n",
        "results = {\n",
        "    \"metadata\": {\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"method\": \"lora\",\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"device\": str(main_device),\n",
        "        \"train_dataset\": TRAIN_AUGMENTED_KEY,\n",
        "        \"test_dataset\": TEST_MANIFEST_KEY,\n",
        "        \"filter_applied\": \"label == 1 (positive pairs only) for training\",\n",
        "    },\n",
        "    \"model_info\": {\n",
        "        \"total_parameters\": int(total_params),\n",
        "        \"trainable_parameters\": int(trainable_params),\n",
        "        \"trainable_ratio\": float(trainable_params / total_params),\n",
        "        \"frozen\": False,\n",
        "    },\n",
        "    \"training_info\": {\n",
        "        \"total_time_seconds\": training_logs[\"total_time_seconds\"],\n",
        "        \"peak_memory_mb\": training_logs[\"peak_memory_mb\"],\n",
        "        \"total_steps\": training_logs[\"total_steps\"],\n",
        "        \"num_epochs\": TRAINING_CONFIG[\"num_epochs\"],\n",
        "        \"final_loss\": float(training_logs[\"epochs\"][-1][\"avg_loss\"]) if training_logs[\"epochs\"] else None,\n",
        "    },\n",
        "    \"dataset_info\": {\n",
        "        \"test_pairs\": len(test_df),\n",
        "        \"test_recipes\": test_df[\"recipe_id\"].nunique(),\n",
        "        \"test_images\": test_df[\"image_key\"].nunique(),\n",
        "        \"test_captions\": test_df[\"caption\"].nunique(),\n",
        "    },\n",
        "    \"metrics\": {\n",
        "        \"text_to_image\": text_to_image_results,\n",
        "        \"image_to_text\": image_to_text_results,\n",
        "    },\n",
        "    \"timing\": {\n",
        "        \"embedding_generation_seconds\": embedding_time,\n",
        "        \"evaluation_seconds\": eval_time,\n",
        "        \"total_seconds\": embedding_time + eval_time,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Save results\n",
        "print(\"=\" * 60)\n",
        "print(\"Saving Results to MinIO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "save_json_to_minio(results, FINE_TUNING_BUCKET, RESULTS_KEY)\n",
        "save_json_to_minio({\n",
        "    \"text_to_image_examples\": text_to_image_examples,\n",
        "    \"image_to_text_examples\": image_to_text_examples\n",
        "}, FINE_TUNING_BUCKET, EXAMPLES_KEY)\n",
        "\n",
        "print(f\"\\n✅ Results saved successfully!\")\n",
        "print(f\"  Results: s3://{FINE_TUNING_BUCKET}/{RESULTS_KEY}\")\n",
        "print(f\"  Examples: s3://{FINE_TUNING_BUCKET}/{EXAMPLES_KEY}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Run ID: {RUN_ID}\")\n",
        "print(f\"Model: {MODEL_NAME} (LoRA fine-tuned)\")\n",
        "print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
        "print(f\"Training time: {training_time/60:.2f} minutes\")\n",
        "print(f\"Peak memory: {peak_memory:.2f} MB\")\n",
        "print(f\"\\nText → Image:\")\n",
        "print(f\"  R@1: {text_to_image_results.get('R@1', 0):.4f}\")\n",
        "print(f\"  R@5: {text_to_image_results.get('R@5', 0):.4f}\")\n",
        "print(f\"  R@10: {text_to_image_results.get('R@10', 0):.4f}\")\n",
        "if \"MRR\" in text_to_image_results:\n",
        "    print(f\"  MRR: {text_to_image_results['MRR']:.4f}\")\n",
        "print(f\"\\nImage → Text:\")\n",
        "print(f\"  R@1: {image_to_text_results.get('R@1', 0):.4f}\")\n",
        "print(f\"  R@5: {image_to_text_results.get('R@5', 0):.4f}\")\n",
        "print(f\"  R@10: {image_to_text_results.get('R@10', 0):.4f}\")\n",
        "if \"MRR\" in image_to_text_results:\n",
        "    print(f\"  MRR: {image_to_text_results['MRR']:.4f}\")\n",
        "print(f\"\\n📁 All outputs saved to: s3://{FINE_TUNING_BUCKET}/{RUN_DIR}/\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
