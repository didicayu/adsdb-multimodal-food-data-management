{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline CLIP Evaluation\n",
        "\n",
        "**This notebook (M0 Baseline):**\n",
        "\n",
        "* Loads a pre-trained CLIP model (no fine-tuning)\n",
        "* Computes embeddings for all test images and captions\n",
        "* Evaluates retrieval performance **at recipe_id level** (multiple valid matches per query):\n",
        "\n",
        "  * **Text ‚Üí Image**: one query per recipe using a **representative caption** (longest caption). Ground truth = **all images** with the same `recipe_id`\n",
        "  * **Image ‚Üí Text**: one query per image. Ground truth = **all captions** with the same `recipe_id`\n",
        "* Computes metrics: **R@1, R@5, R@10, MRR, MedianRank (first-hit), MeanRank (first-hit)**\n",
        "* Tracks performance: **throughput** (images/sec, texts/sec) and **peak VRAM**\n",
        "* Saves results for comparison with fine-tuned models\n",
        "\n",
        "**Inputs:**\n",
        "\n",
        "* `fine-tuning-zone/datasets/test_pairs_positive.csv` (evaluation manifest)\n",
        "* `fine-tuning-zone/images/` and `fine-tuning-zone/augmented_images/` (images)\n",
        "\n",
        "**Outputs:**\n",
        "\n",
        "* `fine-tuning-zone/experiments/baseline/results_baseline.json`\n",
        "* `fine-tuning-zone/experiments/baseline/examples_top5.json`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import boto3\n",
        "from botocore.config import Config\n",
        "from botocore.exceptions import ClientError\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Fix random seeds for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    # Optional: Enable deterministic mode for GPU (may slow down slightly)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# CLIP imports from transformers\n",
        "try:\n",
        "    from transformers import CLIPModel, CLIPProcessor\n",
        "    import transformers\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "    transformers = None\n",
        "    print(\"‚ö† transformers not available. Install with: pip install transformers\")\n",
        "\n",
        "# Load environment variables\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent\n",
        "\n",
        "ENV_PATHS = [\n",
        "    PROJECT_ROOT / \"notebooks\" / \".env\",\n",
        "    PROJECT_ROOT / \"app\" / \".env\",\n",
        "    PROJECT_ROOT / \".env\",\n",
        "]\n",
        "\n",
        "env_loaded = False\n",
        "for env_path in ENV_PATHS:\n",
        "    if env_path.exists():\n",
        "        load_dotenv(env_path)\n",
        "        print(f\"‚úì Loaded .env from: {env_path}\")\n",
        "        env_loaded = True\n",
        "        break\n",
        "\n",
        "if not env_loaded:\n",
        "    print(\"‚ö† No .env file found, trying default load_dotenv()...\")\n",
        "    load_dotenv()\n",
        "\n",
        "# MinIO Configuration\n",
        "MINIO_USER = os.getenv(\"MINIO_USER\")\n",
        "MINIO_PASSWORD = os.getenv(\"MINIO_PASSWORD\")\n",
        "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\n",
        "\n",
        "# Bucket configuration\n",
        "FINE_TUNING_BUCKET = \"fine-tuning-zone\"\n",
        "DATASETS_PREFIX = \"datasets\"\n",
        "IMAGES_PREFIX = \"images\"\n",
        "EXPERIMENTS_PREFIX = \"experiments\"\n",
        "\n",
        "# Input/Output paths\n",
        "TEST_MANIFEST_KEY = f\"{DATASETS_PREFIX}/test_pairs_positive.csv\"\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"  # CLIP ViT-B/32 baseline from Hugging Face\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 32  # Batch size for embedding generation\n",
        "\n",
        "# Evaluation configuration\n",
        "K_VALUES = [1, 5, 10]  # R@1, R@5, R@10\n",
        "COMPUTE_MRR = True\n",
        "\n",
        "# Output paths\n",
        "RUN_ID = f\"baseline_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}\"\n",
        "RESULTS_DIR = f\"{EXPERIMENTS_PREFIX}/baseline\"\n",
        "RESULTS_KEY = f\"{RESULTS_DIR}/results_baseline.json\"\n",
        "EXAMPLES_KEY = f\"{RESULTS_DIR}/examples_top5.json\"\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  K values: {K_VALUES}\")\n",
        "print(f\"  Run ID: {RUN_ID}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize MinIO Client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize S3/MinIO client\n",
        "session = boto3.session.Session(\n",
        "    aws_access_key_id=MINIO_USER,\n",
        "    aws_secret_access_key=MINIO_PASSWORD,\n",
        "    region_name=\"us-east-1\"\n",
        ")\n",
        "s3 = session.client(\n",
        "    \"s3\",\n",
        "    endpoint_url=MINIO_ENDPOINT,\n",
        "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"})\n",
        ")\n",
        "\n",
        "def ensure_bucket_exists(bucket: str) -> bool:\n",
        "    \"\"\"Create bucket if it doesn't exist.\"\"\"\n",
        "    try:\n",
        "        s3.head_bucket(Bucket=bucket)\n",
        "        return True\n",
        "    except ClientError as e:\n",
        "        error_code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
        "        if error_code in (\"404\", \"NoSuchBucket\"):\n",
        "            try:\n",
        "                s3.create_bucket(Bucket=bucket)\n",
        "                print(f\"‚úì Created bucket '{bucket}'\")\n",
        "                return True\n",
        "            except ClientError as create_error:\n",
        "                print(f\"‚úó Failed to create bucket '{bucket}': {create_error}\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "# Verify buckets\n",
        "print(\"Checking buckets...\")\n",
        "ensure_bucket_exists(FINE_TUNING_BUCKET)\n",
        "print(\"‚úì Buckets ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Test Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_csv_from_minio(bucket: str, key: str) -> pd.DataFrame:\n",
        "    \"\"\"Load CSV file from MinIO into a DataFrame.\"\"\"\n",
        "    try:\n",
        "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "        df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
        "        print(f\"‚úì Loaded {len(df)} rows from s3://{bucket}/{key}\")\n",
        "        return df\n",
        "    except ClientError as e:\n",
        "        print(f\"‚úó Failed to load s3://{bucket}/{key}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Load test manifest\n",
        "print(\"Loading test manifest...\")\n",
        "test_df = load_csv_from_minio(FINE_TUNING_BUCKET, TEST_MANIFEST_KEY)\n",
        "\n",
        "if test_df.empty:\n",
        "    raise RuntimeError(f\"Could not load test manifest from s3://{FINE_TUNING_BUCKET}/{TEST_MANIFEST_KEY}\")\n",
        "\n",
        "print(f\"\\nTest dataset shape: {test_df.shape}\")\n",
        "print(f\"Unique recipes: {test_df['recipe_id'].nunique()}\")\n",
        "print(f\"Unique images: {test_df['image_key'].nunique()}\")\n",
        "print(f\"Unique captions: {test_df['caption'].nunique()}\")\n",
        "\n",
        "print(f\"\\nTest dataset preview:\")\n",
        "display(test_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load CLIP Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not TRANSFORMERS_AVAILABLE:\n",
        "    raise ImportError(\"transformers library not available. Install with: pip install transformers\")\n",
        "\n",
        "print(f\"Loading CLIP model: {MODEL_NAME}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "\n",
        "# Load processor and model from transformers\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "model = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Get model info\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n‚úì Model loaded successfully\")\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"  Model is frozen (baseline, no fine-tuning)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Helper Functions for MinIO Image Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image_from_minio(bucket: str, key: str) -> Optional[Image.Image]:\n",
        "    \"\"\"Load an image from MinIO.\"\"\"\n",
        "    try:\n",
        "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "        img = Image.open(io.BytesIO(obj[\"Body\"].read()))\n",
        "        img.load()\n",
        "        return img.convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Failed to load image {key}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_image_path_in_minio(image_key: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Determine bucket and full key for an image.\n",
        "    \n",
        "    Images can be in:\n",
        "    - fine-tuning-zone/images/...\n",
        "    - fine-tuning-zone/augmented_images/...\n",
        "    \n",
        "    Tries images/ first, then augmented_images/ if not found.\n",
        "    \"\"\"\n",
        "    bucket = FINE_TUNING_BUCKET\n",
        "    \n",
        "    # Check if it's already a full path\n",
        "    if image_key.startswith(\"images/\") or image_key.startswith(\"augmented_images/\"):\n",
        "        key = image_key\n",
        "    else:\n",
        "        # Try images/ first\n",
        "        key = f\"{IMAGES_PREFIX}/{image_key}\"\n",
        "        # Check if exists, if not try augmented_images/\n",
        "        try:\n",
        "            s3.head_object(Bucket=bucket, Key=key)\n",
        "        except ClientError as e:\n",
        "            if e.response.get(\"Error\", {}).get(\"Code\") == \"404\":\n",
        "                # Try augmented_images/\n",
        "                key = f\"augmented_images/{image_key}\"\n",
        "            else:\n",
        "                raise\n",
        "    \n",
        "    return bucket, key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generate Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_image_embeddings(\n",
        "    df: pd.DataFrame, \n",
        "    model, \n",
        "    processor, \n",
        "    device: str, \n",
        "    batch_size: int = 32\n",
        ") -> Tuple[np.ndarray, List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Generate image embeddings for all images in the dataset.\n",
        "    \n",
        "    Returns:\n",
        "        embeddings: numpy array of shape (n_images, embedding_dim)\n",
        "        image_keys: list of image keys in the same order\n",
        "        failed_keys: list of image keys that failed to load\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    image_keys = []\n",
        "    failed_keys = []\n",
        "    \n",
        "    # Get unique images\n",
        "    unique_images = df[\"image_key\"].unique()\n",
        "    \n",
        "    print(f\"Generating embeddings for {len(unique_images)} unique images...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(unique_images), batch_size)):\n",
        "            batch_keys = unique_images[i:i+batch_size]\n",
        "            batch_images = []\n",
        "            valid_keys = []\n",
        "            \n",
        "            # Load batch of images\n",
        "            for img_key in batch_keys:\n",
        "                bucket, key = get_image_path_in_minio(img_key)\n",
        "                img = load_image_from_minio(bucket, key)\n",
        "                if img is not None:\n",
        "                    batch_images.append(img)\n",
        "                    valid_keys.append(img_key)\n",
        "                else:\n",
        "                    failed_keys.append(img_key)\n",
        "            \n",
        "            if not batch_images:\n",
        "                continue\n",
        "            \n",
        "            # Process images with processor\n",
        "            inputs = processor(images=batch_images, return_tensors=\"pt\", padding=True).to(device)\n",
        "            \n",
        "            # Get image embeddings\n",
        "            outputs = model.get_image_features(**inputs)\n",
        "            batch_emb = F.normalize(outputs, p=2, dim=1)  # L2 normalize\n",
        "            \n",
        "            embeddings.append(batch_emb.cpu().numpy())\n",
        "            image_keys.extend(valid_keys)\n",
        "    \n",
        "    if failed_keys:\n",
        "        print(f\"‚ö† Failed to load {len(failed_keys)} images\")\n",
        "    \n",
        "    embeddings = np.vstack(embeddings) if embeddings else np.array([])\n",
        "    print(f\"‚úì Generated {len(image_keys)} image embeddings (shape: {embeddings.shape})\")\n",
        "    \n",
        "    return embeddings, image_keys, failed_keys\n",
        "\n",
        "def generate_text_embeddings(\n",
        "    texts: List[str], \n",
        "    model, \n",
        "    processor, \n",
        "    device: str, \n",
        "    batch_size: int = 32\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generate text embeddings for all captions.\n",
        "    \n",
        "    Returns:\n",
        "        embeddings: numpy array of shape (n_texts, embedding_dim)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    \n",
        "    print(f\"Generating embeddings for {len(texts)} texts...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(texts), batch_size)):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            \n",
        "            # Process texts with processor\n",
        "            inputs = processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "            \n",
        "            # Get text embeddings\n",
        "            outputs = model.get_text_features(**inputs)\n",
        "            batch_emb = F.normalize(outputs, p=2, dim=1)  # L2 normalize\n",
        "            \n",
        "            embeddings.append(batch_emb.cpu().numpy())\n",
        "    \n",
        "    embeddings = np.vstack(embeddings) if embeddings else np.array([])\n",
        "    print(f\"‚úì Generated {len(texts)} text embeddings (shape: {embeddings.shape})\")\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "# Generate embeddings\n",
        "print(\"=\" * 60)\n",
        "print(\"Generating Embeddings\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Reset CUDA memory stats if using GPU\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Image embeddings (measure time separately)\n",
        "image_start_time = time.time()\n",
        "image_embeddings, image_keys, failed_image_keys = generate_image_embeddings(\n",
        "    test_df, model, processor, DEVICE, BATCH_SIZE\n",
        ")\n",
        "image_time = time.time() - image_start_time\n",
        "\n",
        "# Text embeddings (measure time separately)\n",
        "text_start_time = time.time()\n",
        "unique_captions = test_df[\"caption\"].unique().tolist()\n",
        "text_embeddings = generate_text_embeddings(unique_captions, model, processor, DEVICE, BATCH_SIZE)\n",
        "text_time = time.time() - text_start_time\n",
        "\n",
        "embedding_time = image_time + text_time\n",
        "\n",
        "# Compute throughput and memory metrics (separate for images and texts)\n",
        "n_images = len(image_keys)\n",
        "n_texts = len(unique_captions)\n",
        "images_per_sec = n_images / image_time if image_time > 0 else 0\n",
        "texts_per_sec = n_texts / text_time if text_time > 0 else 0\n",
        "\n",
        "memory_metrics = {}\n",
        "if torch.cuda.is_available():\n",
        "    peak_memory_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)  # MB\n",
        "    memory_metrics = {\n",
        "        \"peak_vram_mb\": peak_memory_mb,\n",
        "        \"peak_vram_gb\": peak_memory_mb / 1024\n",
        "    }\n",
        "\n",
        "print(f\"\\n‚úì Embedding generation complete in {embedding_time:.2f} seconds\")\n",
        "print(f\"  Image embeddings: {image_embeddings.shape}\")\n",
        "print(f\"  Text embeddings: {text_embeddings.shape}\")\n",
        "print(f\"  Throughput: {images_per_sec:.1f} images/sec, {texts_per_sec:.1f} texts/sec\")\n",
        "if memory_metrics:\n",
        "    print(f\"  Peak VRAM: {memory_metrics['peak_vram_mb']:.1f} MB ({memory_metrics['peak_vram_gb']:.2f} GB)\")\n",
        "\n",
        "# Create mapping dictionaries for lookup\n",
        "image_key_to_idx = {key: idx for idx, key in enumerate(image_keys)}\n",
        "caption_to_idx = {caption: idx for idx, caption in enumerate(unique_captions)}\n",
        "\n",
        "# Precompute lookup dictionaries for O(1) access (optimization)\n",
        "print(\"\\nPrecomputing lookup dictionaries...\")\n",
        "\n",
        "# recipe_id -> list of image indices\n",
        "# Convert recipe_id to string for consistency (pandas can return int64)\n",
        "recipe_to_image_indices: Dict[str, List[int]] = {}\n",
        "for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "    recipe_id_str = str(recipe_id)\n",
        "    recipe_images = test_df[test_df[\"recipe_id\"] == recipe_id][\"image_key\"].unique()\n",
        "    recipe_to_image_indices[recipe_id_str] = [\n",
        "        image_key_to_idx.get(img_key) \n",
        "        for img_key in recipe_images \n",
        "        if image_key_to_idx.get(img_key) is not None\n",
        "    ]\n",
        "\n",
        "# recipe_id -> list of caption indices (one caption per recipe for text‚Üíimage query)\n",
        "# Use longest caption as representative (most informative)\n",
        "recipe_to_caption_indices: Dict[str, List[int]] = {}\n",
        "recipe_to_caption_text: Dict[str, str] = {}\n",
        "# recipe_id -> list of ALL caption indices (for image‚Üítext ground truth)\n",
        "recipe_to_all_caption_indices: Dict[str, List[int]] = {}\n",
        "for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "    recipe_id_str = str(recipe_id)\n",
        "    recipe_captions = test_df[test_df[\"recipe_id\"] == recipe_id][\"caption\"].unique()\n",
        "    if len(recipe_captions) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Use longest caption as representative (most informative)\n",
        "    # If multiple have same length, use first after sorting alphabetically (deterministic)\n",
        "    representative_caption = max(recipe_captions, key=lambda c: (len(c), c))\n",
        "    \n",
        "    # Store representative caption for text‚Üíimage queries\n",
        "    cap_idx = caption_to_idx.get(representative_caption)\n",
        "    if cap_idx is not None:\n",
        "        recipe_to_caption_indices[recipe_id_str] = [cap_idx]\n",
        "        recipe_to_caption_text[recipe_id_str] = representative_caption\n",
        "    \n",
        "    # Store ALL captions for image‚Üítext ground truth (more fair if multiple captions per recipe)\n",
        "    all_caption_indices = []\n",
        "    for caption in recipe_captions:\n",
        "        cap_idx = caption_to_idx.get(caption)\n",
        "        if cap_idx is not None:\n",
        "            all_caption_indices.append(cap_idx)\n",
        "    recipe_to_all_caption_indices[recipe_id_str] = list(set(all_caption_indices))  # Remove duplicates\n",
        "\n",
        "# image_key -> recipe_id (as string for consistency)\n",
        "image_key_to_recipe_id: Dict[str, str] = {}\n",
        "for _, row in test_df.iterrows():\n",
        "    if row[\"image_key\"] not in image_key_to_recipe_id:\n",
        "        image_key_to_recipe_id[row[\"image_key\"]] = str(row[\"recipe_id\"])\n",
        "\n",
        "print(f\"‚úì Precomputed lookups for {len(recipe_to_image_indices)} recipes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_recall_at_k(scores: np.ndarray, ground_truth_indices: List[int], k: int) -> float:\n",
        "    \"\"\"\n",
        "    Compute Recall@K.\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "        k: top-K to consider\n",
        "    \n",
        "    Returns:\n",
        "        recall@k: 1.0 if any ground truth is in top-K, else 0.0\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    top_k_indices = np.argsort(scores)[-k:][::-1]  # Top K, highest first\n",
        "    top_k_set = set(top_k_indices)\n",
        "    \n",
        "    # Check if any ground truth is in top-K\n",
        "    for gt_idx in ground_truth_indices:\n",
        "        if gt_idx in top_k_set:\n",
        "            return 1.0\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "def compute_mrr(scores: np.ndarray, ground_truth_indices: List[int]) -> float:\n",
        "    \"\"\"\n",
        "    Compute Mean Reciprocal Rank.\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "    \n",
        "    Returns:\n",
        "        mrr: 1/rank of first correct result, or 0.0 if none found\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # Sort by score (descending)\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    \n",
        "    # Find rank of first ground truth\n",
        "    for rank, idx in enumerate(sorted_indices, start=1):\n",
        "        if idx in ground_truth_indices:\n",
        "            return 1.0 / rank\n",
        "    \n",
        "    return 0.0\n",
        "\n",
        "def compute_first_hit_rank(scores: np.ndarray, ground_truth_indices: List[int]) -> float:\n",
        "    \"\"\"\n",
        "    Compute rank of first correct result (first hit rank).\n",
        "    \n",
        "    Args:\n",
        "        scores: similarity scores for all candidates (1D array)\n",
        "        ground_truth_indices: list of correct candidate indices\n",
        "    \n",
        "    Returns:\n",
        "        first_hit_rank: rank of first correct result (1-indexed), or len(scores)+1 if not found\n",
        "    \"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return float(len(scores) + 1)\n",
        "    \n",
        "    # Sort by score (descending)\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    \n",
        "    # Find rank of first ground truth\n",
        "    for rank, idx in enumerate(sorted_indices, start=1):\n",
        "        if idx in ground_truth_indices:\n",
        "            return float(rank)\n",
        "    \n",
        "    return float(len(scores) + 1)  # Not found\n",
        "\n",
        "def evaluate_text_to_image_by_recipe(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    k_values: List[int],\n",
        "    should_compute_mrr: bool = True,\n",
        "    should_compute_median_rank: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate Text ‚Üí Image retrieval by recipe_id.\n",
        "    \n",
        "    For each recipe_id, use its representative caption to retrieve images.\n",
        "    Ground truth: all images from that recipe_id.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating Text ‚Üí Image retrieval (by recipe_id)...\")\n",
        "    \n",
        "    unique_recipe_ids = test_df[\"recipe_id\"].unique()\n",
        "    \n",
        "    recalls = {f\"R@{k}\": [] for k in k_values}\n",
        "    mrr_scores = [] if should_compute_mrr else None\n",
        "    median_ranks = [] if should_compute_median_rank else None\n",
        "    \n",
        "    for recipe_id in tqdm(unique_recipe_ids, desc=\"Processing recipes\"):\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption embedding (first caption for this recipe)\n",
        "        caption_indices = recipe_to_caption_indices.get(recipe_id_str, [])\n",
        "        if not caption_indices:\n",
        "            continue\n",
        "        \n",
        "        # Use first caption (or could average multiple)\n",
        "        text_idx = caption_indices[0]\n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        \n",
        "        # Compute similarities with all images\n",
        "        similarities = image_embeddings @ text_emb  # (n_images,)\n",
        "        \n",
        "        # Get ground truth: all images from this recipe_id\n",
        "        ground_truth_indices = recipe_to_image_indices.get(recipe_id_str, [])\n",
        "        ground_truth_indices = [idx for idx in ground_truth_indices if idx is not None]\n",
        "        \n",
        "        if len(ground_truth_indices) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Compute metrics\n",
        "        for k in k_values:\n",
        "            recall = compute_recall_at_k(similarities, ground_truth_indices, k)\n",
        "            recalls[f\"R@{k}\"].append(recall)\n",
        "        \n",
        "        if should_compute_mrr:\n",
        "            mrr = compute_mrr(similarities, ground_truth_indices)\n",
        "            mrr_scores.append(mrr)\n",
        "        \n",
        "        if should_compute_median_rank:\n",
        "            first_hit_rank = compute_first_hit_rank(similarities, ground_truth_indices)\n",
        "            median_ranks.append(first_hit_rank)\n",
        "    \n",
        "    # Average metrics\n",
        "    results = {}\n",
        "    for k in k_values:\n",
        "        results[f\"R@{k}\"] = float(np.mean(recalls[f\"R@{k}\"])) if recalls[f\"R@{k}\"] else 0.0\n",
        "    \n",
        "    if should_compute_mrr and mrr_scores:\n",
        "        results[\"MRR\"] = float(np.mean(mrr_scores))\n",
        "    \n",
        "    if should_compute_median_rank and median_ranks:\n",
        "        results[\"MedianRank_first_hit\"] = float(np.median(median_ranks))\n",
        "        results[\"MeanRank_first_hit\"] = float(np.mean(median_ranks))\n",
        "    \n",
        "    return results\n",
        "\n",
        "def evaluate_image_to_text_by_recipe(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    k_values: List[int],\n",
        "    should_compute_mrr: bool = True,\n",
        "    should_compute_median_rank: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Evaluate Image ‚Üí Text retrieval by recipe_id.\n",
        "    \n",
        "    For each image, retrieve captions. Ground truth: ALL captions from the same recipe_id.\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating Image ‚Üí Text retrieval (by recipe_id)...\")\n",
        "    \n",
        "    unique_images = test_df[\"image_key\"].unique()\n",
        "    \n",
        "    recalls = {f\"R@{k}\": [] for k in k_values}\n",
        "    mrr_scores = [] if should_compute_mrr else None\n",
        "    median_ranks = [] if should_compute_median_rank else None\n",
        "    \n",
        "    for img_key in tqdm(unique_images, desc=\"Processing images\"):\n",
        "        # Get image embedding\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        \n",
        "        # Compute similarities with all captions\n",
        "        similarities = text_embeddings @ img_emb  # (n_texts,)\n",
        "        \n",
        "        # Get recipe_id for this image\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        \n",
        "        # Get ground truth: ALL captions from the same recipe_id (more fair)\n",
        "        ground_truth_indices = recipe_to_all_caption_indices.get(recipe_id_str, [])\n",
        "        ground_truth_indices = [idx for idx in ground_truth_indices if idx is not None]\n",
        "        \n",
        "        if len(ground_truth_indices) == 0:\n",
        "            continue\n",
        "        \n",
        "        # Compute metrics\n",
        "        for k in k_values:\n",
        "            recall = compute_recall_at_k(similarities, ground_truth_indices, k)\n",
        "            recalls[f\"R@{k}\"].append(recall)\n",
        "        \n",
        "        if should_compute_mrr:\n",
        "            mrr = compute_mrr(similarities, ground_truth_indices)\n",
        "            mrr_scores.append(mrr)\n",
        "        \n",
        "        if should_compute_median_rank:\n",
        "            first_hit_rank = compute_first_hit_rank(similarities, ground_truth_indices)\n",
        "            median_ranks.append(first_hit_rank)\n",
        "    \n",
        "    # Average metrics\n",
        "    results = {}\n",
        "    for k in k_values:\n",
        "        results[f\"R@{k}\"] = float(np.mean(recalls[f\"R@{k}\"])) if recalls[f\"R@{k}\"] else 0.0\n",
        "    \n",
        "    if should_compute_mrr and mrr_scores:\n",
        "        results[\"MRR\"] = float(np.mean(mrr_scores))\n",
        "    \n",
        "    if should_compute_median_rank and median_ranks:\n",
        "        results[\"MedianRank_first_hit\"] = float(np.median(median_ranks))\n",
        "        results[\"MeanRank_first_hit\"] = float(np.mean(median_ranks))\n",
        "    \n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"Running Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "eval_start_time = time.time()\n",
        "\n",
        "# Text ‚Üí Image retrieval (by recipe_id)\n",
        "text_to_image_results = evaluate_text_to_image_by_recipe(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_indices,\n",
        "    recipe_to_image_indices,\n",
        "    image_key_to_idx,\n",
        "    K_VALUES,\n",
        "    COMPUTE_MRR,\n",
        "    should_compute_median_rank=True\n",
        ")\n",
        "\n",
        "# Image ‚Üí Text retrieval (by recipe_id)\n",
        "image_to_text_results = evaluate_image_to_text_by_recipe(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    K_VALUES,\n",
        "    COMPUTE_MRR,\n",
        "    should_compute_median_rank=True\n",
        ")\n",
        "\n",
        "eval_time = time.time() - eval_start_time\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Evaluation Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Mapping for display names (only for specific metrics)\n",
        "METRIC_DISPLAY_NAMES = {\n",
        "    \"MedianRank_first_hit\": \"MedianRank\",\n",
        "    \"MeanRank_first_hit\": \"MeanRank\"\n",
        "}\n",
        "\n",
        "print(\"\\nüìä Text ‚Üí Image Retrieval:\")\n",
        "for metric, value in text_to_image_results.items():\n",
        "    # Display with shorter names for readability (only for specific metrics)\n",
        "    display_name = METRIC_DISPLAY_NAMES.get(metric, metric)\n",
        "    # Format ranks with 2 decimals, other metrics with 4 decimals\n",
        "    if metric in (\"MedianRank_first_hit\", \"MeanRank_first_hit\"):\n",
        "        print(f\"  {display_name}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {display_name}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nüìä Image ‚Üí Text Retrieval:\")\n",
        "for metric, value in image_to_text_results.items():\n",
        "    # Display with shorter names for readability (only for specific metrics)\n",
        "    display_name = METRIC_DISPLAY_NAMES.get(metric, metric)\n",
        "    # Format ranks with 2 decimals, other metrics with 4 decimals\n",
        "    if metric in (\"MedianRank_first_hit\", \"MeanRank_first_hit\"):\n",
        "        print(f\"  {display_name}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {display_name}: {value:.4f}\")\n",
        "\n",
        "print(f\"\\n‚è± Evaluation time: {eval_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualize Retrieval Results\n",
        "\n",
        "Visualize qualitative examples showing top-5 retrievals for both directions:\n",
        "- **Text ‚Üí Image**: Show the 5 most similar images for sample captions\n",
        "- **Image ‚Üí Text**: Show the 5 most similar captions for sample images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "\n",
        "def visualize_text_to_image_retrieval(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    caption_to_idx: Dict[str, int],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    n_examples: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize Text ‚Üí Image retrieval: for each recipe, show top-5 images.\n",
        "    \"\"\"\n",
        "    # Sample random recipes\n",
        "    unique_recipe_ids = test_df[\"recipe_id\"].unique()\n",
        "    sample_recipe_ids = pd.Series(unique_recipe_ids).sample(\n",
        "        min(n_examples, len(unique_recipe_ids)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for recipe_idx, recipe_id in enumerate(sample_recipe_ids):\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption for this recipe\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        \n",
        "        # Get caption embedding\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        \n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        similarities = image_embeddings @ text_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]  # Top 5, highest first\n",
        "        \n",
        "        # Get ground truth: images from this recipe_id\n",
        "        gt_image_indices = set(recipe_to_image_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Create figure with more vertical space for captions\n",
        "        fig = plt.figure(figsize=(16, 5))\n",
        "        gs = gridspec.GridSpec(1, 6, width_ratios=[2, 1, 1, 1, 1, 1], hspace=0.3, bottom=0.15)\n",
        "        \n",
        "        # Show query caption\n",
        "        ax0 = fig.add_subplot(gs[0])\n",
        "        ax0.text(0.5, 0.5, f'Query:\\n\"{caption}\"', \n",
        "                ha='center', va='center', fontsize=14, fontweight='bold',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
        "        ax0.axis('off')\n",
        "        \n",
        "        # Show top-5 images\n",
        "        for i, img_idx in enumerate(top5_indices):\n",
        "            img_key = image_keys[img_idx]\n",
        "            bucket, key = get_image_path_in_minio(img_key)\n",
        "            img = load_image_from_minio(bucket, key)\n",
        "            \n",
        "            if img is None:\n",
        "                continue\n",
        "            \n",
        "            # Check if this image is from the ground truth recipe\n",
        "            is_correct = img_idx in gt_image_indices\n",
        "            \n",
        "            # Get recipe_id for this image to show its representative caption\n",
        "            img_recipe_id = image_key_to_recipe_id.get(img_key, \"Unknown\")\n",
        "            # Note: Shows representative caption of the recipe (longest caption), not necessarily the exact caption for this image\n",
        "            img_caption = recipe_to_caption_text.get(img_recipe_id, \"Unknown\")\n",
        "            \n",
        "            ax = fig.add_subplot(gs[i + 1])\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "            \n",
        "            # Color border: green if correct, red if incorrect\n",
        "            border_color = 'green' if is_correct else 'red'\n",
        "            border_width = 3 if is_correct else 2\n",
        "            for spine in ax.spines.values():\n",
        "                spine.set_edgecolor(border_color)\n",
        "                spine.set_linewidth(border_width)\n",
        "            \n",
        "            # Title with similarity score\n",
        "            similarity_score = similarities[img_idx]\n",
        "            rank = i + 1\n",
        "            title = f\"Rank {rank}\\n{similarity_score:.3f}\"\n",
        "            if is_correct:\n",
        "                title += \"\\n‚úì Correct\"\n",
        "            ax.set_title(title, fontsize=10, color=border_color, fontweight='bold')\n",
        "            \n",
        "            # Add recipe caption below the image\n",
        "            # Truncate long captions for display\n",
        "            display_caption = img_caption if len(img_caption) <= 40 else img_caption[:37] + \"...\"\n",
        "            ax.text(0.5, -0.15, f'\"{display_caption}\"', \n",
        "                   ha='center', va='top', fontsize=9, \n",
        "                   transform=ax.transAxes,\n",
        "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'),\n",
        "                   wrap=True)\n",
        "        \n",
        "        plt.suptitle(f'Text ‚Üí Image Retrieval Example {recipe_idx + 1} (Recipe: {recipe_id_str})', \n",
        "                     fontsize=16, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print()\n",
        "\n",
        "def visualize_image_to_text_retrieval(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    n_examples: int = 3\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize Image ‚Üí Text retrieval: for each image, show top-5 captions.\n",
        "    \"\"\"\n",
        "    # Sample random images\n",
        "    unique_images = test_df[\"image_key\"].unique()\n",
        "    sample_images = pd.Series(unique_images).sample(\n",
        "        min(n_examples, len(unique_images)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for img_idx_example, img_key in enumerate(sample_images):\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        similarities = text_embeddings @ img_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]  # Top 5, highest first\n",
        "        \n",
        "        # Get ground truth: recipe_id for this image\n",
        "        gt_recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        # Use all captions for ground truth (consistent with evaluation)\n",
        "        gt_caption_indices = set(recipe_to_all_caption_indices.get(gt_recipe_id_str, [])) if gt_recipe_id_str else set()\n",
        "        \n",
        "        # Load query image\n",
        "        bucket, key = get_image_path_in_minio(img_key)\n",
        "        query_img = load_image_from_minio(bucket, key)\n",
        "        \n",
        "        if query_img is None:\n",
        "            continue\n",
        "        \n",
        "        # Create figure with more vertical space for recipe name\n",
        "        fig = plt.figure(figsize=(16, 4))\n",
        "        gs = gridspec.GridSpec(1, 6, width_ratios=[1, 1, 1, 1, 1, 1], hspace=0.3, bottom=0.15)\n",
        "        \n",
        "        # Show query image\n",
        "        ax0 = fig.add_subplot(gs[0])\n",
        "        ax0.imshow(query_img)\n",
        "        ax0.axis('off')\n",
        "        ax0.set_title('Query Image', fontsize=12, fontweight='bold')\n",
        "        \n",
        "        # Add recipe name below the query image\n",
        "        if gt_recipe_id_str:\n",
        "            recipe_caption = recipe_to_caption_text.get(gt_recipe_id_str, \"Unknown\")\n",
        "            # Truncate long captions for display\n",
        "            display_recipe = recipe_caption if len(recipe_caption) <= 40 else recipe_caption[:37] + \"...\"\n",
        "            ax0.text(0.5, -0.15, f'Recipe: {gt_recipe_id_str}\\n\"{display_recipe}\"', \n",
        "                   ha='center', va='top', fontsize=9, \n",
        "                   transform=ax0.transAxes,\n",
        "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'),\n",
        "                   wrap=True)\n",
        "        \n",
        "        # Show top-5 captions\n",
        "        for i, text_idx in enumerate(top5_indices):\n",
        "            caption = unique_captions[text_idx]\n",
        "            \n",
        "            # Check if this caption is from the ground truth recipe\n",
        "            is_correct = text_idx in gt_caption_indices\n",
        "            \n",
        "            ax = fig.add_subplot(gs[i + 1])\n",
        "            ax.axis('off')\n",
        "            \n",
        "            # Color border: green if correct, red if incorrect\n",
        "            border_color = 'green' if is_correct else 'red'\n",
        "            border_width = 3 if is_correct else 2\n",
        "            \n",
        "            # Create text box\n",
        "            similarity_score = similarities[text_idx]\n",
        "            rank = i + 1\n",
        "            text_content = f\"Rank {rank}\\n\\n\\\"{caption}\\\"\\n\\nSimilarity: {similarity_score:.3f}\"\n",
        "            if is_correct:\n",
        "                text_content += \"\\n\\n‚úì Correct\"\n",
        "            \n",
        "            ax.text(0.5, 0.5, text_content,\n",
        "                   ha='center', va='center', fontsize=10,\n",
        "                   bbox=dict(boxstyle='round', facecolor='lightyellow', \n",
        "                            edgecolor=border_color, linewidth=border_width, alpha=0.8),\n",
        "                   wrap=True)\n",
        "        \n",
        "        plt.suptitle(f'Image ‚Üí Text Retrieval Example {img_idx_example + 1} (Recipe: {gt_recipe_id_str})', \n",
        "                     fontsize=16, fontweight='bold', y=1.02)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print()\n",
        "\n",
        "# Visualize results\n",
        "print(\"=\" * 60)\n",
        "print(\"Visualizing Retrieval Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìù Text ‚Üí Image Retrieval (Top-5 images for each recipe):\")\n",
        "print(\"-\" * 60)\n",
        "visualize_text_to_image_retrieval(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_text,\n",
        "    recipe_to_image_indices,\n",
        "    caption_to_idx,\n",
        "    image_key_to_idx,\n",
        "    n_examples=3\n",
        ")\n",
        "\n",
        "print(\"\\nüñºÔ∏è Image ‚Üí Text Retrieval (Top-5 captions for each image):\")\n",
        "print(\"-\" * 60)\n",
        "visualize_image_to_text_retrieval(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    recipe_to_caption_text,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    n_examples=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Generate Qualitative Examples (Top-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_top5_examples(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    caption_to_idx: Dict[str, int],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    n_examples: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate qualitative examples showing top-5 retrievals by recipe_id.\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    \n",
        "    # Filter recipes with valid ground truth before sampling (avoid bias)\n",
        "    valid_recipe_ids = []\n",
        "    for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        gt_image_indices = recipe_to_image_indices.get(recipe_id_str, [])\n",
        "        if len(gt_image_indices) > 0:  # Only include if has valid GT\n",
        "            valid_recipe_ids.append(recipe_id)\n",
        "    \n",
        "    # Sample random recipes from valid ones\n",
        "    if len(valid_recipe_ids) == 0:\n",
        "        print(\"‚ö† No valid recipes with ground truth images found\")\n",
        "        return examples\n",
        "    \n",
        "    sample_recipe_ids = pd.Series(valid_recipe_ids).sample(\n",
        "        min(n_examples, len(valid_recipe_ids)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for recipe_id in sample_recipe_ids:\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        # Get representative caption for this recipe\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        \n",
        "        # Get caption embedding\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        \n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        similarities = image_embeddings @ text_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        \n",
        "        # Get ground truth: images from this recipe_id\n",
        "        gt_image_indices = set(recipe_to_image_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Get top-5 retrieved images\n",
        "        top5_images = []\n",
        "        for i, idx in enumerate(top5_indices):\n",
        "            img_key = image_keys[idx]\n",
        "            img_recipe_id = image_key_to_recipe_id.get(img_key)\n",
        "            is_correct = idx in gt_image_indices\n",
        "            \n",
        "            top5_images.append({\n",
        "                \"rank\": i + 1,\n",
        "                \"image_key\": img_key,\n",
        "                \"recipe_id\": img_recipe_id,\n",
        "                \"similarity\": float(similarities[idx]),\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "        \n",
        "        examples.append({\n",
        "            \"query_type\": \"text_to_image\",\n",
        "            \"recipe_id\": recipe_id_str,\n",
        "            \"query_caption\": caption,\n",
        "            \"ground_truth_recipe_id\": recipe_id_str,\n",
        "            \"top5_results\": top5_images\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "def generate_image_to_text_examples(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    unique_captions: List[str],\n",
        "    n_examples: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Generate qualitative examples showing top-5 captions for images (Image‚ÜíText retrieval).\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    \n",
        "    # Filter images with valid ground truth before sampling (avoid bias)\n",
        "    valid_images = []\n",
        "    for img_key in test_df[\"image_key\"].unique():\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        gt_caption_indices = recipe_to_all_caption_indices.get(recipe_id_str, [])\n",
        "        if len(gt_caption_indices) > 0:  # Only include if has valid GT\n",
        "            valid_images.append(img_key)\n",
        "    \n",
        "    # Sample random images from valid ones\n",
        "    if len(valid_images) == 0:\n",
        "        print(\"‚ö† No valid images with ground truth captions found\")\n",
        "        return examples\n",
        "    \n",
        "    sample_images = pd.Series(valid_images).sample(\n",
        "        min(n_examples, len(valid_images)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for img_key in sample_images:\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        \n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        similarities = text_embeddings @ img_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        \n",
        "        # Get ground truth: recipe_id for this image\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        \n",
        "        # Get ground truth: all captions from this recipe_id\n",
        "        gt_caption_indices = set(recipe_to_all_caption_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        # Get top-5 retrieved captions\n",
        "        top5_captions = []\n",
        "        for i, idx in enumerate(top5_indices):\n",
        "            caption = unique_captions[idx]\n",
        "            is_correct = idx in gt_caption_indices\n",
        "            \n",
        "            top5_captions.append({\n",
        "                \"rank\": i + 1,\n",
        "                \"caption\": caption,\n",
        "                \"caption_index\": int(idx),\n",
        "                \"similarity\": float(similarities[idx]),\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "        \n",
        "        examples.append({\n",
        "            \"query_type\": \"image_to_text\",\n",
        "            \"recipe_id\": recipe_id_str,\n",
        "            \"query_image_key\": img_key,\n",
        "            \"recipe_representative_caption\": recipe_to_caption_text.get(recipe_id_str),\n",
        "            \"ground_truth_recipe_id\": recipe_id_str,\n",
        "            \"top5_results\": top5_captions\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "# Generate examples\n",
        "N_QUALITATIVE_EXAMPLES = 5\n",
        "print(\"Generating qualitative examples...\")\n",
        "\n",
        "# Text ‚Üí Image examples\n",
        "text_to_image_examples = generate_top5_examples(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_caption_text,\n",
        "    recipe_to_image_indices,\n",
        "    caption_to_idx,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    n_examples=N_QUALITATIVE_EXAMPLES\n",
        ")\n",
        "\n",
        "# Image ‚Üí Text examples\n",
        "image_to_text_examples = generate_image_to_text_examples(\n",
        "    test_df,\n",
        "    text_embeddings,\n",
        "    image_embeddings,\n",
        "    recipe_to_all_caption_indices,\n",
        "    recipe_to_caption_text,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    unique_captions=unique_captions,\n",
        "    n_examples=N_QUALITATIVE_EXAMPLES\n",
        ")\n",
        "\n",
        "print(f\"‚úì Generated {len(text_to_image_examples)} text‚Üíimage examples (requested: {N_QUALITATIVE_EXAMPLES})\")\n",
        "print(f\"‚úì Generated {len(image_to_text_examples)} image‚Üítext examples (requested: {N_QUALITATIVE_EXAMPLES})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Results to MinIO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_json_to_minio(data: Dict, bucket: str, key: str) -> bool:\n",
        "    \"\"\"Save dictionary as JSON to MinIO.\"\"\"\n",
        "    try:\n",
        "        json_bytes = json.dumps(data, indent=2).encode(\"utf-8\")\n",
        "        \n",
        "        s3.put_object(\n",
        "            Bucket=bucket,\n",
        "            Key=key,\n",
        "            Body=json_bytes,\n",
        "            ContentType=\"application/json\",\n",
        "        )\n",
        "        \n",
        "        size_kb = len(json_bytes) / 1024\n",
        "        print(f\"‚úì Saved to s3://{bucket}/{key} ({size_kb:.1f} KB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Failed to save: {e}\")\n",
        "        return False\n",
        "\n",
        "# Create results dictionary\n",
        "results = {\n",
        "    \"metadata\": {\n",
        "        \"run_id\": RUN_ID,\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"method\": \"baseline\",\n",
        "        \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),\n",
        "        \"device\": DEVICE,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"random_seed\": RANDOM_SEED,\n",
        "        \"test_manifest\": TEST_MANIFEST_KEY,\n",
        "        \"evaluation_unit\": \"recipe_id\",  # NEW: document evaluation approach\n",
        "        \"evaluation_protocol\": {\n",
        "            \"text_to_image\": \"query=representative_caption(recipe_id), gt=all_images(recipe_id)\",\n",
        "            \"image_to_text\": \"query=image, gt=all_captions(recipe_id)\",\n",
        "            \"representative_caption_rule\": \"max(len(caption), caption) - longest caption per recipe (deterministic tie-break)\",\n",
        "            \"rank_definition\": \"rank of first correct match in sorted list (1-indexed)\"\n",
        "        },\n",
        "        \"failed_images\": {\n",
        "            \"n_failed\": len(failed_image_keys),\n",
        "            \"failed_image_keys_sample\": failed_image_keys[:10] if len(failed_image_keys) > 0 else []\n",
        "        },\n",
        "        \"qualitative_examples\": {\n",
        "            \"text_to_image\": {\n",
        "                \"n_examples\": len(text_to_image_examples),\n",
        "                \"n_requested\": N_QUALITATIVE_EXAMPLES\n",
        "            },\n",
        "            \"image_to_text\": {\n",
        "                \"n_examples\": len(image_to_text_examples),\n",
        "                \"n_requested\": N_QUALITATIVE_EXAMPLES\n",
        "            }\n",
        "        },\n",
        "        \"versions\": {\n",
        "            \"python\": sys.version,\n",
        "            \"torch\": torch.__version__,\n",
        "            \"transformers\": transformers.__version__ if transformers is not None else \"N/A\",\n",
        "            \"numpy\": np.__version__,\n",
        "        },\n",
        "    },\n",
        "    \"model_info\": {\n",
        "        \"total_parameters\": total_params,\n",
        "        \"trainable_parameters\": trainable_params,\n",
        "        \"frozen\": True\n",
        "    },\n",
        "    \"dataset_info\": {\n",
        "        \"test_pairs\": len(test_df),\n",
        "        \"test_recipes\": test_df[\"recipe_id\"].nunique(),\n",
        "        \"test_images\": test_df[\"image_key\"].nunique(),\n",
        "        \"embedded_images\": len(image_keys),\n",
        "        \"embedded_images_ratio\": float(len(image_keys) / test_df[\"image_key\"].nunique()) if test_df[\"image_key\"].nunique() > 0 else 0.0,\n",
        "        \"test_captions\": test_df[\"caption\"].nunique(),\n",
        "        \"avg_captions_per_recipe\": float(test_df.groupby(\"recipe_id\")[\"caption\"].nunique().mean()),\n",
        "        \"max_captions_per_recipe\": int(test_df.groupby(\"recipe_id\")[\"caption\"].nunique().max()),\n",
        "    },\n",
        "    \"metrics\": {\n",
        "        \"text_to_image\": text_to_image_results,\n",
        "        \"image_to_text\": image_to_text_results,\n",
        "    },\n",
        "    \"timing\": {\n",
        "        \"embedding_generation_seconds\": embedding_time,\n",
        "        \"image_embedding_seconds\": image_time,\n",
        "        \"text_embedding_seconds\": text_time,\n",
        "        \"evaluation_seconds\": eval_time,\n",
        "        \"total_seconds\": embedding_time + eval_time,\n",
        "    },\n",
        "    \"throughput\": {\n",
        "        \"images_per_second\": images_per_sec,\n",
        "        \"texts_per_second\": texts_per_sec,\n",
        "    },\n",
        "    \"memory\": memory_metrics,\n",
        "}\n",
        "\n",
        "# Save results\n",
        "print(\"=\" * 60)\n",
        "print(\"Saving Results to MinIO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "save_json_to_minio(results, FINE_TUNING_BUCKET, RESULTS_KEY)\n",
        "save_json_to_minio({\n",
        "    \"text_to_image_examples\": text_to_image_examples,\n",
        "    \"image_to_text_examples\": image_to_text_examples\n",
        "}, FINE_TUNING_BUCKET, EXAMPLES_KEY)\n",
        "\n",
        "print(f\"\\n‚úÖ Results saved successfully!\")\n",
        "print(f\"  Results: s3://{FINE_TUNING_BUCKET}/{RESULTS_KEY}\")\n",
        "print(f\"  Examples: s3://{FINE_TUNING_BUCKET}/{EXAMPLES_KEY}\")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Summary\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Run ID: {RUN_ID}\")\n",
        "print(f\"Model: {MODEL_NAME} (baseline, no fine-tuning)\")\n",
        "print(f\"\\nText ‚Üí Image:\")\n",
        "print(f\"  R@1: {text_to_image_results.get('R@1', 0):.4f}\")\n",
        "print(f\"  R@5: {text_to_image_results.get('R@5', 0):.4f}\")\n",
        "print(f\"  R@10: {text_to_image_results.get('R@10', 0):.4f}\")\n",
        "if \"MRR\" in text_to_image_results:\n",
        "    print(f\"  MRR: {text_to_image_results['MRR']:.4f}\")\n",
        "if \"MedianRank_first_hit\" in text_to_image_results:\n",
        "    print(f\"  MedianRank: {text_to_image_results['MedianRank_first_hit']:.2f}\")\n",
        "    print(f\"  MeanRank: {text_to_image_results['MeanRank_first_hit']:.2f}\")\n",
        "print(f\"\\nImage ‚Üí Text:\")\n",
        "print(f\"  R@1: {image_to_text_results.get('R@1', 0):.4f}\")\n",
        "print(f\"  R@5: {image_to_text_results.get('R@5', 0):.4f}\")\n",
        "print(f\"  R@10: {image_to_text_results.get('R@10', 0):.4f}\")\n",
        "if \"MRR\" in image_to_text_results:\n",
        "    print(f\"  MRR: {image_to_text_results['MRR']:.4f}\")\n",
        "if \"MedianRank_first_hit\" in image_to_text_results:\n",
        "    print(f\"  MedianRank: {image_to_text_results['MedianRank_first_hit']:.2f}\")\n",
        "    print(f\"  MeanRank: {image_to_text_results['MeanRank_first_hit']:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
