{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Analysis and Train/Test Split\n",
        "\n",
        "This notebook analyzes the augmented dataset and creates a reproducible train/test split for CLIP fine-tuning experiments.\n",
        "\n",
        "**Objectives:**\n",
        "1. Load the augmented dataset from MinIO (`train_pairs_augmented_with_negatives.csv`)\n",
        "2. Filter only positive pairs (label=1) for training/evaluation\n",
        "3. Create train/test split by `recipe_id` (no data leakage)\n",
        "4. Generate dataset statistics and verification report\n",
        "5. Save split manifests to MinIO for reproducibility\n",
        "\n",
        "**Outputs:**\n",
        "- `fine-tuning-zone/datasets/train_manifest.csv` â€” Training pairs\n",
        "- `fine-tuning-zone/datasets/test_manifest.csv` â€” Test pairs\n",
        "- `fine-tuning-zone/datasets/dataset_report.json` â€” Statistics and metadata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import json\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Set, Tuple\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import boto3\n",
        "from botocore.config import Config\n",
        "from botocore.exceptions import ClientError\n",
        "from dotenv import load_dotenv\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load environment variables\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent\n",
        "\n",
        "ENV_PATHS = [\n",
        "    PROJECT_ROOT / \"notebooks\" / \".env\",\n",
        "    PROJECT_ROOT / \"app\" / \".env\",\n",
        "    PROJECT_ROOT / \".env\",\n",
        "]\n",
        "\n",
        "env_loaded = False\n",
        "for env_path in ENV_PATHS:\n",
        "    if env_path.exists():\n",
        "        load_dotenv(env_path)\n",
        "        print(f\"âœ“ Loaded .env from: {env_path}\")\n",
        "        env_loaded = True\n",
        "        break\n",
        "\n",
        "if not env_loaded:\n",
        "    print(\"âš  No .env file found, trying default load_dotenv()...\")\n",
        "    load_dotenv()\n",
        "\n",
        "# MinIO Configuration\n",
        "MINIO_USER = os.getenv(\"MINIO_USER\")\n",
        "MINIO_PASSWORD = os.getenv(\"MINIO_PASSWORD\")\n",
        "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\n",
        "\n",
        "# Bucket configuration\n",
        "FINE_TUNING_BUCKET = \"fine-tuning-zone\"\n",
        "DATASETS_PREFIX = \"datasets\"\n",
        "\n",
        "# Input/Output paths\n",
        "INPUT_DATASET_KEY = f\"{DATASETS_PREFIX}/train_pairs_augmented_with_negatives.csv\"\n",
        "TRAIN_MANIFEST_KEY = f\"{DATASETS_PREFIX}/train_manifest.csv\"\n",
        "TEST_MANIFEST_KEY = f\"{DATASETS_PREFIX}/test_manifest.csv\"\n",
        "REPORT_KEY = f\"{DATASETS_PREFIX}/dataset_report.json\"\n",
        "\n",
        "# Split configuration\n",
        "TEST_SIZE = 0.2  # 80% train, 20% test\n",
        "RANDOM_SEED = 42  # Fixed seed for reproducibility\n",
        "SPLIT_BY_RECIPE = True  # Split by recipe_id to avoid leakage\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  MinIO Endpoint: {MINIO_ENDPOINT}\")\n",
        "print(f\"  Fine-tuning Bucket: {FINE_TUNING_BUCKET}\")\n",
        "print(f\"  Test Size: {TEST_SIZE * 100:.0f}%\")\n",
        "print(f\"  Random Seed: {RANDOM_SEED}\")\n",
        "print(f\"  Split by Recipe ID: {SPLIT_BY_RECIPE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize MinIO Client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize S3/MinIO client\n",
        "session = boto3.session.Session(\n",
        "    aws_access_key_id=MINIO_USER,\n",
        "    aws_secret_access_key=MINIO_PASSWORD,\n",
        "    region_name=\"us-east-1\"\n",
        ")\n",
        "s3 = session.client(\n",
        "    \"s3\",\n",
        "    endpoint_url=MINIO_ENDPOINT,\n",
        "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"})\n",
        ")\n",
        "\n",
        "def ensure_bucket_exists(bucket: str) -> bool:\n",
        "    \"\"\"Create bucket if it doesn't exist.\"\"\"\n",
        "    try:\n",
        "        s3.head_bucket(Bucket=bucket)\n",
        "        return True\n",
        "    except ClientError as e:\n",
        "        error_code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
        "        if error_code in (\"404\", \"NoSuchBucket\"):\n",
        "            try:\n",
        "                s3.create_bucket(Bucket=bucket)\n",
        "                print(f\"âœ“ Created bucket '{bucket}'\")\n",
        "                return True\n",
        "            except ClientError as create_error:\n",
        "                print(f\"âœ— Failed to create bucket '{bucket}': {create_error}\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "# Verify buckets\n",
        "print(\"Checking buckets...\")\n",
        "ensure_bucket_exists(FINE_TUNING_BUCKET)\n",
        "print(\"âœ“ Buckets ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Filter Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_csv_from_minio(bucket: str, key: str) -> pd.DataFrame:\n",
        "    \"\"\"Load CSV file from MinIO into a DataFrame.\"\"\"\n",
        "    try:\n",
        "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "        df = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
        "        print(f\"âœ“ Loaded {len(df)} rows from s3://{bucket}/{key}\")\n",
        "        return df\n",
        "    except ClientError as e:\n",
        "        print(f\"âœ— Failed to load s3://{bucket}/{key}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Load augmented dataset\n",
        "print(\"Loading augmented dataset from MinIO...\")\n",
        "full_df = load_csv_from_minio(FINE_TUNING_BUCKET, INPUT_DATASET_KEY)\n",
        "\n",
        "if full_df.empty:\n",
        "    raise RuntimeError(f\"Could not load dataset from s3://{FINE_TUNING_BUCKET}/{INPUT_DATASET_KEY}\")\n",
        "\n",
        "print(f\"\\nFull dataset shape: {full_df.shape}\")\n",
        "print(f\"Columns: {list(full_df.columns)}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(full_df[\"label\"].value_counts())\n",
        "\n",
        "# Filter only positive pairs (label=1) for training/evaluation\n",
        "positive_df = full_df[full_df[\"label\"] == 1].copy()\n",
        "print(f\"\\nâœ“ Filtered to {len(positive_df)} positive pairs (label=1)\")\n",
        "print(f\"  Removed {len(full_df) - len(positive_df)} negative pairs\")\n",
        "\n",
        "print(f\"\\nPositive pairs preview:\")\n",
        "display(positive_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train/Test Split by Recipe ID\n",
        "\n",
        "**Critical:** We split by `recipe_id` to ensure no data leakage. All images and captions from the same recipe stay together in either train or test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_by_recipe_id(df: pd.DataFrame, test_size: float, random_seed: int) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split dataset by recipe_id to avoid data leakage.\n",
        "    \n",
        "    All pairs from the same recipe_id go to the same split.\n",
        "    \"\"\"\n",
        "    # Get unique recipe IDs\n",
        "    unique_recipes = df[\"recipe_id\"].unique()\n",
        "    n_recipes = len(unique_recipes)\n",
        "    \n",
        "    # Split recipe IDs\n",
        "    train_recipe_ids, test_recipe_ids = train_test_split(\n",
        "        unique_recipes,\n",
        "        test_size=test_size,\n",
        "        random_state=random_seed\n",
        "    )\n",
        "    \n",
        "    # Create train and test DataFrames\n",
        "    train_df = df[df[\"recipe_id\"].isin(train_recipe_ids)].copy()\n",
        "    test_df = df[df[\"recipe_id\"].isin(test_recipe_ids)].copy()\n",
        "    \n",
        "    return train_df, test_df, train_recipe_ids, test_recipe_ids\n",
        "\n",
        "# Perform split\n",
        "print(\"Creating train/test split by recipe_id...\")\n",
        "train_df, test_df, train_recipe_ids, test_recipe_ids = split_by_recipe_id(\n",
        "    positive_df,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ“ Split complete:\")\n",
        "print(f\"  Train recipes: {len(train_recipe_ids)}\")\n",
        "print(f\"  Test recipes: {len(test_recipe_ids)}\")\n",
        "print(f\"  Train pairs: {len(train_df)}\")\n",
        "print(f\"  Test pairs: {len(test_df)}\")\n",
        "\n",
        "# Verify no leakage\n",
        "train_recipe_set = set(train_recipe_ids)\n",
        "test_recipe_set = set(test_recipe_ids)\n",
        "overlap = train_recipe_set & test_recipe_set\n",
        "\n",
        "if overlap:\n",
        "    print(f\"\\nâš  WARNING: Found {len(overlap)} overlapping recipe IDs between train and test!\")\n",
        "else:\n",
        "    print(f\"\\nâœ“ No data leakage: train and test recipe sets are disjoint\")\n",
        "\n",
        "print(f\"\\nTrain set preview:\")\n",
        "display(train_df.head())\n",
        "print(f\"\\nTest set preview:\")\n",
        "display(test_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Dataset Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_dataset_stats(df: pd.DataFrame, split_name: str) -> Dict:\n",
        "    \"\"\"Compute comprehensive statistics for a dataset split.\"\"\"\n",
        "    stats = {\n",
        "        \"split\": split_name,\n",
        "        \"total_pairs\": len(df),\n",
        "        \"unique_recipes\": df[\"recipe_id\"].nunique(),\n",
        "        \"unique_images\": df[\"image_key\"].nunique(),\n",
        "        \"unique_captions\": df[\"caption\"].nunique(),\n",
        "    }\n",
        "    \n",
        "    # Images per recipe\n",
        "    images_per_recipe = df.groupby(\"recipe_id\")[\"image_key\"].nunique()\n",
        "    stats[\"images_per_recipe\"] = {\n",
        "        \"mean\": float(images_per_recipe.mean()),\n",
        "        \"median\": float(images_per_recipe.median()),\n",
        "        \"min\": int(images_per_recipe.min()),\n",
        "        \"max\": int(images_per_recipe.max()),\n",
        "        \"std\": float(images_per_recipe.std())\n",
        "    }\n",
        "    \n",
        "    # Captions per recipe\n",
        "    captions_per_recipe = df.groupby(\"recipe_id\")[\"caption\"].nunique()\n",
        "    stats[\"captions_per_recipe\"] = {\n",
        "        \"mean\": float(captions_per_recipe.mean()),\n",
        "        \"median\": float(captions_per_recipe.median()),\n",
        "        \"min\": int(captions_per_recipe.min()),\n",
        "        \"max\": int(captions_per_recipe.max()),\n",
        "        \"std\": float(captions_per_recipe.std())\n",
        "    }\n",
        "    \n",
        "    # Caption length distribution\n",
        "    caption_lengths = df[\"caption\"].str.len()\n",
        "    stats[\"caption_length\"] = {\n",
        "        \"mean\": float(caption_lengths.mean()),\n",
        "        \"median\": float(caption_lengths.median()),\n",
        "        \"min\": int(caption_lengths.min()),\n",
        "        \"max\": int(caption_lengths.max()),\n",
        "        \"std\": float(caption_lengths.std())\n",
        "    }\n",
        "    \n",
        "    return stats\n",
        "\n",
        "# Compute statistics\n",
        "train_stats = compute_dataset_stats(train_df, \"train\")\n",
        "test_stats = compute_dataset_stats(test_df, \"test\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Dataset Statistics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nðŸ“Š TRAIN SET:\")\n",
        "print(f\"  Total pairs: {train_stats['total_pairs']}\")\n",
        "print(f\"  Unique recipes: {train_stats['unique_recipes']}\")\n",
        "print(f\"  Unique images: {train_stats['unique_images']}\")\n",
        "print(f\"  Unique captions: {train_stats['unique_captions']}\")\n",
        "print(f\"  Images per recipe: {train_stats['images_per_recipe']['mean']:.2f} (mean), {train_stats['images_per_recipe']['median']:.1f} (median)\")\n",
        "print(f\"  Caption length: {train_stats['caption_length']['mean']:.1f} chars (mean), {train_stats['caption_length']['median']:.1f} (median)\")\n",
        "\n",
        "print(f\"\\nðŸ“Š TEST SET:\")\n",
        "print(f\"  Total pairs: {test_stats['total_pairs']}\")\n",
        "print(f\"  Unique recipes: {test_stats['unique_recipes']}\")\n",
        "print(f\"  Unique images: {test_stats['unique_images']}\")\n",
        "print(f\"  Unique captions: {test_stats['unique_captions']}\")\n",
        "print(f\"  Images per recipe: {test_stats['images_per_recipe']['mean']:.2f} (mean), {test_stats['images_per_recipe']['median']:.1f} (median)\")\n",
        "print(f\"  Caption length: {test_stats['caption_length']['mean']:.1f} chars (mean), {test_stats['caption_length']['median']:.1f} (median)\")\n",
        "\n",
        "# Create full report\n",
        "dataset_report = {\n",
        "    \"metadata\": {\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "        \"random_seed\": RANDOM_SEED,\n",
        "        \"test_size\": TEST_SIZE,\n",
        "        \"split_method\": \"by_recipe_id\",\n",
        "        \"source_dataset\": INPUT_DATASET_KEY,\n",
        "        \"filter_applied\": \"label == 1 (positive pairs only)\"\n",
        "    },\n",
        "    \"train\": train_stats,\n",
        "    \"test\": test_stats,\n",
        "    \"verification\": {\n",
        "        \"no_leakage\": len(set(train_recipe_ids) & set(test_recipe_ids)) == 0,\n",
        "        \"train_recipe_count\": len(train_recipe_ids),\n",
        "        \"test_recipe_count\": len(test_recipe_ids)\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"\\nâœ“ Dataset report generated\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_csv_to_minio(df: pd.DataFrame, bucket: str, key: str) -> bool:\n",
        "    \"\"\"Save DataFrame as CSV to MinIO.\"\"\"\n",
        "    try:\n",
        "        csv_buffer = io.StringIO()\n",
        "        df.to_csv(csv_buffer, index=False, encoding=\"utf-8\")\n",
        "        csv_bytes = csv_buffer.getvalue().encode(\"utf-8\")\n",
        "        \n",
        "        s3.put_object(\n",
        "            Bucket=bucket,\n",
        "            Key=key,\n",
        "            Body=csv_bytes,\n",
        "            ContentType=\"text/csv\",\n",
        "            Metadata={\n",
        "                \"rows\": str(len(df)),\n",
        "                \"random_seed\": str(RANDOM_SEED),\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        size_kb = len(csv_bytes) / 1024\n",
        "        print(f\"âœ“ Saved to s3://{bucket}/{key} ({size_kb:.1f} KB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Failed to save: {e}\")\n",
        "        return False\n",
        "\n",
        "def save_json_to_minio(data: Dict, bucket: str, key: str) -> bool:\n",
        "    \"\"\"Save dictionary as JSON to MinIO.\"\"\"\n",
        "    try:\n",
        "        json_bytes = json.dumps(data, indent=2).encode(\"utf-8\")\n",
        "        \n",
        "        s3.put_object(\n",
        "            Bucket=bucket,\n",
        "            Key=key,\n",
        "            Body=json_bytes,\n",
        "            ContentType=\"application/json\",\n",
        "        )\n",
        "        \n",
        "        size_kb = len(json_bytes) / 1024\n",
        "        print(f\"âœ“ Saved to s3://{bucket}/{key} ({size_kb:.1f} KB)\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"âœ— Failed to save: {e}\")\n",
        "        return False\n",
        "\n",
        "# Save manifests and report\n",
        "print(\"=\" * 60)\n",
        "print(\"Saving to MinIO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Select only necessary columns for manifests\n",
        "manifest_columns = [\"recipe_id\", \"image_key\", \"caption\"]\n",
        "\n",
        "save_csv_to_minio(train_df[manifest_columns], FINE_TUNING_BUCKET, TRAIN_MANIFEST_KEY)\n",
        "save_csv_to_minio(test_df[manifest_columns], FINE_TUNING_BUCKET, TEST_MANIFEST_KEY)\n",
        "save_json_to_minio(dataset_report, FINE_TUNING_BUCKET, REPORT_KEY)\n",
        "\n",
        "print(f\"\\nâœ… All files saved successfully!\")\n",
        "print(f\"  Train manifest: s3://{FINE_TUNING_BUCKET}/{TRAIN_MANIFEST_KEY}\")\n",
        "print(f\"  Test manifest: s3://{FINE_TUNING_BUCKET}/{TEST_MANIFEST_KEY}\")\n",
        "print(f\"  Dataset report: s3://{FINE_TUNING_BUCKET}/{REPORT_KEY}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
