{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoRA Fine-Tuning for CLIP\n",
        "\n",
        "This notebook fine-tunes CLIP using **LoRA** (Low-Rank Adaptation) on the food recipe dataset.\n",
        "\n",
        "**Hypothesis H1:** The fine-tuned model improves text↔image alignment compared to the CLIP baseline, measured with retrieval metrics (R@K, MRR).\n",
        "\n",
        "**This notebook (M1 LoRA):**\n",
        "- Loads pre-trained CLIP model (no quantization)\n",
        "- Applies LoRA adapters to trainable modules\n",
        "- Fine-tunes on training set using CLIP contrastive loss (only positive pairs, in-batch negatives)\n",
        "- Evaluates on test set with **same protocol as baseline** (by recipe_id)\n",
        "- Saves adapters, logs, and results for comparison\n",
        "\n",
        "**Inputs:**\n",
        "- `fine-tuning-zone/datasets/train_pairs_augmented_with_negatives.csv` (for training, filtered to label==1)\n",
        "- `fine-tuning-zone/datasets/test_pairs_positive.csv` (for evaluation)\n",
        "- `fine-tuning-zone/images/` and `fine-tuning-zone/augmented_images/` (images)\n",
        "\n",
        "**Outputs:**\n",
        "- `fine-tuning-zone/experiments/lora/run_{run_id}/` — Complete run directory with:\n",
        "  - `config.yaml` — Training configuration\n",
        "  - `adapters/` — LoRA adapter weights\n",
        "  - `training_logs.json` — Loss and timing logs\n",
        "  - `results_lora.json` — Evaluation metrics\n",
        "  - `examples_top5.json` — Qualitative examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import math\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import boto3\n",
        "from botocore.config import Config\n",
        "from botocore.exceptions import ClientError\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "import yaml\n",
        "\n",
        "# CLIP and LoRA imports\n",
        "TRANSFORMERS_AVAILABLE = False\n",
        "PEFT_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from transformers import CLIPModel, CLIPProcessor\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "    print(\"✓ transformers imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Failed to import transformers: {e}\")\n",
        "\n",
        "try:\n",
        "    from peft import LoraConfig, get_peft_model, TaskType\n",
        "    PEFT_AVAILABLE = True\n",
        "    print(\"✓ peft imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Failed to import peft: {e}\")\n",
        "    TaskType = None\n",
        "\n",
        "# Load environment variables\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent\n",
        "\n",
        "ENV_PATHS = [\n",
        "    PROJECT_ROOT / \"notebooks\" / \".env\",\n",
        "    PROJECT_ROOT / \"app\" / \".env\",\n",
        "    PROJECT_ROOT / \".env\",\n",
        "]\n",
        "\n",
        "env_loaded = False\n",
        "for env_path in ENV_PATHS:\n",
        "    if env_path.exists():\n",
        "        load_dotenv(env_path)\n",
        "        print(f\"✓ Loaded .env from: {env_path}\")\n",
        "        env_loaded = True\n",
        "        break\n",
        "\n",
        "if not env_loaded:\n",
        "    print(\"⚠ No .env file found, trying default load_dotenv()...\")\n",
        "    load_dotenv()\n",
        "\n",
        "# MinIO Configuration\n",
        "MINIO_USER = os.getenv(\"MINIO_USER\")\n",
        "MINIO_PASSWORD = os.getenv(\"MINIO_PASSWORD\")\n",
        "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\n",
        "\n",
        "# Bucket configuration\n",
        "FINE_TUNING_BUCKET = \"fine-tuning-zone\"\n",
        "DATASETS_PREFIX = \"datasets\"\n",
        "IMAGES_PREFIX = \"images\"\n",
        "EXPERIMENTS_PREFIX = \"experiments\"\n",
        "\n",
        "# Model configuration\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "# Device selection (robust)\n",
        "def pick_device():\n",
        "    \"\"\"Pick device, testing if CUDA actually works (not just available).\"\"\"\n",
        "    if not torch.cuda.is_available():\n",
        "        return \"cpu\"\n",
        "    try:\n",
        "        _ = torch.randn(1, device=\"cuda\")\n",
        "        return \"cuda\"\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ CUDA visible pero no usable: {e}\")\n",
        "        return \"cpu\"\n",
        "\n",
        "DEVICE = pick_device()\n",
        "\n",
        "# LoRA configuration\n",
        "LORA_CONFIG = {\n",
        "    \"r\": 8,\n",
        "    \"lora_alpha\": 16,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \"bias\": \"none\",\n",
        "    \"task_type\": TaskType.FEATURE_EXTRACTION if TaskType else None,  # Use enum or omit\n",
        "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],  # Only attention modules\n",
        "}\n",
        "\n",
        "# Training configuration\n",
        "TRAINING_CONFIG = {\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"num_epochs\": 3,\n",
        "    \"batch_size\": 2,  # Smaller batch for CPU training\n",
        "    \"gradient_accumulation_steps\": 16,  # Effective batch size = 2 * 16 = 32\n",
        "    \"warmup_steps\": 100,\n",
        "    \"save_steps\": 500,\n",
        "    \"logging_steps\": 50,\n",
        "    \"random_seed\": 42,\n",
        "}\n",
        "\n",
        "# Logit scale clamping constants (CLIP best practice)\n",
        "LOGIT_SCALE_MIN = math.log(1/100)  # Minimum temperature: 0.01\n",
        "LOGIT_SCALE_MAX = math.log(100)     # Maximum temperature: 100\n",
        "\n",
        "# Run configuration\n",
        "RUN_ID = f\"lora_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
        "RUN_DIR = f\"{EXPERIMENTS_PREFIX}/lora/run_{RUN_ID}\"\n",
        "\n",
        "# Input/Output paths\n",
        "TRAIN_AUGMENTED_KEY = f\"{DATASETS_PREFIX}/train_pairs_augmented_with_negatives.csv\"\n",
        "TEST_MANIFEST_KEY = f\"{DATASETS_PREFIX}/test_pairs_positive.csv\"\n",
        "\n",
        "CONFIG_KEY = f\"{RUN_DIR}/config.yaml\"\n",
        "ADAPTERS_DIR = f\"{RUN_DIR}/adapters\"\n",
        "LOGS_KEY = f\"{RUN_DIR}/training_logs.json\"\n",
        "RESULTS_KEY = f\"{RUN_DIR}/results_lora.json\"\n",
        "EXAMPLES_KEY = f\"{RUN_DIR}/examples_top5.json\"\n",
        "\n",
        "# Update config to include dataset info\n",
        "TRAINING_CONFIG[\"train_dataset\"] = TRAIN_AUGMENTED_KEY\n",
        "TRAINING_CONFIG[\"test_dataset\"] = TEST_MANIFEST_KEY\n",
        "TRAINING_CONFIG[\"filter_applied\"] = \"label == 1 (positive pairs only)\"\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Model: {MODEL_NAME}\")\n",
        "print(f\"  Device: {DEVICE}\")\n",
        "print(f\"  Method: LoRA\")\n",
        "print(f\"  LoRA r: {LORA_CONFIG['r']}, alpha: {LORA_CONFIG['lora_alpha']}\")\n",
        "print(f\"  Training: {TRAINING_CONFIG['num_epochs']} epochs, LR={TRAINING_CONFIG['learning_rate']}\")\n",
        "print(f\"  Batch size: {TRAINING_CONFIG['batch_size']} (effective: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']})\")\n",
        "print(f\"  Run ID: {RUN_ID}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize MinIO Client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize S3/MinIO client\n",
        "session = boto3.session.Session(\n",
        "    aws_access_key_id=MINIO_USER,\n",
        "    aws_secret_access_key=MINIO_PASSWORD,\n",
        "    region_name=\"us-east-1\"\n",
        ")\n",
        "s3 = session.client(\n",
        "    \"s3\",\n",
        "    endpoint_url=MINIO_ENDPOINT,\n",
        "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"})\n",
        ")\n",
        "\n",
        "def ensure_bucket_exists(bucket: str) -> bool:\n",
        "    \"\"\"Create bucket if it doesn't exist.\"\"\"\n",
        "    try:\n",
        "        s3.head_bucket(Bucket=bucket)\n",
        "        return True\n",
        "    except ClientError as e:\n",
        "        error_code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
        "        if error_code in (\"404\", \"NoSuchBucket\"):\n",
        "            try:\n",
        "                s3.create_bucket(Bucket=bucket)\n",
        "                print(f\"✓ Created bucket '{bucket}'\")\n",
        "                return True\n",
        "            except ClientError as create_error:\n",
        "                print(f\"✗ Failed to create bucket '{bucket}': {create_error}\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "# Verify buckets\n",
        "print(\"Checking buckets...\")\n",
        "ensure_bucket_exists(FINE_TUNING_BUCKET)\n",
        "print(\"✓ Buckets ready\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Training and Test Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training dataset\n",
        "print(\"Loading training dataset...\")\n",
        "train_obj = s3.get_object(Bucket=FINE_TUNING_BUCKET, Key=TRAIN_AUGMENTED_KEY)\n",
        "train_full_df = pd.read_csv(io.BytesIO(train_obj[\"Body\"].read()))\n",
        "\n",
        "# Filter to positive pairs only (label == 1)\n",
        "if \"label\" in train_full_df.columns:\n",
        "    train_df = train_full_df[train_full_df[\"label\"] == 1].copy()\n",
        "    print(f\"✓ Loaded {len(train_df)} positive pairs\")\n",
        "    print(f\"  Removed {len(train_full_df) - len(train_df)} negative pairs\")\n",
        "else:\n",
        "    train_df = train_full_df.copy()\n",
        "    print(f\"\\n✓ No label column found, using all {len(train_df)} pairs as positive\")\n",
        "\n",
        "# Load test dataset (from baseline split, already filtered)\n",
        "print(\"\\nLoading test dataset...\")\n",
        "test_obj = s3.get_object(Bucket=FINE_TUNING_BUCKET, Key=TEST_MANIFEST_KEY)\n",
        "test_df = pd.read_csv(io.BytesIO(test_obj[\"Body\"].read()))\n",
        "print(f\"✓ Loaded {len(test_df)} test pairs\")\n",
        "print(f\"  Recipes: {test_df['recipe_id'].nunique()}\")\n",
        "print(f\"  Images: {test_df['image_key'].nunique()}\")\n",
        "print(f\"  Captions: {test_df['caption'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Helper Functions for Image Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image_from_minio(bucket: str, key: str) -> Optional[Image.Image]:\n",
        "    \"\"\"Load an image from MinIO.\"\"\"\n",
        "    try:\n",
        "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "        img = Image.open(io.BytesIO(obj[\"Body\"].read()))\n",
        "        img.load()\n",
        "        return img.convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "# Cache for image paths to avoid head_object per sample (performance optimization)\n",
        "_image_path_cache: Dict[str, Tuple[str, str]] = {}\n",
        "\n",
        "def build_image_path_cache(df: pd.DataFrame) -> Dict[str, Tuple[str, str]]:\n",
        "    \"\"\"\n",
        "    Pre-build cache of image_key -> (bucket, full_key) to avoid head_object calls during training.\n",
        "    This significantly improves throughput when loading images from MinIO.\n",
        "    \"\"\"\n",
        "    print(\"Building image path cache...\")\n",
        "    cache = {}\n",
        "    bucket = FINE_TUNING_BUCKET\n",
        "    unique_keys = df[\"image_key\"].unique()\n",
        "    \n",
        "    for image_key in tqdm(unique_keys, desc=\"Caching image paths\"):\n",
        "        # Check if it's already a full path\n",
        "        if image_key.startswith(\"images/\") or image_key.startswith(\"augmented_images/\"):\n",
        "            cache[image_key] = (bucket, image_key)\n",
        "        else:\n",
        "            # Try images/ first\n",
        "            key = f\"{IMAGES_PREFIX}/{image_key}\"\n",
        "            # Check if exists, if not try augmented_images/\n",
        "            try:\n",
        "                s3.head_object(Bucket=bucket, Key=key)\n",
        "                cache[image_key] = (bucket, key)\n",
        "            except ClientError as e:\n",
        "                if e.response.get(\"Error\", {}).get(\"Code\") == \"404\":\n",
        "                    # Try augmented_images/\n",
        "                    key = f\"augmented_images/{image_key}\"\n",
        "                    cache[image_key] = (bucket, key)\n",
        "                else:\n",
        "                    raise\n",
        "    \n",
        "    print(f\"✓ Cached {len(cache)} image paths\")\n",
        "    return cache\n",
        "\n",
        "def get_image_path_in_minio(image_key: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Determine bucket and full key for an image (uses cache if available).\n",
        "    \n",
        "    Images can be in:\n",
        "    - fine-tuning-zone/images/...\n",
        "    - fine-tuning-zone/augmented_images/...\n",
        "    \n",
        "    If cache is built, uses it. Otherwise falls back to head_object (slower).\n",
        "    \"\"\"\n",
        "    # Use cache if available\n",
        "    if image_key in _image_path_cache:\n",
        "        return _image_path_cache[image_key]\n",
        "    \n",
        "    # Fallback: check if it's already a full path\n",
        "    bucket = FINE_TUNING_BUCKET\n",
        "    if image_key.startswith(\"images/\") or image_key.startswith(\"augmented_images/\"):\n",
        "        key = image_key\n",
        "    else:\n",
        "        # Try images/ first\n",
        "        key = f\"{IMAGES_PREFIX}/{image_key}\"\n",
        "        # Check if exists, if not try augmented_images/\n",
        "        try:\n",
        "            s3.head_object(Bucket=bucket, Key=key)\n",
        "        except ClientError as e:\n",
        "            if e.response.get(\"Error\", {}).get(\"Code\") == \"404\":\n",
        "                # Try augmented_images/\n",
        "                key = f\"augmented_images/{image_key}\"\n",
        "            else:\n",
        "                raise\n",
        "    \n",
        "    return bucket, key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create Dataset and DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "class CLIPDataset(Dataset):\n",
        "    \"\"\"Dataset for CLIP training with image-text pairs.\"\"\"\n",
        "    \n",
        "    def __init__(self, df: pd.DataFrame, processor, s3_client, bucket: str):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.processor = processor\n",
        "        self.s3 = s3_client\n",
        "        self.bucket = bucket\n",
        "        self.failed_image_count = 0  # Track failed image loads per instance\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        \n",
        "        # Load image\n",
        "        bucket, key = get_image_path_in_minio(row[\"image_key\"])\n",
        "        img = load_image_from_minio(bucket, key)\n",
        "        \n",
        "        # Get caption\n",
        "        caption = row[\"caption\"]\n",
        "        \n",
        "        if img is None:\n",
        "            # Count failed image loads (guardrail: detect if training is contaminated)\n",
        "            self.failed_image_count += 1\n",
        "            # Return invalid sample (will be filtered in collate_fn)\n",
        "            return {\n",
        "                \"image\": None,\n",
        "                \"text\": caption,\n",
        "                \"is_valid\": False\n",
        "            }\n",
        "        \n",
        "        return {\n",
        "            \"image\": img,\n",
        "            \"text\": caption,\n",
        "            \"is_valid\": True\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Collate function for DataLoader.\"\"\"\n",
        "    # Filter out invalid samples (images that failed to load)\n",
        "    batch = [b for b in batch if b.get(\"is_valid\", True) and b[\"image\"] is not None]\n",
        "    \n",
        "    if len(batch) == 0:\n",
        "        return None  # Signal to skip this batch\n",
        "    \n",
        "    images = [item[\"image\"] for item in batch]\n",
        "    texts = [item[\"text\"] for item in batch]\n",
        "    \n",
        "    # Process with CLIP processor\n",
        "    inputs = processor(\n",
        "        text=texts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "    )\n",
        "    \n",
        "    # Filter to only include what CLIPModel.forward() accepts\n",
        "    allowed_keys = {\"input_ids\", \"attention_mask\", \"pixel_values\"}\n",
        "    return {k: v for k, v in inputs.items() if k in allowed_keys}\n",
        "\n",
        "# Build image path cache (performance optimization: avoid head_object per sample)\n",
        "print(\"Building image path cache for training set...\")\n",
        "_image_path_cache.update(build_image_path_cache(train_df))\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "print(\"\\nCreating datasets and dataloaders...\")\n",
        "\n",
        "# Load processor (needed for dataset)\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "\n",
        "train_dataset = CLIPDataset(train_df, processor, s3, FINE_TUNING_BUCKET)\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,  # Set to 0 to avoid issues with MinIO\n",
        ")\n",
        "\n",
        "print(f\"✓ Created training dataloader\")\n",
        "print(f\"  Total batches: {len(train_dataloader)}\")\n",
        "print(f\"  Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
        "print(f\"  Gradient accumulation steps: {TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
        "print(f\"  Effective batch size: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Load CLIP Model with LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LORA_SCOPE = \"both\"   # \"vision\" | \"text\" | \"both\"\n",
        "TRAIN_ONLY_LORA = True\n",
        "\n",
        "if not TRANSFORMERS_AVAILABLE or not PEFT_AVAILABLE:\n",
        "    raise ImportError(\"Install: pip install transformers peft\")\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = CLIPModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_CONFIG[\"r\"],\n",
        "    lora_alpha=LORA_CONFIG[\"lora_alpha\"],\n",
        "    lora_dropout=LORA_CONFIG[\"lora_dropout\"],\n",
        "    bias=LORA_CONFIG[\"bias\"],\n",
        "    task_type=LORA_CONFIG[\"task_type\"],\n",
        "    target_modules=LORA_CONFIG[\"target_modules\"],\n",
        ")\n",
        "\n",
        "# Apply LoRA adapters\n",
        "print(\"Applying LoRA adapters...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "def count_lora_params_by_scope(model):\n",
        "    counts = {\"vision\": 0, \"text\": 0, \"other\": 0, \"total\": 0}\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"lora_\" not in name:\n",
        "            continue\n",
        "        counts[\"total\"] += p.numel()\n",
        "        if \"vision_model\" in name:\n",
        "            counts[\"vision\"] += p.numel()\n",
        "        elif \"text_model\" in name:\n",
        "            counts[\"text\"] += p.numel()\n",
        "        else:\n",
        "            counts[\"other\"] += p.numel()\n",
        "    return counts\n",
        "\n",
        "counts = count_lora_params_by_scope(model)\n",
        "print(\"LoRA param distribution:\", counts)\n",
        "\n",
        "if counts[\"total\"] == 0:\n",
        "    raise RuntimeError(\"No LoRA parameters found at all. target_modules mismatch.\")\n",
        "if LORA_SCOPE == \"vision\" and counts[\"vision\"] == 0:\n",
        "    raise RuntimeError(\"LORA_SCOPE='vision' but no LoRA params in vision_model.\")\n",
        "if LORA_SCOPE == \"text\" and counts[\"text\"] == 0:\n",
        "    raise RuntimeError(\"LORA_SCOPE='text' but no LoRA params in text_model.\")\n",
        "\n",
        "def freeze_non_lora_params(model):\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"lora_\" in name:\n",
        "            p.requires_grad = True\n",
        "        elif name.endswith(\"logit_scale\"):\n",
        "            p.requires_grad = True\n",
        "        else:\n",
        "            p.requires_grad = False\n",
        "\n",
        "def freeze_scope(model, scope: str):\n",
        "    freeze_non_lora_params(model)\n",
        "    if scope == \"both\":\n",
        "        return\n",
        "    for name, p in model.named_parameters():\n",
        "        if \"lora_\" not in name:\n",
        "            continue\n",
        "        if scope == \"vision\" and \"vision_model\" not in name:\n",
        "            p.requires_grad = False\n",
        "        if scope == \"text\" and \"text_model\" not in name:\n",
        "            p.requires_grad = False\n",
        "\n",
        "if TRAIN_ONLY_LORA:\n",
        "    freeze_scope(model, LORA_SCOPE)\n",
        "\n",
        "print(\"\\nTrainable parameters after freezing:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(f\"\\nlogit_scale requires_grad: {model.logit_scale.requires_grad}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clip_contrastive_loss(image_embeds, text_embeds, logit_scale):\n",
        "    \"\"\"\n",
        "    CLIP contrastive loss with in-batch negatives.\n",
        "    \n",
        "    Args:\n",
        "        image_embeds: Image embeddings (batch_size, embed_dim)\n",
        "        text_embeds: Text embeddings (batch_size, embed_dim)\n",
        "        logit_scale: Learnable temperature parameter\n",
        "    \n",
        "    Returns:\n",
        "        loss: Contrastive loss\n",
        "    \"\"\"\n",
        "    # Normalize embeddings\n",
        "    image_embeds = F.normalize(image_embeds, dim=-1)\n",
        "    text_embeds = F.normalize(text_embeds, dim=-1)\n",
        "    \n",
        "    # Compute logits\n",
        "    logit_scale = logit_scale.exp()\n",
        "    logits_per_text = logit_scale * (text_embeds @ image_embeds.T)  # (B, B)\n",
        "    logits_per_image = logits_per_text.T  # (B, B)\n",
        "    \n",
        "    # Labels: diagonal (each text matches its corresponding image)\n",
        "    batch_size = image_embeds.size(0)\n",
        "    labels = torch.arange(batch_size, device=image_embeds.device)\n",
        "    \n",
        "    # Cross-entropy losses\n",
        "    loss_t = F.cross_entropy(logits_per_text, labels)\n",
        "    loss_i = F.cross_entropy(logits_per_image, labels)\n",
        "    \n",
        "    # Average\n",
        "    loss = (loss_t + loss_i) / 2.0\n",
        "    \n",
        "    return loss\n",
        "\n",
        "# Setup optimizer (only trainable parameters)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=TRAINING_CONFIG[\"learning_rate\"],\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Setup learning rate scheduler (linear warmup + cosine decay)\n",
        "# Calculate steps per epoch accounting for gradient accumulation\n",
        "# Defensive check: ensure we have batches to train on\n",
        "if len(train_dataloader) == 0:\n",
        "    raise RuntimeError(\"train_dataloader has 0 batches. Check dataset filtering / loading.\")\n",
        "\n",
        "steps_per_epoch = int(np.ceil(len(train_dataloader) / TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "steps_per_epoch = max(1, steps_per_epoch)  # Ensure at least 1 step\n",
        "total_steps = steps_per_epoch * TRAINING_CONFIG[\"num_epochs\"]\n",
        "# Ensure warmup_steps doesn't exceed total_steps\n",
        "warmup_steps = min(TRAINING_CONFIG[\"warmup_steps\"], total_steps)\n",
        "\n",
        "def get_lr_scheduler(optimizer, num_warmup_steps, num_training_steps):\n",
        "    \"\"\"Create learning rate scheduler with warmup.\"\"\"\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < num_warmup_steps:\n",
        "            # Linear warmup\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        # Cosine decay\n",
        "        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))\n",
        "    \n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "scheduler = get_lr_scheduler(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training Setup\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"  Optimizer: AdamW (lr={TRAINING_CONFIG['learning_rate']})\")\n",
        "print(f\"  Scheduler: Linear warmup + Cosine decay\")\n",
        "print(f\"  Total steps: {total_steps}\")\n",
        "print(f\"  Warmup steps: {warmup_steps}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training logs\n",
        "training_logs = {\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"start_time\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"config\": {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"lora\": LORA_CONFIG,\n",
        "        \"training\": TRAINING_CONFIG,\n",
        "    },\n",
        "    \"epochs\": [],\n",
        "    \"total_time_seconds\": 0,\n",
        "    \"peak_memory_mb\": 0,\n",
        "}\n",
        "\n",
        "# Get device for inputs\n",
        "if DEVICE == \"cuda\":\n",
        "    main_device = next(model.parameters()).device\n",
        "else:\n",
        "    main_device = torch.device(\"cpu\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Starting Training\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Device: {main_device}\")\n",
        "print(f\"Total epochs: {TRAINING_CONFIG['num_epochs']}\")\n",
        "batches_per_epoch = len(train_dataloader)\n",
        "optimizer_steps_per_epoch = int(np.ceil(batches_per_epoch / TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "print(f\"Batches per epoch: {batches_per_epoch}\")\n",
        "print(f\"Optimizer steps per epoch: {optimizer_steps_per_epoch}\")\n",
        "print(f\"Total optimizer steps: {total_steps}\")\n",
        "print()\n",
        "\n",
        "model.train()\n",
        "training_start_time = time.time()\n",
        "global_step = 0\n",
        "peak_memory = 0\n",
        "step_losses = []  # Track losses per step (not per batch)\n",
        "\n",
        "# Setup mixed precision training (AMP) for GPU\n",
        "# Use generic torch.amp API (works correctly on both CPU and CUDA)\n",
        "use_amp = (DEVICE == \"cuda\")\n",
        "if use_amp:\n",
        "    scaler = torch.amp.GradScaler(\"cuda\", enabled=True)\n",
        "    print(\"✓ Mixed precision (AMP) enabled for GPU training\")\n",
        "else:\n",
        "    scaler = None  # No scaler needed for CPU\n",
        "    print(\"ℹ Mixed precision disabled (CPU training)\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(TRAINING_CONFIG[\"random_seed\"])\n",
        "np.random.seed(TRAINING_CONFIG[\"random_seed\"])\n",
        "random.seed(TRAINING_CONFIG[\"random_seed\"])\n",
        "\n",
        "for epoch in range(TRAINING_CONFIG[\"num_epochs\"]):\n",
        "    epoch_start_time = time.time()\n",
        "    epoch_losses = []\n",
        "    running_loss = 0.0  # Accumulate loss across gradient accumulation steps\n",
        "    \n",
        "    print(f\"\\nEpoch {epoch + 1}/{TRAINING_CONFIG['num_epochs']}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Reset failed image counter for this epoch\n",
        "    train_dataset.failed_image_count = 0\n",
        "    \n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n",
        "    \n",
        "    # Track last batch_idx explicitly (more robust than using locals())\n",
        "    last_batch_idx = -1\n",
        "    accum_counter = 0  # Count only valid batches for gradient accumulation\n",
        "    skipped_batches = 0  # Track skipped batches for logging\n",
        "    \n",
        "    for batch_idx, inputs in enumerate(progress_bar):\n",
        "        # Skip batches with no valid samples (filtered out in collate_fn)\n",
        "        if inputs is None:\n",
        "            skipped_batches += 1\n",
        "            continue\n",
        "        \n",
        "        last_batch_idx = batch_idx\n",
        "        accum_counter += 1  # Only increment for valid batches\n",
        "        \n",
        "        # Move inputs to device\n",
        "        input_ids = inputs[\"input_ids\"].to(main_device)\n",
        "        attention_mask = inputs.get(\"attention_mask\", None)\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.to(main_device)\n",
        "        pixel_values = inputs[\"pixel_values\"].to(main_device)\n",
        "        \n",
        "        # Forward pass with mixed precision (AMP) when on GPU\n",
        "        # Using enabled=use_amp for robustness (works even if refactored)\n",
        "        with torch.amp.autocast(\"cuda\", enabled=use_amp):\n",
        "            # Get embeddings separately\n",
        "            text_embeds = model.get_text_features(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask if attention_mask is not None else None\n",
        "            )\n",
        "            image_embeds = model.get_image_features(pixel_values=pixel_values)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = clip_contrastive_loss(\n",
        "                image_embeds,\n",
        "                text_embeds,\n",
        "                model.logit_scale\n",
        "            )\n",
        "            \n",
        "            # Scale loss for gradient accumulation\n",
        "            loss = loss / TRAINING_CONFIG[\"gradient_accumulation_steps\"]\n",
        "        \n",
        "        # Backward pass (with scaler for AMP, direct for CPU)\n",
        "        if use_amp:\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "        \n",
        "        # Accumulate unscaled loss for this accumulation step\n",
        "        running_loss += float(loss.item() * TRAINING_CONFIG[\"gradient_accumulation_steps\"])\n",
        "        \n",
        "        # Store unscaled loss for epoch average\n",
        "        epoch_losses.append(float(loss.item() * TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "        \n",
        "        # Update weights (only after accumulation steps, counting only valid batches)\n",
        "        if accum_counter % TRAINING_CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
        "            if use_amp:\n",
        "                # Gradient clipping (especially important with trainable logit_scale)\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    [p for p in model.parameters() if p.requires_grad],\n",
        "                    max_norm=1.0\n",
        "                )\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                # Gradient clipping for CPU\n",
        "                torch.nn.utils.clip_grad_norm_(\n",
        "                    [p for p in model.parameters() if p.requires_grad],\n",
        "                    max_norm=1.0\n",
        "                )\n",
        "                optimizer.step()\n",
        "            \n",
        "            # Clamp logit_scale to prevent temperature explosion (CLIP best practice)\n",
        "            with torch.no_grad():\n",
        "                model.logit_scale.clamp_(LOGIT_SCALE_MIN, LOGIT_SCALE_MAX)\n",
        "            \n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            # Store average loss for this step (across accumulation)\n",
        "            step_losses.append(running_loss / TRAINING_CONFIG[\"gradient_accumulation_steps\"])\n",
        "            running_loss = 0.0\n",
        "        \n",
        "        # Logging (by step, not by batch)\n",
        "        if global_step > 0 and global_step % TRAINING_CONFIG[\"logging_steps\"] == 0:\n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            # Use step_losses for accurate logging\n",
        "            avg_loss = np.mean(step_losses[-TRAINING_CONFIG[\"logging_steps\"]:]) if len(step_losses) >= TRAINING_CONFIG[\"logging_steps\"] else np.mean(step_losses)\n",
        "            # Log logit_scale (temperature) to monitor stability\n",
        "            logit_scale_value = float(model.logit_scale.exp().item())\n",
        "            progress_bar.set_postfix({\n",
        "                \"loss\": f\"{avg_loss:.4f}\",\n",
        "                \"lr\": f\"{current_lr:.2e}\",\n",
        "                \"temp\": f\"{logit_scale_value:.2f}\",  # Temperature (exp of logit_scale)\n",
        "                \"step\": global_step\n",
        "            })\n",
        "            \n",
        "            # Store in training logs for analysis\n",
        "            if \"step_logs\" not in training_logs:\n",
        "                training_logs[\"step_logs\"] = []\n",
        "            training_logs[\"step_logs\"].append({\n",
        "                \"global_step\": global_step,\n",
        "                \"loss\": float(avg_loss),\n",
        "                \"learning_rate\": float(current_lr),\n",
        "                \"logit_scale\": float(model.logit_scale.item()),\n",
        "                \"temperature\": logit_scale_value\n",
        "            })\n",
        "        \n",
        "        # Track peak memory\n",
        "        if DEVICE == \"cuda\":\n",
        "            current_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
        "            peak_memory = max(peak_memory, current_memory)\n",
        "    \n",
        "    # Process remaining gradients at end of epoch if not aligned with accumulation steps\n",
        "    # Use accum_counter (valid batches only) instead of batch_idx\n",
        "    remaining_steps = accum_counter % TRAINING_CONFIG[\"gradient_accumulation_steps\"]\n",
        "    \n",
        "    if remaining_steps > 0:\n",
        "        if use_amp:\n",
        "            # Gradient clipping (especially important with trainable logit_scale)\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in model.parameters() if p.requires_grad],\n",
        "                max_norm=1.0\n",
        "            )\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # Gradient clipping for CPU\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in model.parameters() if p.requires_grad],\n",
        "                max_norm=1.0\n",
        "            )\n",
        "            optimizer.step()\n",
        "        \n",
        "        # Clamp logit_scale to prevent temperature explosion (CLIP best practice)\n",
        "        with torch.no_grad():\n",
        "            model.logit_scale.clamp_(LOGIT_SCALE_MIN, LOGIT_SCALE_MAX)\n",
        "        \n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        global_step += 1\n",
        "        # Store average loss for remaining accumulation steps\n",
        "        step_losses.append(running_loss / remaining_steps)\n",
        "        running_loss = 0.0\n",
        "    \n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    avg_epoch_loss = np.mean(epoch_losses)\n",
        "    \n",
        "    # Calculate optimizer steps for this epoch\n",
        "    batches = len(train_dataloader)\n",
        "    optimizer_steps_this_epoch = int(np.ceil(batches / TRAINING_CONFIG[\"gradient_accumulation_steps\"]))\n",
        "    \n",
        "    # Report failed image loads and skipped batches (guardrail: detect if training is contaminated)\n",
        "    failed_images_this_epoch = train_dataset.failed_image_count\n",
        "    total_samples = len(train_dataloader) * TRAINING_CONFIG[\"batch_size\"]\n",
        "    valid_samples = (len(train_dataloader) - skipped_batches) * TRAINING_CONFIG[\"batch_size\"]\n",
        "    failure_rate = (failed_images_this_epoch / total_samples * 100) if total_samples > 0 else 0\n",
        "    skip_rate = (skipped_batches / len(train_dataloader) * 100) if len(train_dataloader) > 0 else 0\n",
        "    \n",
        "    print(f\"  Valid batches processed: {accum_counter} (skipped: {skipped_batches}, {skip_rate:.2f}%)\")\n",
        "    print(f\"  Valid samples: {valid_samples}\")\n",
        "    \n",
        "    if failed_images_this_epoch > 0:\n",
        "        print(f\"  ⚠ Failed image loads: {failed_images_this_epoch} ({failure_rate:.2f}% of samples)\")\n",
        "        \n",
        "        # Abort if contamination is too high (>0.5% threshold)\n",
        "        CONTAMINATION_THRESHOLD = 0.5\n",
        "        if failure_rate > CONTAMINATION_THRESHOLD:\n",
        "            raise RuntimeError(\n",
        "                f\"Training aborted: image load failure rate ({failure_rate:.2f}%) exceeds threshold ({CONTAMINATION_THRESHOLD}%). \"\n",
        "                f\"Training would be contaminated. Check image paths and MinIO connectivity.\"\n",
        "            )\n",
        "    \n",
        "    # Abort if too many batches were skipped (>10% threshold)\n",
        "    SKIP_THRESHOLD = 10.0\n",
        "    if skip_rate > SKIP_THRESHOLD:\n",
        "        raise RuntimeError(\n",
        "            f\"Training aborted: batch skip rate ({skip_rate:.2f}%) exceeds threshold ({SKIP_THRESHOLD}%). \"\n",
        "            f\"Too many batches have no valid samples. Check dataset and image loading.\"\n",
        "        )\n",
        "    \n",
        "    epoch_log = {\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"avg_loss\": float(avg_epoch_loss),\n",
        "        \"time_seconds\": float(epoch_time),\n",
        "        \"batches\": batches,\n",
        "        \"valid_batches\": accum_counter,\n",
        "        \"skipped_batches\": skipped_batches,\n",
        "        \"optimizer_steps\": optimizer_steps_this_epoch,\n",
        "        \"failed_image_loads\": failed_images_this_epoch,\n",
        "        \"skip_rate\": skip_rate,\n",
        "        \"failure_rate\": failure_rate,\n",
        "    }\n",
        "    training_logs[\"epochs\"].append(epoch_log)\n",
        "    \n",
        "    print(f\"  Average loss: {avg_epoch_loss:.4f}\")\n",
        "    print(f\"  Time: {epoch_time:.2f} seconds\")\n",
        "\n",
        "training_time = time.time() - training_start_time\n",
        "training_logs[\"total_time_seconds\"] = float(training_time)\n",
        "training_logs[\"peak_memory_mb\"] = float(peak_memory)\n",
        "training_logs[\"end_time\"] = datetime.utcnow().isoformat() + \"Z\"\n",
        "training_logs[\"total_steps\"] = global_step\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training Complete\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
        "print(f\"Peak memory: {peak_memory:.2f} MB\")\n",
        "print(f\"Total steps: {global_step}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Adapters and Training Logs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_json_to_minio(data: Dict, bucket: str, key: str) -> bool:\n",
        "    \"\"\"Save dictionary as JSON to MinIO.\"\"\"\n",
        "    try:\n",
        "        json_bytes = json.dumps(data, indent=2).encode(\"utf-8\")\n",
        "        s3.put_object(Bucket=bucket, Key=key, Body=json_bytes, ContentType=\"application/json\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to save JSON to {key}: {e}\")\n",
        "        return False\n",
        "\n",
        "def save_yaml_to_minio(data: Dict, bucket: str, key: str) -> bool:\n",
        "    \"\"\"Save dictionary as YAML to MinIO.\"\"\"\n",
        "    try:\n",
        "        yaml_str = yaml.dump(data, default_flow_style=False, sort_keys=False)\n",
        "        yaml_bytes = yaml_str.encode(\"utf-8\")\n",
        "        s3.put_object(Bucket=bucket, Key=key, Body=yaml_bytes, ContentType=\"text/yaml\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to save YAML to {key}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Save adapters\n",
        "print(\"Saving adapters...\")\n",
        "local_adapters_dir = Path(f\"adapters_{RUN_ID}\")\n",
        "model.save_pretrained(str(local_adapters_dir))\n",
        "\n",
        "# Upload adapters to MinIO\n",
        "print(f\"Uploading adapters to {ADAPTERS_DIR}/...\")\n",
        "adapter_files = [\"adapter_config.json\", \"adapter_model.bin\", \"adapter_model.safetensors\"]\n",
        "for filename in adapter_files:\n",
        "    local_path = local_adapters_dir / filename\n",
        "    if local_path.exists():\n",
        "        with open(local_path, \"rb\") as f:\n",
        "            s3.put_object(\n",
        "                Bucket=FINE_TUNING_BUCKET,\n",
        "                Key=f\"{ADAPTERS_DIR}/{filename}\",\n",
        "                Body=f.read(),\n",
        "                ContentType=\"application/octet-stream\" if filename.endswith(\".bin\") or filename.endswith(\".safetensors\") else \"application/json\"\n",
        "            )\n",
        "        print(f\"  ✓ Uploaded {filename}\")\n",
        "\n",
        "# Save training logs\n",
        "print(f\"\\nSaving training logs to {LOGS_KEY}...\")\n",
        "save_json_to_minio(training_logs, FINE_TUNING_BUCKET, LOGS_KEY)\n",
        "\n",
        "# Save config\n",
        "config_data = {\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"device\": DEVICE,\n",
        "    \"method\": \"lora\",\n",
        "    \"lora_config\": LORA_CONFIG,\n",
        "    \"training_config\": TRAINING_CONFIG,\n",
        "    \"model_info\": {\n",
        "        \"total_parameters\": int(total_params),\n",
        "        \"trainable_parameters\": int(trainable_params),\n",
        "        \"trainable_ratio\": float(trainable_params / total_params),\n",
        "    },\n",
        "}\n",
        "\n",
        "print(f\"Saving config to {CONFIG_KEY}...\")\n",
        "save_yaml_to_minio(config_data, FINE_TUNING_BUCKET, CONFIG_KEY)\n",
        "\n",
        "print(\"✓ Adapters and logs saved successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Generate Embeddings for Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "def generate_image_embeddings_from_keys(\n",
        "    image_keys: List[str],\n",
        "    model,\n",
        "    processor,\n",
        "    device: Union[str, torch.device],\n",
        "    batch_size: int = 32,\n",
        ") -> Tuple[np.ndarray, List[str]]:\n",
        "    \"\"\"Generate image embeddings for a list of unique image_keys (no duplicates).\"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    kept_keys = []\n",
        "    failed_count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx in tqdm(range(0, len(image_keys), batch_size), desc=\"Generating image embeddings\"):\n",
        "            batch_keys = image_keys[idx:idx+batch_size]\n",
        "            batch_images = []\n",
        "            batch_kept_keys = []\n",
        "\n",
        "            for img_key in batch_keys:\n",
        "                bucket, key = get_image_path_in_minio(img_key)\n",
        "                img = load_image_from_minio(bucket, key)\n",
        "                if img is not None:\n",
        "                    batch_images.append(img)\n",
        "                    batch_kept_keys.append(img_key)\n",
        "                else:\n",
        "                    failed_count += 1\n",
        "\n",
        "            if len(batch_images) == 0:\n",
        "                continue\n",
        "\n",
        "            inputs = processor(images=batch_images, return_tensors=\"pt\", padding=True)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            image_embeds = model.get_image_features(**inputs)\n",
        "            embeddings.append(image_embeds.cpu().numpy())\n",
        "            kept_keys.extend(batch_kept_keys)\n",
        "\n",
        "    if failed_count > 0:\n",
        "        print(f\"⚠ {failed_count} images failed to load during embedding generation\")\n",
        "\n",
        "    embeddings = np.vstack(embeddings) if embeddings else np.array([])\n",
        "    print(f\"✓ Generated {len(kept_keys)} unique image embeddings (shape: {embeddings.shape})\")\n",
        "    return embeddings, kept_keys\n",
        "\n",
        "\n",
        "def generate_text_embeddings(\n",
        "    texts: List[str], \n",
        "    model, \n",
        "    processor, \n",
        "    device: Union[str, torch.device], \n",
        "    batch_size: int = 32\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Generate text embeddings for all captions.\"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx in tqdm(range(0, len(texts), batch_size), desc=\"Generating text embeddings\"):\n",
        "            batch_texts = texts[idx:idx+batch_size]\n",
        "            \n",
        "            # Process texts\n",
        "            inputs = processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            \n",
        "            # Get embeddings\n",
        "            text_embeds = model.get_text_features(**inputs)\n",
        "            embeddings.append(text_embeds.cpu().numpy())\n",
        "    \n",
        "    embeddings = np.vstack(embeddings) if embeddings else np.array([])\n",
        "    print(f\"✓ Generated {len(texts)} text embeddings (shape: {embeddings.shape})\")\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "# Generate embeddings\n",
        "print(\"=\" * 60)\n",
        "print(\"Generating Embeddings for Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Image embeddings\n",
        "unique_image_keys = test_df[\"image_key\"].dropna().astype(str).unique().tolist()\n",
        "\n",
        "image_embeddings, image_keys = generate_image_embeddings_from_keys(\n",
        "    unique_image_keys, model, processor, DEVICE, batch_size=16\n",
        ")\n",
        "\n",
        "\n",
        "# Text embeddings\n",
        "unique_captions = test_df[\"caption\"].unique().tolist()\n",
        "text_embeddings = generate_text_embeddings(unique_captions, model, processor, DEVICE, batch_size=16)\n",
        "\n",
        "# Create official copy to ensure alignment with text_embeddings (never modify this)\n",
        "unique_captions_list = list(unique_captions)  # Explicit copy to prevent accidental modification\n",
        "\n",
        "embedding_time = time.time() - start_time\n",
        "\n",
        "# Create mapping dictionaries\n",
        "image_key_to_idx = {key: idx for idx, key in enumerate(image_keys)}\n",
        "caption_to_idx = {caption: idx for idx, caption in enumerate(unique_captions_list)}\n",
        "\n",
        "# Precompute lookup dictionaries for O(1) access (same as baseline)\n",
        "print(\"\\nPrecomputing lookup dictionaries...\")\n",
        "\n",
        "# recipe_id -> list of image indices\n",
        "recipe_to_image_indices: Dict[str, List[int]] = {}\n",
        "for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "    recipe_id_str = str(recipe_id)\n",
        "    recipe_images = test_df[test_df[\"recipe_id\"] == recipe_id][\"image_key\"].unique()\n",
        "    recipe_to_image_indices[recipe_id_str] = [\n",
        "        image_key_to_idx.get(img_key) \n",
        "        for img_key in recipe_images \n",
        "        if image_key_to_idx.get(img_key) is not None\n",
        "    ]\n",
        "\n",
        "# recipe_id -> representative caption (longest, same as baseline)\n",
        "recipe_to_caption_indices: Dict[str, List[int]] = {}\n",
        "recipe_to_caption_text: Dict[str, str] = {}\n",
        "# recipe_id -> list of ALL caption indices (for image→text ground truth)\n",
        "recipe_to_all_caption_indices: Dict[str, List[int]] = {}\n",
        "for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "    recipe_id_str = str(recipe_id)\n",
        "    recipe_captions = test_df[test_df[\"recipe_id\"] == recipe_id][\"caption\"].unique()\n",
        "    if len(recipe_captions) == 0:\n",
        "        continue\n",
        "    \n",
        "    # Use longest caption as representative (same as baseline)\n",
        "    representative_caption = max(recipe_captions, key=lambda c: (len(c), c))\n",
        "    \n",
        "    # Store representative caption for text→image queries\n",
        "    cap_idx = caption_to_idx.get(representative_caption)\n",
        "    if cap_idx is not None:\n",
        "        recipe_to_caption_indices[recipe_id_str] = [cap_idx]\n",
        "        recipe_to_caption_text[recipe_id_str] = representative_caption\n",
        "    \n",
        "    # Store ALL captions for image→text ground truth\n",
        "    all_caption_indices = []\n",
        "    for caption in recipe_captions:\n",
        "        cap_idx = caption_to_idx.get(caption)\n",
        "        if cap_idx is not None:\n",
        "            all_caption_indices.append(cap_idx)\n",
        "    recipe_to_all_caption_indices[recipe_id_str] = sorted(set(all_caption_indices))  # Sorted for determinism\n",
        "\n",
        "# image_key -> recipe_id (as string for consistency)\n",
        "image_key_to_recipe_id: Dict[str, str] = {}\n",
        "for _, row in test_df.iterrows():\n",
        "    image_key_to_recipe_id[str(row[\"image_key\"])] = str(row[\"recipe_id\"])\n",
        "    \n",
        "print(f\"✓ Embedding generation complete in {embedding_time:.2f} seconds\")\n",
        "print(f\"  Image embeddings: {image_embeddings.shape}\")\n",
        "print(f\"  Text embeddings: {text_embeddings.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Evaluation Functions (Same Protocol as Baseline)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation configuration (same as baseline)\n",
        "K_VALUES = [1, 5, 10]\n",
        "COMPUTE_MRR = True\n",
        "COMPUTE_MEDIAN_RANK = True\n",
        "\n",
        "def compute_recall_at_k(scores: np.ndarray, ground_truth_indices: List[int], k: int) -> float:\n",
        "    \"\"\"Compute Recall@K.\"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return 0.0\n",
        "    top_k_indices = np.argsort(scores)[-k:][::-1]\n",
        "    top_k_set = set(top_k_indices)\n",
        "    for gt_idx in ground_truth_indices:\n",
        "        if gt_idx in top_k_set:\n",
        "            return 1.0\n",
        "    return 0.0\n",
        "\n",
        "def compute_mrr(scores: np.ndarray, ground_truth_indices: List[int]) -> float:\n",
        "    \"\"\"Compute Mean Reciprocal Rank.\"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return 0.0\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    for rank, idx in enumerate(sorted_indices, start=1):\n",
        "        if idx in ground_truth_indices:\n",
        "            return 1.0 / rank\n",
        "    return 0.0\n",
        "\n",
        "def compute_first_hit_rank(scores: np.ndarray, ground_truth_indices: List[int]) -> float:\n",
        "    \"\"\"Compute rank of first correct result (first hit rank).\"\"\"\n",
        "    if len(ground_truth_indices) == 0:\n",
        "        return float(len(scores) + 1)\n",
        "    sorted_indices = np.argsort(scores)[::-1]\n",
        "    for rank, idx in enumerate(sorted_indices, start=1):\n",
        "        if idx in ground_truth_indices:\n",
        "            return float(rank)\n",
        "    return float(len(scores) + 1)\n",
        "\n",
        "def evaluate_text_to_image_by_recipe(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_indices: Dict[str, List[int]],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    k_values: List[int],\n",
        "    should_compute_mrr: bool = True,\n",
        "    should_compute_median_rank: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"Evaluate Text → Image retrieval by recipe_id (same as baseline).\"\"\"\n",
        "    print(\"\\nEvaluating Text → Image retrieval (by recipe_id)...\")\n",
        "    unique_recipe_ids = test_df[\"recipe_id\"].unique()\n",
        "    recalls = {f\"R@{k}\": [] for k in k_values}\n",
        "    mrr_scores = [] if should_compute_mrr else None\n",
        "    median_ranks = [] if should_compute_median_rank else None\n",
        "    \n",
        "    for recipe_id in tqdm(unique_recipe_ids, desc=\"Processing recipes\"):\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        caption_indices = recipe_to_caption_indices.get(recipe_id_str, [])\n",
        "        if not caption_indices:\n",
        "            continue\n",
        "        text_idx = caption_indices[0]\n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        similarities = image_embeddings @ text_emb\n",
        "        ground_truth_indices = recipe_to_image_indices.get(recipe_id_str, [])\n",
        "        ground_truth_indices = [idx for idx in ground_truth_indices if idx is not None]\n",
        "        if len(ground_truth_indices) == 0:\n",
        "            continue\n",
        "        for k in k_values:\n",
        "            recall = compute_recall_at_k(similarities, ground_truth_indices, k)\n",
        "            recalls[f\"R@{k}\"].append(recall)\n",
        "        if should_compute_mrr:\n",
        "            mrr = compute_mrr(similarities, ground_truth_indices)\n",
        "            mrr_scores.append(mrr)\n",
        "        if should_compute_median_rank:\n",
        "            first_hit_rank = compute_first_hit_rank(similarities, ground_truth_indices)\n",
        "            median_ranks.append(first_hit_rank)\n",
        "    \n",
        "    results = {}\n",
        "    for k in k_values:\n",
        "        results[f\"R@{k}\"] = float(np.mean(recalls[f\"R@{k}\"])) if recalls[f\"R@{k}\"] else 0.0\n",
        "    if should_compute_mrr and mrr_scores:\n",
        "        results[\"MRR\"] = float(np.mean(mrr_scores))\n",
        "    if should_compute_median_rank and median_ranks:\n",
        "        results[\"MedianRank_first_hit\"] = float(np.median(median_ranks))\n",
        "        results[\"MeanRank_first_hit\"] = float(np.mean(median_ranks))\n",
        "    return results\n",
        "\n",
        "def evaluate_image_to_text_by_recipe(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    k_values: List[int],\n",
        "    should_compute_mrr: bool = True,\n",
        "    should_compute_median_rank: bool = True\n",
        ") -> Dict:\n",
        "    \"\"\"Evaluate Image → Text retrieval by recipe_id (same as baseline).\"\"\"\n",
        "    print(\"\\nEvaluating Image → Text retrieval (by recipe_id)...\")\n",
        "    unique_images = test_df[\"image_key\"].dropna().astype(str).unique()\n",
        "    recalls = {f\"R@{k}\": [] for k in k_values}\n",
        "    mrr_scores = [] if should_compute_mrr else None\n",
        "    median_ranks = [] if should_compute_median_rank else None\n",
        "    \n",
        "    for img_key in tqdm(unique_images, desc=\"Processing images\"):\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        similarities = text_embeddings @ img_emb\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        if not recipe_id_str:\n",
        "            continue\n",
        "        ground_truth_indices = recipe_to_all_caption_indices.get(recipe_id_str, [])\n",
        "        ground_truth_indices = [idx for idx in ground_truth_indices if idx is not None]\n",
        "        if len(ground_truth_indices) == 0:\n",
        "            continue\n",
        "        for k in k_values:\n",
        "            recall = compute_recall_at_k(similarities, ground_truth_indices, k)\n",
        "            recalls[f\"R@{k}\"].append(recall)\n",
        "        if should_compute_mrr:\n",
        "            mrr = compute_mrr(similarities, ground_truth_indices)\n",
        "            mrr_scores.append(mrr)\n",
        "        if should_compute_median_rank:\n",
        "            first_hit_rank = compute_first_hit_rank(similarities, ground_truth_indices)\n",
        "            median_ranks.append(first_hit_rank)\n",
        "    \n",
        "    results = {}\n",
        "    for k in k_values:\n",
        "        results[f\"R@{k}\"] = float(np.mean(recalls[f\"R@{k}\"])) if recalls[f\"R@{k}\"] else 0.0\n",
        "    if should_compute_mrr and mrr_scores:\n",
        "        results[\"MRR\"] = float(np.mean(mrr_scores))\n",
        "    if should_compute_median_rank and median_ranks:\n",
        "        results[\"MedianRank_first_hit\"] = float(np.median(median_ranks))\n",
        "        results[\"MeanRank_first_hit\"] = float(np.mean(median_ranks))\n",
        "    return results\n",
        "\n",
        "def l2_normalize_np(x: np.ndarray, axis: int = -1, eps: float = 1e-12) -> np.ndarray:\n",
        "    norm = np.linalg.norm(x, axis=axis, keepdims=True)\n",
        "    return x / np.maximum(norm, eps)\n",
        "\n",
        "def maybe_normalize_embeddings(\n",
        "    image_embeddings: np.ndarray,\n",
        "    text_embeddings: np.ndarray,\n",
        "    normalize: bool\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    if not normalize:\n",
        "        return image_embeddings, text_embeddings\n",
        "    return l2_normalize_np(image_embeddings), l2_normalize_np(text_embeddings)\n",
        "\n",
        "\n",
        "NORMALIZE_FOR_EVAL = True\n",
        "\n",
        "image_embeddings_eval, text_embeddings_eval = maybe_normalize_embeddings(\n",
        "    image_embeddings, text_embeddings, normalize=NORMALIZE_FOR_EVAL\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "print(\"=\" * 60)\n",
        "print(\"Evaluation (Same Protocol as Baseline)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "text_to_image_results = evaluate_text_to_image_by_recipe(\n",
        "    test_df,\n",
        "    text_embeddings_eval,\n",
        "    image_embeddings_eval,\n",
        "    recipe_to_caption_indices,\n",
        "    recipe_to_image_indices,\n",
        "    K_VALUES,\n",
        "    COMPUTE_MRR,\n",
        "    COMPUTE_MEDIAN_RANK\n",
        ")\n",
        "\n",
        "image_to_text_results = evaluate_image_to_text_by_recipe(\n",
        "    test_df,\n",
        "    text_embeddings_eval,\n",
        "    image_embeddings_eval,\n",
        "    recipe_to_all_caption_indices,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    K_VALUES,\n",
        "    COMPUTE_MRR,\n",
        "    COMPUTE_MEDIAN_RANK\n",
        ")\n",
        "\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Evaluation Results\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nText → Image Retrieval:\")\n",
        "for metric, value in text_to_image_results.items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nImage → Text Retrieval:\")\n",
        "for metric, value in image_to_text_results.items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "\n",
        "# Use random seed from training config\n",
        "RANDOM_SEED = TRAINING_CONFIG[\"random_seed\"]\n",
        "\n",
        "def visualize_text_to_image_retrieval(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    caption_to_idx: Dict[str, int],\n",
        "    image_keys: List[str],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    n_examples: int = 3\n",
        "):\n",
        "    \"\"\"Visualize Text → Image retrieval: for each recipe, show top-5 images.\"\"\"\n",
        "    unique_recipe_ids = test_df[\"recipe_id\"].unique()\n",
        "    sample_recipe_ids = pd.Series(unique_recipe_ids).sample(\n",
        "        min(n_examples, len(unique_recipe_ids)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for recipe_idx, recipe_id in enumerate(sample_recipe_ids):\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        similarities = image_embeddings @ text_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        gt_image_indices = set(recipe_to_image_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        fig = plt.figure(figsize=(16, 5))\n",
        "        gs = gridspec.GridSpec(1, 6, width_ratios=[2, 1, 1, 1, 1, 1], hspace=0.3, bottom=0.15)\n",
        "        \n",
        "        ax0 = fig.add_subplot(gs[0])\n",
        "        ax0.text(0.5, 0.5, f'Query:\\n\"{caption}\"', \n",
        "                ha='center', va='center', fontsize=14, fontweight='bold',\n",
        "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
        "        ax0.axis('off')\n",
        "        \n",
        "        for i, img_idx in enumerate(top5_indices):\n",
        "            img_key = image_keys[img_idx]\n",
        "            bucket, key = get_image_path_in_minio(img_key)\n",
        "            img = load_image_from_minio(bucket, key)\n",
        "            if img is None:\n",
        "                continue\n",
        "            is_correct = img_idx in gt_image_indices\n",
        "            ax = fig.add_subplot(gs[i + 1])\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "            border_color = 'green' if is_correct else 'red'\n",
        "            border_width = 3 if is_correct else 2\n",
        "            for spine in ax.spines.values():\n",
        "                spine.set_edgecolor(border_color)\n",
        "                spine.set_linewidth(border_width)\n",
        "            ax.set_title(f\"Rank {i+1}\\n{'✓' if is_correct else '✗'}\", fontsize=10, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def visualize_image_to_text_retrieval(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    unique_captions: List[str],\n",
        "    n_examples: int = 3\n",
        "):\n",
        "    \"\"\"Visualize Image → Text retrieval: for each image, show top-5 captions.\"\"\"\n",
        "    unique_images = test_df[\"image_key\"].dropna().astype(str).unique()\n",
        "    sample_images = pd.Series(unique_images).sample(\n",
        "        min(n_examples, len(unique_images)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for img_idx, img_key in enumerate(sample_images):\n",
        "        img_emb_idx = image_key_to_idx.get(img_key)\n",
        "        if img_emb_idx is None:\n",
        "            continue\n",
        "        img_emb = image_embeddings[img_emb_idx]\n",
        "        similarities = text_embeddings @ img_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        gt_caption_indices = set(recipe_to_all_caption_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        bucket, key = get_image_path_in_minio(img_key)\n",
        "        query_img = load_image_from_minio(bucket, key)\n",
        "        if query_img is None:\n",
        "            continue\n",
        "        \n",
        "        fig = plt.figure(figsize=(16, 4))\n",
        "        gs = gridspec.GridSpec(1, 6, width_ratios=[1, 1, 1, 1, 1, 1], hspace=0.3, bottom=0.15)\n",
        "        \n",
        "        ax0 = fig.add_subplot(gs[0])\n",
        "        ax0.imshow(query_img)\n",
        "        ax0.axis('off')\n",
        "        ax0.set_title('Query Image', fontsize=12, fontweight='bold')\n",
        "        \n",
        "        if recipe_id_str:\n",
        "            recipe_caption = recipe_to_caption_text.get(recipe_id_str, \"Unknown\")\n",
        "            display_recipe = recipe_caption if len(recipe_caption) <= 40 else recipe_caption[:37] + \"...\"\n",
        "            ax0.text(0.5, -0.15, f'Recipe: {recipe_id_str}\\n\"{display_recipe}\"',\n",
        "                   ha='center', va='top', fontsize=9,\n",
        "                   transform=ax0.transAxes,\n",
        "                   bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8, edgecolor='gray'),\n",
        "                   wrap=True)\n",
        "        \n",
        "        for i, text_idx in enumerate(top5_indices):\n",
        "            caption = unique_captions[text_idx]\n",
        "            is_correct = text_idx in gt_caption_indices\n",
        "            ax = fig.add_subplot(gs[i + 1])\n",
        "            ax.text(0.5, 0.5, f'Rank {i+1}\\n{\"✓\" if is_correct else \"✗\"}\\n\\n\"{caption}\"',\n",
        "                   ha='center', va='center', fontsize=10,\n",
        "                   bbox=dict(boxstyle='round', facecolor='lightgreen' if is_correct else 'lightcoral', alpha=0.7),\n",
        "                   wrap=True)\n",
        "            ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Visualize Retrieval Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify required variables exist (safety check)\n",
        "assert \"image_key_to_recipe_id\" in globals(), \"image_key_to_recipe_id not found. Run evaluation section first.\"\n",
        "assert \"text_embeddings\" in globals(), \"text_embeddings not found. Run evaluation section first.\"\n",
        "assert \"image_embeddings\" in globals(), \"image_embeddings not found. Run evaluation section first.\"\n",
        "assert \"unique_captions_list\" in globals(), \"unique_captions_list not found. Run evaluation section first.\"\n",
        "assert \"image_keys\" in globals(), \"image_keys not found. Run evaluation section first.\"\n",
        "\n",
        "print(\"\\n📝 Text → Image Retrieval (Top-5 images for each recipe):\")\n",
        "visualize_text_to_image_retrieval(\n",
        "    test_df,\n",
        "    text_embeddings_eval,\n",
        "    image_embeddings_eval,\n",
        "    recipe_to_caption_text,\n",
        "    recipe_to_image_indices,\n",
        "    caption_to_idx,\n",
        "    image_keys,\n",
        "    image_key_to_recipe_id,\n",
        "    n_examples=3\n",
        ")\n",
        "\n",
        "print(\"\\n🖼️ Image → Text Retrieval (Top-5 captions for each image):\")\n",
        "visualize_image_to_text_retrieval(\n",
        "    test_df,\n",
        "    text_embeddings_eval,\n",
        "    image_embeddings_eval,\n",
        "    recipe_to_all_caption_indices,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    unique_captions_list,\n",
        "    n_examples=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Generate Qualitative Examples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "N_QUALITATIVE_EXAMPLES = 5\n",
        "\n",
        "def generate_top5_examples(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_caption_text: Dict[str, str],\n",
        "    recipe_to_image_indices: Dict[str, List[int]],\n",
        "    caption_to_idx: Dict[str, int],\n",
        "    image_keys: List[str],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    n_examples: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"Generate qualitative examples showing top-5 retrievals by recipe_id.\"\"\"\n",
        "    examples = []\n",
        "    valid_recipe_ids = []\n",
        "    for recipe_id in test_df[\"recipe_id\"].unique():\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        gt_image_indices = recipe_to_image_indices.get(recipe_id_str, [])\n",
        "        if len(gt_image_indices) > 0:\n",
        "            valid_recipe_ids.append(recipe_id)\n",
        "    \n",
        "    if len(valid_recipe_ids) == 0:\n",
        "        print(\"⚠ No valid recipes with ground truth images found\")\n",
        "        return examples\n",
        "    \n",
        "    sample_recipe_ids = pd.Series(valid_recipe_ids).sample(\n",
        "        min(n_examples, len(valid_recipe_ids)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for recipe_id in sample_recipe_ids:\n",
        "        recipe_id_str = str(recipe_id)\n",
        "        caption = recipe_to_caption_text.get(recipe_id_str)\n",
        "        if not caption:\n",
        "            continue\n",
        "        text_idx = caption_to_idx.get(caption)\n",
        "        if text_idx is None:\n",
        "            continue\n",
        "        text_emb = text_embeddings[text_idx]\n",
        "        similarities = image_embeddings @ text_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        gt_image_indices = set(recipe_to_image_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        top5_images = []\n",
        "        for i, idx in enumerate(top5_indices):\n",
        "            img_key = image_keys[idx]\n",
        "            img_recipe_id = image_key_to_recipe_id.get(img_key)\n",
        "            is_correct = idx in gt_image_indices\n",
        "            top5_images.append({\n",
        "                \"rank\": i + 1,\n",
        "                \"image_key\": img_key,\n",
        "                \"recipe_id\": img_recipe_id,\n",
        "                \"similarity\": float(similarities[idx]),\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "        \n",
        "        examples.append({\n",
        "            \"query_type\": \"text_to_image\",\n",
        "            \"recipe_id\": recipe_id_str,\n",
        "            \"query_caption\": caption,\n",
        "            \"top5_images\": top5_images\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "def generate_image_to_text_examples(\n",
        "    test_df: pd.DataFrame,\n",
        "    text_embeddings: np.ndarray,\n",
        "    image_embeddings: np.ndarray,\n",
        "    recipe_to_all_caption_indices: Dict[str, List[int]],\n",
        "    image_key_to_idx: Dict[str, int],\n",
        "    image_key_to_recipe_id: Dict[str, str],\n",
        "    unique_captions: List[str],\n",
        "    n_examples: int = 5\n",
        ") -> List[Dict]:\n",
        "    \"\"\"Generate qualitative examples for Image → Text retrieval.\"\"\"\n",
        "    examples = []\n",
        "    unique_images = test_df[\"image_key\"].dropna().astype(str).unique()\n",
        "    sample_images = pd.Series(unique_images).sample(\n",
        "        min(n_examples, len(unique_images)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    \n",
        "    for img_key in sample_images:\n",
        "        img_idx = image_key_to_idx.get(img_key)\n",
        "        if img_idx is None:\n",
        "            continue\n",
        "        img_emb = image_embeddings[img_idx]\n",
        "        similarities = text_embeddings @ img_emb\n",
        "        top5_indices = np.argsort(similarities)[-5:][::-1]\n",
        "        recipe_id_str = image_key_to_recipe_id.get(img_key)\n",
        "        gt_caption_indices = set(recipe_to_all_caption_indices.get(recipe_id_str, []))\n",
        "        \n",
        "        top5_captions = []\n",
        "        for i, text_idx in enumerate(top5_indices):\n",
        "            caption = unique_captions[text_idx]\n",
        "            is_correct = text_idx in gt_caption_indices\n",
        "            top5_captions.append({\n",
        "                \"rank\": i + 1,\n",
        "                \"caption\": caption,\n",
        "                \"similarity\": float(similarities[text_idx]),\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "        \n",
        "        examples.append({\n",
        "            \"query_type\": \"image_to_text\",\n",
        "            \"recipe_id\": recipe_id_str,\n",
        "            \"query_image_key\": img_key,\n",
        "            \"top5_captions\": top5_captions\n",
        "        })\n",
        "    \n",
        "    return examples\n",
        "\n",
        "# Generate examples\n",
        "assert \"unique_captions_list\" in globals(), \"unique_captions_list not found\"\n",
        "\n",
        "text_to_image_examples = generate_top5_examples(\n",
        "    test_df,\n",
        "    text_embeddings_eval,\n",
        "    image_embeddings_eval,\n",
        "    recipe_to_caption_text,\n",
        "    recipe_to_image_indices,\n",
        "    caption_to_idx,\n",
        "    image_keys,\n",
        "    image_key_to_recipe_id,\n",
        "    n_examples=N_QUALITATIVE_EXAMPLES\n",
        ")\n",
        "\n",
        "image_to_text_examples = generate_image_to_text_examples(\n",
        "    test_df,\n",
        "    text_embeddings_eval,\n",
        "    image_embeddings_eval,\n",
        "    recipe_to_all_caption_indices,\n",
        "    image_key_to_idx,\n",
        "    image_key_to_recipe_id,\n",
        "    unique_captions_list,\n",
        "    n_examples=N_QUALITATIVE_EXAMPLES\n",
        ")\n",
        "\n",
        "all_examples = text_to_image_examples + image_to_text_examples\n",
        "print(f\"✓ Generated {len(all_examples)} qualitative examples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare results\n",
        "results = {\n",
        "    \"run_id\": RUN_ID,\n",
        "    \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"method\": \"lora\",\n",
        "    \"metrics\": {\n",
        "        \"text_to_image\": text_to_image_results,\n",
        "        \"image_to_text\": image_to_text_results,\n",
        "    },\n",
        "    \"metadata\": {\n",
        "        \"device\": DEVICE,\n",
        "        \"embedding_time_seconds\": float(embedding_time),\n",
        "        \"filter_applied\": \"label == 1 (positive pairs only) for training\",\n",
        "    },\n",
        "    \"model_info\": {\n",
        "        \"total_parameters\": int(total_params),\n",
        "        \"trainable_parameters\": int(trainable_params),\n",
        "        \"trainable_ratio\": float(trainable_params / total_params),\n",
        "    },\n",
        "    \"training_info\": {\n",
        "        \"total_time_seconds\": training_logs[\"total_time_seconds\"],\n",
        "        \"peak_memory_mb\": training_logs[\"peak_memory_mb\"],\n",
        "        \"total_steps\": training_logs[\"total_steps\"],\n",
        "        \"num_epochs\": TRAINING_CONFIG[\"num_epochs\"],\n",
        "        \"final_loss\": float(training_logs[\"epochs\"][-1][\"avg_loss\"]) if training_logs[\"epochs\"] else None,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Save results\n",
        "print(f\"Saving results to {RESULTS_KEY}...\")\n",
        "save_json_to_minio(results, FINE_TUNING_BUCKET, RESULTS_KEY)\n",
        "\n",
        "# Save qualitative examples\n",
        "print(f\"Saving qualitative examples to {EXAMPLES_KEY}...\")\n",
        "save_json_to_minio({\"examples\": all_examples}, FINE_TUNING_BUCKET, EXAMPLES_KEY)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"✓ All results saved successfully\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model: {MODEL_NAME} (LoRA fine-tuned)\")\n",
        "print(f\"Run ID: {RUN_ID}\")\n",
        "print(f\"\\nResults saved to:\")\n",
        "print(f\"  - {RESULTS_KEY}\")\n",
        "print(f\"  - {EXAMPLES_KEY}\")\n",
        "print(f\"  - {LOGS_KEY}\")\n",
        "print(f\"  - {CONFIG_KEY}\")\n",
        "print(f\"  - {ADAPTERS_DIR}/\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
