{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-markdown",
      "metadata": {},
      "source": [
        "# Fine-Tuning Dataset Preparation\n",
        "\n",
        "This notebook prepares the multimodal dataset for fine-tuning embedding models such as CLIP.\n",
        "\n",
        "**Objectives:**\n",
        "1. Load the curated multimodal data from the Trusted Zone\n",
        "2. Create image-text positive pairs suitable for contrastive learning\n",
        "3. Clean the data (remove missing values, duplicates)\n",
        "4. Split into reproducible train/test sets\n",
        "5. Copy referenced images to the fine-tuning zone\n",
        "6. Save datasets to MinIO for downstream fine-tuning\n",
        "\n",
        "**Data Sources:**\n",
        "- `trusted-zone/documents/recipes.jsonl` — Recipe metadata with cleaned text\n",
        "- `recipe_ids_with_images.json` — Mapping from recipe IDs to image keys\n",
        "- `trusted-zone/images/` — Normalized images (512×512 JPEG)\n",
        "\n",
        "**Outputs:**\n",
        "- `fine-tuning-zone/datasets/train_pairs_positive.csv`\n",
        "- `fine-tuning-zone/datasets/test_pairs_positive.csv`\n",
        "- `fine-tuning-zone/images/` — Copies of training/test images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "setup-header",
      "metadata": {},
      "source": [
        "## 1. Setup and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "setup-code",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded .env from: /home/didac/Desktop/upc/adsdb/adsdb-multimodal-food-data-management/notebooks/.env\n",
            "Configuration:\n",
            "  MinIO Endpoint: http://localhost:9000\n",
            "  Trusted Bucket: trusted-zone\n",
            "  Output Directory: /home/didac/Desktop/upc/adsdb/adsdb-multimodal-food-data-management/fine_tuning/training_data\n",
            "  Test Size: 20%\n",
            "  Random Seed: 42\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import io\n",
        "import json\n",
        "from pathlib import Path, PurePosixPath\n",
        "from typing import Dict, List, Optional, Set, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import boto3\n",
        "from botocore.config import Config\n",
        "from botocore.exceptions import ClientError\n",
        "from dotenv import load_dotenv\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load environment variables from project root or notebooks folder\n",
        "# The .env file is located outside the fine_tuning folder\n",
        "NOTEBOOK_DIR = Path.cwd()  # fine_tuning/training_data/\n",
        "PROJECT_ROOT = NOTEBOOK_DIR.parent.parent  # Go up two levels to project root\n",
        "\n",
        "ENV_PATHS = [\n",
        "    PROJECT_ROOT / \"notebooks\" / \".env\",\n",
        "    PROJECT_ROOT / \"app\" / \".env\",\n",
        "    PROJECT_ROOT / \".env\",\n",
        "]\n",
        "\n",
        "env_loaded = False\n",
        "for env_path in ENV_PATHS:\n",
        "    if env_path.exists():\n",
        "        load_dotenv(env_path)\n",
        "        print(f\"✓ Loaded .env from: {env_path}\")\n",
        "        env_loaded = True\n",
        "        break\n",
        "\n",
        "if not env_loaded:\n",
        "    print(\"⚠ No .env file found in expected locations. Trying default load_dotenv()...\")\n",
        "    load_dotenv()\n",
        "\n",
        "# Configuration\n",
        "MINIO_USER = os.getenv(\"MINIO_USER\")\n",
        "MINIO_PASSWORD = os.getenv(\"MINIO_PASSWORD\")\n",
        "MINIO_ENDPOINT = os.getenv(\"MINIO_ENDPOINT\")\n",
        "\n",
        "# Data paths in MinIO\n",
        "TRUSTED_BUCKET = \"trusted-zone\"\n",
        "TRUSTED_DOCS_KEY = \"documents/recipes.jsonl\"\n",
        "TRUSTED_IMAGES_PREFIX = \"images\"\n",
        "\n",
        "# Fine-tuning zone configuration (for storing training datasets and images in MinIO)\n",
        "FINE_TUNING_BUCKET = \"fine-tuning-zone\"\n",
        "FINE_TUNING_PREFIX = \"datasets\"\n",
        "FINE_TUNING_IMAGES_PREFIX = \"images\"  # Store copies of training images here\n",
        "\n",
        "# Output paths (local)\n",
        "OUTPUT_DIR = Path(\".\")  # Current directory (fine_tuning/training_data/)\n",
        "TRAIN_OUTPUT_FILE = OUTPUT_DIR / \"train_pairs_positive.csv\"\n",
        "TEST_OUTPUT_FILE = OUTPUT_DIR / \"test_pairs_positive.csv\"\n",
        "\n",
        "# Split configuration\n",
        "TEST_SIZE = 0.2  # 80% train, 20% test\n",
        "RANDOM_SEED = 42  # Fixed seed for reproducibility\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  MinIO Endpoint: {MINIO_ENDPOINT}\")\n",
        "print(f\"  Trusted Bucket: {TRUSTED_BUCKET}\")\n",
        "print(f\"  Output Directory: {OUTPUT_DIR.absolute()}\")\n",
        "print(f\"  Test Size: {TEST_SIZE * 100:.0f}%\")\n",
        "print(f\"  Random Seed: {RANDOM_SEED}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "s3-client",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Successfully connected to MinIO, bucket 'trusted-zone' is accessible\n"
          ]
        }
      ],
      "source": [
        "# Initialize S3/MinIO client\n",
        "session = boto3.session.Session(\n",
        "    aws_access_key_id=MINIO_USER,\n",
        "    aws_secret_access_key=MINIO_PASSWORD,\n",
        "    region_name=\"us-east-1\"\n",
        ")\n",
        "s3 = session.client(\n",
        "    \"s3\",\n",
        "    endpoint_url=MINIO_ENDPOINT,\n",
        "    config=Config(signature_version=\"s3v4\", s3={\"addressing_style\": \"path\"})\n",
        ")\n",
        "\n",
        "def check_bucket_exists(bucket: str) -> bool:\n",
        "    \"\"\"Check if a bucket exists and is accessible.\"\"\"\n",
        "    try:\n",
        "        s3.head_bucket(Bucket=bucket)\n",
        "        return True\n",
        "    except ClientError:\n",
        "        return False\n",
        "\n",
        "# Verify connectivity\n",
        "if check_bucket_exists(TRUSTED_BUCKET):\n",
        "    print(f\"✓ Successfully connected to MinIO, bucket '{TRUSTED_BUCKET}' is accessible\")\n",
        "else:\n",
        "    print(f\"⚠ Could not access bucket '{TRUSTED_BUCKET}' — check MinIO is running and credentials are correct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-data-header",
      "metadata": {},
      "source": [
        "## 2. Load Data from Trusted Zone\n",
        "\n",
        "We load the recipe documents (JSONL) and the image mapping file to build our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "load-recipes",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded 272 recipes from s3://trusted-zone/documents/recipes.jsonl\n",
            "\n",
            "Sample recipe keys: ['valid__from_det_ingrs', 'id', 'ingredients__from_det_ingrs', 'ingredients__from_layer1', 'title__from_layer1', 'instructions__from_layer1', 'has_nutrition_data', 'title_text_raw', 'ingredients_text_raw', 'instructions_text_raw', 'title_text_clean', 'ingredients_text_clean', 'instructions_text_clean', 'nutrition_normalized']...\n",
            "Sample ID: 00003a70b1\n",
            "Sample title (raw): Crunchy Onion Potato Bake...\n"
          ]
        }
      ],
      "source": [
        "def load_recipes_from_s3(bucket: str, key: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Load recipes from a JSONL file stored in S3/MinIO.\n",
        "    Returns a list of recipe dictionaries.\n",
        "    \"\"\"\n",
        "    recipes = []\n",
        "    try:\n",
        "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
        "        for line in obj[\"Body\"].iter_lines():\n",
        "            if line:\n",
        "                recipes.append(json.loads(line))\n",
        "        print(f\"✓ Loaded {len(recipes)} recipes from s3://{bucket}/{key}\")\n",
        "    except ClientError as e:\n",
        "        print(f\"✗ Failed to load recipes: {e}\")\n",
        "    return recipes\n",
        "\n",
        "# Load recipes from trusted zone\n",
        "recipes = load_recipes_from_s3(TRUSTED_BUCKET, TRUSTED_DOCS_KEY)\n",
        "\n",
        "if recipes:\n",
        "    # Show sample record structure\n",
        "    sample = recipes[0]\n",
        "    print(f\"\\nSample recipe keys: {list(sample.keys())[:15]}...\")\n",
        "    print(f\"Sample ID: {sample.get('id')}\")\n",
        "    print(f\"Sample title (raw): {sample.get('title_text_raw', 'N/A')[:80]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "load-images-mapping",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Found 462 images for 269 unique recipes\n",
            "\n",
            "Images per recipe distribution:\n",
            "  Min: 1, Max: 11, Mean: 1.72\n"
          ]
        }
      ],
      "source": [
        "def list_trusted_images(bucket: str, prefix: str) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    List all images in the trusted zone and build a mapping from recipe ID to image keys.\n",
        "    Image filenames follow the pattern: ...type$src$ts$hash__<recipeId>_<position>.jpg\n",
        "    \"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Pattern to extract recipe ID from image filename\n",
        "    # e.g., \"type$src$ts$hash__000018c8a5_0.jpg\" -> recipe_id = \"000018c8a5\"\n",
        "    id_pattern = re.compile(r\"__([A-Za-z0-9_\\-]+)_(\\d+)\\.(?:jpe?g|png|webp|gif|bmp|tiff)$\", re.IGNORECASE)\n",
        "    \n",
        "    recipe_to_images: Dict[str, List[str]] = {}\n",
        "    total_images = 0\n",
        "    \n",
        "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
        "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
        "        for obj in page.get(\"Contents\", []):\n",
        "            key = obj[\"Key\"]\n",
        "            if key.endswith(\"/\"):\n",
        "                continue\n",
        "            \n",
        "            # Extract recipe ID from filename\n",
        "            filename = PurePosixPath(key).name\n",
        "            match = id_pattern.search(filename)\n",
        "            if match:\n",
        "                recipe_id = match.group(1)\n",
        "                recipe_to_images.setdefault(recipe_id, []).append(key)\n",
        "                total_images += 1\n",
        "    \n",
        "    # Sort image lists for determinism\n",
        "    for rid in recipe_to_images:\n",
        "        recipe_to_images[rid].sort()\n",
        "    \n",
        "    print(f\"✓ Found {total_images} images for {len(recipe_to_images)} unique recipes\")\n",
        "    return recipe_to_images\n",
        "\n",
        "# Build image mapping from trusted zone\n",
        "recipe_to_images = list_trusted_images(TRUSTED_BUCKET, TRUSTED_IMAGES_PREFIX)\n",
        "\n",
        "if recipe_to_images:\n",
        "    # Show distribution of images per recipe\n",
        "    images_per_recipe = [len(v) for v in recipe_to_images.values()]\n",
        "    print(f\"\\nImages per recipe distribution:\")\n",
        "    print(f\"  Min: {min(images_per_recipe)}, Max: {max(images_per_recipe)}, Mean: {sum(images_per_recipe)/len(images_per_recipe):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "build-pairs-header",
      "metadata": {},
      "source": [
        "## 3. Build Image-Text Positive Pairs\n",
        "\n",
        "For each recipe that has both text and images, we create positive pairs:\n",
        "- **Image**: S3 key (path) to the image in the trusted zone\n",
        "- **Text**: Recipe title (cleaned) — this will serve as the caption for contrastive learning\n",
        "\n",
        "We use the cleaned title as it provides a concise, descriptive caption that aligns well with the visual content of food images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "build-pairs",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Built 462 image-text pairs from 269 unique recipes\n",
            "\n",
            "DataFrame shape: (462, 3)\n",
            "\n",
            "Sample pairs:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>recipe_id</th>\n",
              "      <th>image_key</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>004205a8a0</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Banana French Toast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>004205a8a0</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Banana French Toast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>004205a8a0</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Banana French Toast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>004205a8a0</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Banana French Toast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>004205a8a0</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Banana French Toast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0036b28b5f</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Pain Au Riz (Rice Bread)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0036b28b5f</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Pain Au Riz (Rice Bread)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0021f004a6</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Soured Milk Cake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0021f004a6</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Soured Milk Cake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0021f004a6</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Soured Milk Cake</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    recipe_id                                          image_key  \\\n",
              "0  004205a8a0  images/image$adsdb-multimodal-food-data-manage...   \n",
              "1  004205a8a0  images/image$adsdb-multimodal-food-data-manage...   \n",
              "2  004205a8a0  images/image$adsdb-multimodal-food-data-manage...   \n",
              "3  004205a8a0  images/image$adsdb-multimodal-food-data-manage...   \n",
              "4  004205a8a0  images/image$adsdb-multimodal-food-data-manage...   \n",
              "5  0036b28b5f  images/image$adsdb-multimodal-food-data-manage...   \n",
              "6  0036b28b5f  images/image$adsdb-multimodal-food-data-manage...   \n",
              "7  0021f004a6  images/image$adsdb-multimodal-food-data-manage...   \n",
              "8  0021f004a6  images/image$adsdb-multimodal-food-data-manage...   \n",
              "9  0021f004a6  images/image$adsdb-multimodal-food-data-manage...   \n",
              "\n",
              "                    caption  \n",
              "0       Banana French Toast  \n",
              "1       Banana French Toast  \n",
              "2       Banana French Toast  \n",
              "3       Banana French Toast  \n",
              "4       Banana French Toast  \n",
              "5  Pain Au Riz (Rice Bread)  \n",
              "6  Pain Au Riz (Rice Bread)  \n",
              "7          Soured Milk Cake  \n",
              "8          Soured Milk Cake  \n",
              "9          Soured Milk Cake  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def build_positive_pairs(\n",
        "    recipes: List[Dict],\n",
        "    recipe_to_images: Dict[str, List[str]],\n",
        "    text_field: str = \"title_text_raw\",\n",
        "    fallback_fields: Optional[List[str]] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build a DataFrame of positive image-text pairs.\n",
        "    \n",
        "    For each recipe with images, creates one row per image with:\n",
        "    - recipe_id: Unique identifier\n",
        "    - image_key: S3 path to the image\n",
        "    - caption: Text description (title)\n",
        "    \n",
        "    Args:\n",
        "        recipes: List of recipe dictionaries from trusted zone\n",
        "        recipe_to_images: Mapping from recipe ID to list of image keys\n",
        "        text_field: Primary field to use for caption\n",
        "        fallback_fields: Alternative fields if primary is empty\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with columns: recipe_id, image_key, caption\n",
        "    \"\"\"\n",
        "    if fallback_fields is None:\n",
        "        fallback_fields = [\"title_text_clean\", \"title__from_layer1\", \"title\"]\n",
        "    \n",
        "    pairs = []\n",
        "    \n",
        "    # Build recipe lookup by ID\n",
        "    recipe_by_id = {r.get(\"id\"): r for r in recipes if r.get(\"id\")}\n",
        "    \n",
        "    for recipe_id, image_keys in recipe_to_images.items():\n",
        "        recipe = recipe_by_id.get(recipe_id)\n",
        "        if not recipe:\n",
        "            continue\n",
        "        \n",
        "        # Get caption text\n",
        "        caption = recipe.get(text_field, \"\").strip()\n",
        "        if not caption:\n",
        "            for fallback in fallback_fields:\n",
        "                caption = recipe.get(fallback, \"\").strip()\n",
        "                if caption:\n",
        "                    break\n",
        "        \n",
        "        if not caption:\n",
        "            continue\n",
        "        \n",
        "        # Create one pair per image\n",
        "        for image_key in image_keys:\n",
        "            pairs.append({\n",
        "                \"recipe_id\": recipe_id,\n",
        "                \"image_key\": image_key,\n",
        "                \"caption\": caption\n",
        "            })\n",
        "    \n",
        "    df = pd.DataFrame(pairs)\n",
        "    print(f\"✓ Built {len(df)} image-text pairs from {df['recipe_id'].nunique()} unique recipes\")\n",
        "    return df\n",
        "\n",
        "# Build positive pairs\n",
        "pairs_df = build_positive_pairs(recipes, recipe_to_images)\n",
        "\n",
        "if not pairs_df.empty:\n",
        "    print(f\"\\nDataFrame shape: {pairs_df.shape}\")\n",
        "    print(f\"\\nSample pairs:\")\n",
        "    display(pairs_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "clean-data-header",
      "metadata": {},
      "source": [
        "## 4. Data Cleaning\n",
        "\n",
        "Clean the dataset by:\n",
        "1. Removing rows with missing image keys or captions\n",
        "2. Removing duplicate pairs\n",
        "3. Removing very short or very long captions (potential noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "clean-data",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Data Cleaning Report\n",
            "============================================================\n",
            "Initial count: 462 pairs\n",
            "After removing missing values: 462 pairs (removed 0)\n",
            "After removing empty strings: 462 pairs\n",
            "After caption length filter [3, 200]: 462 pairs\n",
            "After removing duplicates: 462 pairs (removed 0 duplicates)\n",
            "\n",
            "✓ Final cleaned dataset: 462 pairs (100.0% retained)\n"
          ]
        }
      ],
      "source": [
        "def clean_pairs_dataframe(\n",
        "    df: pd.DataFrame,\n",
        "    min_caption_len: int = 3,\n",
        "    max_caption_len: int = 200\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Clean the pairs DataFrame by removing invalid/duplicate entries.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame with columns (recipe_id, image_key, caption)\n",
        "        min_caption_len: Minimum caption length in characters\n",
        "        max_caption_len: Maximum caption length in characters\n",
        "    \n",
        "    Returns:\n",
        "        Cleaned DataFrame\n",
        "    \"\"\"\n",
        "    initial_count = len(df)\n",
        "    print(f\"Initial count: {initial_count} pairs\")\n",
        "    \n",
        "    # 1. Remove rows with missing values\n",
        "    df = df.dropna(subset=[\"image_key\", \"caption\"])\n",
        "    print(f\"After removing missing values: {len(df)} pairs (removed {initial_count - len(df)})\")\n",
        "    \n",
        "    # 2. Remove empty strings\n",
        "    df = df[df[\"image_key\"].str.strip().astype(bool)]\n",
        "    df = df[df[\"caption\"].str.strip().astype(bool)]\n",
        "    print(f\"After removing empty strings: {len(df)} pairs\")\n",
        "    \n",
        "    # 3. Filter by caption length\n",
        "    df = df[df[\"caption\"].str.len() >= min_caption_len]\n",
        "    df = df[df[\"caption\"].str.len() <= max_caption_len]\n",
        "    print(f\"After caption length filter [{min_caption_len}, {max_caption_len}]: {len(df)} pairs\")\n",
        "    \n",
        "    # 4. Remove exact duplicates (same image_key + caption)\n",
        "    before_dedup = len(df)\n",
        "    df = df.drop_duplicates(subset=[\"image_key\", \"caption\"])\n",
        "    print(f\"After removing duplicates: {len(df)} pairs (removed {before_dedup - len(df)} duplicates)\")\n",
        "    \n",
        "    # 5. Reset index\n",
        "    df = df.reset_index(drop=True)\n",
        "    \n",
        "    print(f\"\\n✓ Final cleaned dataset: {len(df)} pairs ({len(df)/initial_count*100:.1f}% retained)\")\n",
        "    return df\n",
        "\n",
        "# Clean the dataset\n",
        "print(\"=\" * 60)\n",
        "print(\"Data Cleaning Report\")\n",
        "print(\"=\" * 60)\n",
        "cleaned_df = clean_pairs_dataframe(pairs_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "data-stats",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Cleaned Dataset Statistics\n",
            "============================================================\n",
            "\n",
            "Total pairs: 462\n",
            "Unique recipes: 269\n",
            "Unique images: 462\n",
            "Unique captions: 268\n",
            "\n",
            "Caption length statistics:\n",
            "  Min: 9 characters\n",
            "  Max: 118 characters\n",
            "  Mean: 28.5 characters\n",
            "  Median: 26.0 characters\n",
            "\n",
            "Images per recipe:\n",
            "  Min: 1\n",
            "  Max: 11\n",
            "  Mean: 1.72\n",
            "\n",
            "Sample captions:\n",
            "  1. Peg's Chili\n",
            "  2. Classic Lasagna\n",
            "  3. Aunt Marie's Peas\n",
            "  4. Roast Salmon With Spiced Coconut Crumbs\n",
            "  5. Crunchy Onion Potato Bake\n"
          ]
        }
      ],
      "source": [
        "# Display cleaned data statistics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Cleaned Dataset Statistics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nTotal pairs: {len(cleaned_df)}\")\n",
        "print(f\"Unique recipes: {cleaned_df['recipe_id'].nunique()}\")\n",
        "print(f\"Unique images: {cleaned_df['image_key'].nunique()}\")\n",
        "print(f\"Unique captions: {cleaned_df['caption'].nunique()}\")\n",
        "\n",
        "# Caption length statistics\n",
        "caption_lengths = cleaned_df[\"caption\"].str.len()\n",
        "print(f\"\\nCaption length statistics:\")\n",
        "print(f\"  Min: {caption_lengths.min()} characters\")\n",
        "print(f\"  Max: {caption_lengths.max()} characters\")\n",
        "print(f\"  Mean: {caption_lengths.mean():.1f} characters\")\n",
        "print(f\"  Median: {caption_lengths.median():.1f} characters\")\n",
        "\n",
        "# Images per recipe\n",
        "images_per_recipe = cleaned_df.groupby(\"recipe_id\").size()\n",
        "print(f\"\\nImages per recipe:\")\n",
        "print(f\"  Min: {images_per_recipe.min()}\")\n",
        "print(f\"  Max: {images_per_recipe.max()}\")\n",
        "print(f\"  Mean: {images_per_recipe.mean():.2f}\")\n",
        "\n",
        "# Sample captions\n",
        "print(f\"\\nSample captions:\")\n",
        "for i, caption in enumerate(cleaned_df[\"caption\"].sample(5, random_state=RANDOM_SEED).values):\n",
        "    print(f\"  {i+1}. {caption[:80]}{'...' if len(caption) > 80 else ''}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "split-data-header",
      "metadata": {},
      "source": [
        "## 5. Train/Test Split\n",
        "\n",
        "Split the data into training and test sets with a fixed random seed for reproducibility.\n",
        "\n",
        "**Important:** We split by recipe ID (not by individual pairs) to prevent data leakage — all images from the same recipe go to either train or test, not both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "split-data",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split by recipe ID (seed=42):\n",
            "  Train: 365 pairs (215 recipes)\n",
            "  Test: 97 pairs (54 recipes)\n",
            "  Actual test ratio: 21.0%\n"
          ]
        }
      ],
      "source": [
        "def split_by_recipe(\n",
        "    df: pd.DataFrame,\n",
        "    test_size: float = 0.2,\n",
        "    random_seed: int = 42\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split DataFrame into train/test by recipe ID to avoid data leakage.\n",
        "    \n",
        "    All pairs from the same recipe go to either train or test, ensuring\n",
        "    that similar images from the same recipe don't appear in both sets.\n",
        "    \n",
        "    Args:\n",
        "        df: Input DataFrame with 'recipe_id' column\n",
        "        test_size: Fraction of recipes for test set\n",
        "        random_seed: Random seed for reproducibility\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (train_df, test_df)\n",
        "    \"\"\"\n",
        "    # Get unique recipe IDs\n",
        "    recipe_ids = df[\"recipe_id\"].unique()\n",
        "    \n",
        "    # Split recipe IDs\n",
        "    train_ids, test_ids = train_test_split(\n",
        "        recipe_ids,\n",
        "        test_size=test_size,\n",
        "        random_state=random_seed\n",
        "    )\n",
        "    \n",
        "    # Create train/test DataFrames\n",
        "    train_df = df[df[\"recipe_id\"].isin(train_ids)].copy()\n",
        "    test_df = df[df[\"recipe_id\"].isin(test_ids)].copy()\n",
        "    \n",
        "    # Reset indices\n",
        "    train_df = train_df.reset_index(drop=True)\n",
        "    test_df = test_df.reset_index(drop=True)\n",
        "    \n",
        "    print(f\"Split by recipe ID (seed={random_seed}):\")\n",
        "    print(f\"  Train: {len(train_df)} pairs ({len(train_ids)} recipes)\")\n",
        "    print(f\"  Test: {len(test_df)} pairs ({len(test_ids)} recipes)\")\n",
        "    print(f\"  Actual test ratio: {len(test_df) / len(df) * 100:.1f}%\")\n",
        "    \n",
        "    return train_df, test_df\n",
        "\n",
        "# Perform split\n",
        "train_df, test_df = split_by_recipe(cleaned_df, test_size=TEST_SIZE, random_seed=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "verify-split",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Verification:\n",
            "  Train recipe IDs: 215\n",
            "  Test recipe IDs: 54\n",
            "  Overlap: 0 recipes\n",
            "  ✓ No data leakage - train and test sets are properly separated\n"
          ]
        }
      ],
      "source": [
        "# Verify no overlap between train and test\n",
        "train_recipes = set(train_df[\"recipe_id\"].unique())\n",
        "test_recipes = set(test_df[\"recipe_id\"].unique())\n",
        "overlap = train_recipes.intersection(test_recipes)\n",
        "\n",
        "print(f\"\\nVerification:\")\n",
        "print(f\"  Train recipe IDs: {len(train_recipes)}\")\n",
        "print(f\"  Test recipe IDs: {len(test_recipes)}\")\n",
        "print(f\"  Overlap: {len(overlap)} recipes\")\n",
        "\n",
        "if len(overlap) == 0:\n",
        "    print(\"  ✓ No data leakage - train and test sets are properly separated\")\n",
        "else:\n",
        "    print(\"  ⚠ Warning: There is overlap between train and test sets!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e63a5c5f",
      "metadata": {},
      "source": [
        "## 6. Copy Images to Fine-Tuning Zone\n",
        "\n",
        "Copy all training and test images from the trusted zone to the fine-tuning zone for tidiness. This keeps all fine-tuning data in one bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8feffe17",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Copying Images to Fine-Tuning Zone\n",
            "============================================================\n",
            "Copying train images to s3://fine-tuning-zone/images/...\n",
            "  ✓ Copied 365 images, 0 failed\n",
            "Copying test images to s3://fine-tuning-zone/images/...\n",
            "  ✓ Copied 97 images, 0 failed\n",
            "\n",
            "✓ All images copied to s3://fine-tuning-zone/images/\n"
          ]
        }
      ],
      "source": [
        "def copy_image_to_fine_tuning(\n",
        "    src_bucket: str,\n",
        "    src_key: str,\n",
        "    dst_bucket: str,\n",
        "    dst_prefix: str\n",
        ") -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Copy an image from trusted zone to fine-tuning zone.\n",
        "    \n",
        "    Args:\n",
        "        src_bucket: Source bucket (trusted-zone)\n",
        "        src_key: Source image key\n",
        "        dst_bucket: Destination bucket (fine-tuning-zone)\n",
        "        dst_prefix: Destination prefix (images)\n",
        "    \n",
        "    Returns:\n",
        "        New image key in destination bucket, or None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract just the filename from the source key\n",
        "        filename = PurePosixPath(src_key).name\n",
        "        dst_key = f\"{dst_prefix}/{filename}\"\n",
        "        \n",
        "        # Copy the object\n",
        "        s3.copy_object(\n",
        "            Bucket=dst_bucket,\n",
        "            Key=dst_key,\n",
        "            CopySource={\"Bucket\": src_bucket, \"Key\": src_key}\n",
        "        )\n",
        "        return dst_key\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "\n",
        "def copy_dataset_images(\n",
        "    df: pd.DataFrame,\n",
        "    src_bucket: str,\n",
        "    dst_bucket: str,\n",
        "    dst_prefix: str,\n",
        "    split_name: str = \"train\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Copy all images referenced in a DataFrame to the fine-tuning zone.\n",
        "    Updates the image_key column to point to the new location.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with image_key column\n",
        "        src_bucket: Source bucket\n",
        "        dst_bucket: Destination bucket  \n",
        "        dst_prefix: Destination prefix\n",
        "        split_name: Name of the split (for logging)\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with updated image_key values\n",
        "    \"\"\"\n",
        "    print(f\"Copying {split_name} images to s3://{dst_bucket}/{dst_prefix}/...\")\n",
        "    \n",
        "    df = df.copy()\n",
        "    unique_images = df[\"image_key\"].unique()\n",
        "    \n",
        "    copied = 0\n",
        "    failed = 0\n",
        "    key_mapping = {}\n",
        "    \n",
        "    for src_key in unique_images:\n",
        "        new_key = copy_image_to_fine_tuning(src_bucket, src_key, dst_bucket, dst_prefix)\n",
        "        if new_key:\n",
        "            key_mapping[src_key] = new_key\n",
        "            copied += 1\n",
        "        else:\n",
        "            key_mapping[src_key] = src_key  # Keep original if copy fails\n",
        "            failed += 1\n",
        "    \n",
        "    # Update image_key column with new keys\n",
        "    df[\"image_key\"] = df[\"image_key\"].map(key_mapping)\n",
        "    \n",
        "    print(f\"  ✓ Copied {copied} images, {failed} failed\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# Ensure fine-tuning bucket exists\n",
        "def ensure_bucket_exists_for_copy(bucket: str) -> bool:\n",
        "    \"\"\"Create bucket if it doesn't exist.\"\"\"\n",
        "    try:\n",
        "        s3.head_bucket(Bucket=bucket)\n",
        "        return True\n",
        "    except ClientError as e:\n",
        "        error_code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
        "        if error_code in (\"404\", \"NoSuchBucket\"):\n",
        "            try:\n",
        "                s3.create_bucket(Bucket=bucket)\n",
        "                print(f\"✓ Created bucket '{bucket}'\")\n",
        "                return True\n",
        "            except ClientError:\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "# Copy images to fine-tuning zone\n",
        "print(\"=\" * 60)\n",
        "print(\"Copying Images to Fine-Tuning Zone\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if ensure_bucket_exists_for_copy(FINE_TUNING_BUCKET):\n",
        "    # Copy train images\n",
        "    train_df = copy_dataset_images(\n",
        "        train_df,\n",
        "        src_bucket=TRUSTED_BUCKET,\n",
        "        dst_bucket=FINE_TUNING_BUCKET,\n",
        "        dst_prefix=FINE_TUNING_IMAGES_PREFIX,\n",
        "        split_name=\"train\"\n",
        "    )\n",
        "    \n",
        "    # Copy test images\n",
        "    test_df = copy_dataset_images(\n",
        "        test_df,\n",
        "        src_bucket=TRUSTED_BUCKET,\n",
        "        dst_bucket=FINE_TUNING_BUCKET,\n",
        "        dst_prefix=FINE_TUNING_IMAGES_PREFIX,\n",
        "        split_name=\"test\"\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n✓ All images copied to s3://{FINE_TUNING_BUCKET}/{FINE_TUNING_IMAGES_PREFIX}/\")\n",
        "else:\n",
        "    print(\"⚠ Could not access fine-tuning bucket, keeping original image paths\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save-data-header",
      "metadata": {},
      "source": [
        "## 7. Save Output Files (Local)\n",
        "\n",
        "Save the train and test splits as CSV files locally for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "save-data",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Saving Output Files\n",
            "============================================================\n",
            "✓ Saved 365 pairs to train_pairs_positive.csv (58.5 KB)\n",
            "✓ Saved 97 pairs to test_pairs_positive.csv (15.3 KB)\n"
          ]
        }
      ],
      "source": [
        "def save_pairs_to_csv(df: pd.DataFrame, filepath: Path) -> None:\n",
        "    \"\"\"\n",
        "    Save pairs DataFrame to CSV file.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame to save\n",
        "        filepath: Output file path\n",
        "    \"\"\"\n",
        "    # Ensure output directory exists\n",
        "    filepath.parent.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Save to CSV\n",
        "    df.to_csv(filepath, index=False, encoding=\"utf-8\")\n",
        "    \n",
        "    # Report\n",
        "    file_size_kb = filepath.stat().st_size / 1024\n",
        "    print(f\"✓ Saved {len(df)} pairs to {filepath} ({file_size_kb:.1f} KB)\")\n",
        "\n",
        "# Save train and test files\n",
        "print(\"=\" * 60)\n",
        "print(\"Saving Output Files\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "save_pairs_to_csv(train_df, TRAIN_OUTPUT_FILE)\n",
        "save_pairs_to_csv(test_df, TEST_OUTPUT_FILE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "preview-header",
      "metadata": {},
      "source": [
        "## 8. Preview Saved Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "preview-files",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preview of train_pairs_positive.csv:\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>recipe_id</th>\n",
              "      <th>image_key</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>004205a8a0</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Banana French Toast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>004205a8a0</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Banana French Toast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>004205a8a0</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Banana French Toast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>004205a8a0</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Banana French Toast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>004205a8a0</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Banana French Toast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0036b28b5f</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Pain Au Riz (Rice Bread)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0036b28b5f</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Pain Au Riz (Rice Bread)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0021f004a6</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Soured Milk Cake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0021f004a6</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Soured Milk Cake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0021f004a6</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Soured Milk Cake</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    recipe_id                                          image_key  \\\n",
              "0  004205a8a0  images/image$adsdb-multimodal-food-data-manage...   \n",
              "1  004205a8a0  images/image$adsdb-multimodal-food-data-manage...   \n",
              "2  004205a8a0  images/image$adsdb-multimodal-food-data-manage...   \n",
              "3  004205a8a0  images/image$adsdb-multimodal-food-data-manage...   \n",
              "4  004205a8a0  images/image$adsdb-multimodal-food-data-manage...   \n",
              "5  0036b28b5f  images/image$adsdb-multimodal-food-data-manage...   \n",
              "6  0036b28b5f  images/image$adsdb-multimodal-food-data-manage...   \n",
              "7  0021f004a6  images/image$adsdb-multimodal-food-data-manage...   \n",
              "8  0021f004a6  images/image$adsdb-multimodal-food-data-manage...   \n",
              "9  0021f004a6  images/image$adsdb-multimodal-food-data-manage...   \n",
              "\n",
              "                    caption  \n",
              "0       Banana French Toast  \n",
              "1       Banana French Toast  \n",
              "2       Banana French Toast  \n",
              "3       Banana French Toast  \n",
              "4       Banana French Toast  \n",
              "5  Pain Au Riz (Rice Bread)  \n",
              "6  Pain Au Riz (Rice Bread)  \n",
              "7          Soured Milk Cake  \n",
              "8          Soured Milk Cake  \n",
              "9          Soured Milk Cake  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Preview of test_pairs_positive.csv:\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>recipe_id</th>\n",
              "      <th>image_key</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>004a63989e</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Lighter Spicy Garlic Shrimp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>002481e577</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Quick &amp; Easy Chicken Parmigiana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>002481e577</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Quick &amp; Easy Chicken Parmigiana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>002481e577</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Quick &amp; Easy Chicken Parmigiana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0044818076</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Noodles With Spicy Peanut Sauce</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0044818076</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Noodles With Spicy Peanut Sauce</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0044818076</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Noodles With Spicy Peanut Sauce</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0044818076</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Noodles With Spicy Peanut Sauce</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0044818076</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Noodles With Spicy Peanut Sauce</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0044818076</td>\n",
              "      <td>images/image$adsdb-multimodal-food-data-manage...</td>\n",
              "      <td>Noodles With Spicy Peanut Sauce</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    recipe_id                                          image_key  \\\n",
              "0  004a63989e  images/image$adsdb-multimodal-food-data-manage...   \n",
              "1  002481e577  images/image$adsdb-multimodal-food-data-manage...   \n",
              "2  002481e577  images/image$adsdb-multimodal-food-data-manage...   \n",
              "3  002481e577  images/image$adsdb-multimodal-food-data-manage...   \n",
              "4  0044818076  images/image$adsdb-multimodal-food-data-manage...   \n",
              "5  0044818076  images/image$adsdb-multimodal-food-data-manage...   \n",
              "6  0044818076  images/image$adsdb-multimodal-food-data-manage...   \n",
              "7  0044818076  images/image$adsdb-multimodal-food-data-manage...   \n",
              "8  0044818076  images/image$adsdb-multimodal-food-data-manage...   \n",
              "9  0044818076  images/image$adsdb-multimodal-food-data-manage...   \n",
              "\n",
              "                           caption  \n",
              "0      Lighter Spicy Garlic Shrimp  \n",
              "1  Quick & Easy Chicken Parmigiana  \n",
              "2  Quick & Easy Chicken Parmigiana  \n",
              "3  Quick & Easy Chicken Parmigiana  \n",
              "4  Noodles With Spicy Peanut Sauce  \n",
              "5  Noodles With Spicy Peanut Sauce  \n",
              "6  Noodles With Spicy Peanut Sauce  \n",
              "7  Noodles With Spicy Peanut Sauce  \n",
              "8  Noodles With Spicy Peanut Sauce  \n",
              "9  Noodles With Spicy Peanut Sauce  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Preview the saved CSV files\n",
        "print(\"Preview of train_pairs_positive.csv:\")\n",
        "print(\"-\" * 60)\n",
        "display(pd.read_csv(TRAIN_OUTPUT_FILE).head(10))\n",
        "\n",
        "print(\"\\nPreview of test_pairs_positive.csv:\")\n",
        "print(\"-\" * 60)\n",
        "display(pd.read_csv(TEST_OUTPUT_FILE).head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da313b06",
      "metadata": {},
      "source": [
        "## 9. Upload Datasets to MinIO\n",
        "\n",
        "Store the training datasets in the `fine-tuning-zone` bucket for centralized access and version control. This keeps all pipeline data organized in MinIO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5decd0cd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Uploading Datasets to MinIO\n",
            "============================================================\n",
            "✓ Bucket 'fine-tuning-zone' already exists\n",
            "✓ Uploaded 365 rows to s3://fine-tuning-zone/datasets/train_pairs_positive.csv (58.5 KB)\n",
            "✓ Uploaded 97 rows to s3://fine-tuning-zone/datasets/test_pairs_positive.csv (15.3 KB)\n",
            "\n",
            "✅ Datasets available at:\n",
            "   s3://fine-tuning-zone/datasets/train_pairs_positive.csv\n",
            "   s3://fine-tuning-zone/datasets/test_pairs_positive.csv\n"
          ]
        }
      ],
      "source": [
        "def ensure_bucket_exists(bucket: str) -> bool:\n",
        "    \"\"\"\n",
        "    Create bucket if it doesn't exist.\n",
        "    Returns True if bucket exists or was created successfully.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        s3.head_bucket(Bucket=bucket)\n",
        "        print(f\"✓ Bucket '{bucket}' already exists\")\n",
        "        return True\n",
        "    except ClientError as e:\n",
        "        error_code = e.response.get(\"Error\", {}).get(\"Code\", \"\")\n",
        "        if error_code in (\"404\", \"NoSuchBucket\"):\n",
        "            try:\n",
        "                s3.create_bucket(Bucket=bucket)\n",
        "                print(f\"✓ Created bucket '{bucket}'\")\n",
        "                return True\n",
        "            except ClientError as create_error:\n",
        "                print(f\"✗ Failed to create bucket '{bucket}': {create_error}\")\n",
        "                return False\n",
        "        else:\n",
        "            print(f\"✗ Error checking bucket '{bucket}': {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "def upload_csv_to_minio(df: pd.DataFrame, bucket: str, key: str) -> bool:\n",
        "    \"\"\"\n",
        "    Upload a DataFrame as CSV to MinIO.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame to upload\n",
        "        bucket: Target bucket name\n",
        "        key: Object key (path) in the bucket\n",
        "    \n",
        "    Returns:\n",
        "        True if upload succeeded, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert DataFrame to CSV bytes\n",
        "        csv_buffer = io.StringIO()\n",
        "        df.to_csv(csv_buffer, index=False, encoding=\"utf-8\")\n",
        "        csv_bytes = csv_buffer.getvalue().encode(\"utf-8\")\n",
        "        \n",
        "        # Upload to MinIO\n",
        "        s3.put_object(\n",
        "            Bucket=bucket,\n",
        "            Key=key,\n",
        "            Body=csv_bytes,\n",
        "            ContentType=\"text/csv\",\n",
        "            Metadata={\n",
        "                \"rows\": str(len(df)),\n",
        "                \"columns\": \",\".join(df.columns.tolist()),\n",
        "                \"created_at\": pd.Timestamp.now().isoformat(),\n",
        "                \"random_seed\": str(RANDOM_SEED),\n",
        "                \"test_size\": str(TEST_SIZE),\n",
        "            }\n",
        "        )\n",
        "        \n",
        "        size_kb = len(csv_bytes) / 1024\n",
        "        print(f\"✓ Uploaded {len(df)} rows to s3://{bucket}/{key} ({size_kb:.1f} KB)\")\n",
        "        return True\n",
        "        \n",
        "    except ClientError as e:\n",
        "        print(f\"✗ Failed to upload to s3://{bucket}/{key}: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# Upload datasets to MinIO\n",
        "print(\"=\" * 60)\n",
        "print(\"Uploading Datasets to MinIO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Ensure the fine-tuning bucket exists\n",
        "if ensure_bucket_exists(FINE_TUNING_BUCKET):\n",
        "    # Define S3 keys for the datasets\n",
        "    train_key = f\"{FINE_TUNING_PREFIX}/train_pairs_positive.csv\"\n",
        "    test_key = f\"{FINE_TUNING_PREFIX}/test_pairs_positive.csv\"\n",
        "    \n",
        "    # Upload both datasets\n",
        "    upload_csv_to_minio(train_df, FINE_TUNING_BUCKET, train_key)\n",
        "    upload_csv_to_minio(test_df, FINE_TUNING_BUCKET, test_key)\n",
        "    \n",
        "    print(f\"\\n✅ Datasets available at:\")\n",
        "    print(f\"   s3://{FINE_TUNING_BUCKET}/{train_key}\")\n",
        "    print(f\"   s3://{FINE_TUNING_BUCKET}/{test_key}\")\n",
        "else:\n",
        "    print(\"⚠ Could not create/access fine-tuning bucket. Datasets saved locally only.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
